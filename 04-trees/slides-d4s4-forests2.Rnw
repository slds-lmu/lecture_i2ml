% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{3}{Random Forest cont.}
\lecture{Introduction to Machine Learning}

\sloppy


\begin{vbframe}{Variable importance}

\begin{itemize}
  \item Single trees are highly interpretable
  \item Random Forests as an ensemble of many trees lose this feature
  \item Hence, contributions of a single covariate to the fit are difficult to evaluate
  \item Way out: variable importance measures
\end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \caption*{Measure based on permutations of OOB observations}
  \begin{algorithmic}[1]
    \State After growing tree $\blmh$, pass down OOB observations and record
    predictive accuracy.
    \State Permute OOB observations of $j$th variable.
    \State Pass down the permuted OOB observations and evaluate predictive accuracy again.
    \State The loss of goodness induced by permutation is averaged over all trees and is used as a measure for the importance of the $j^\text{th}$ variable.
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \small
  \caption*{Measure based on improvement in split criterion}
  \begin{algorithmic}[1]
    \State At each split in tree $\blmh$ the improvement in the split criterion is attributed as variable importance measure for the splitting variable.
    \State For each variable, this improvement is accumulated over all trees for the importance measure.
  \end{algorithmic}
\end{algorithm}
\end{vbframe}

\begin{vbframe}{Variable Importance based on permutations of OOB observations}
\begin{center}
\includegraphics[width = 10.3cm]{figure_man/rF_varImp_permutation.jpg}
\end{center}
\end{vbframe}

\begin{vbframe}{Variable importance}
\begin{figure}
<<size="footnotesize", fig.height=3>>=
model = randomForest(Species ~ ., data = iris, importance = TRUE)
randomForest::varImpPlot(model)
@
\caption{Two importance measures on iris.}
\end{figure}

\begin{figure}
<<size="footnotesize", fig.height=3>>=
v = generateFilterValuesData(iris.task,
  method = c("randomForest.importance", "cforest.importance"))
plotFilterValues(v)
@
\caption{RF importance as filters in mlr.}
\end{figure}

\end{vbframe}

\begin{vbframe}{Random Forest: Advantages}

\begin{itemize}
  \item Easy to implement
  \item Can be applied to basically any model
  \item Easy to parallelize
  \item Often works well (enough)
  \item Enables variance analysis
  \item Integrated estimation of OOB error
  \item Can work on high-dimensional data
  \item Often not much tuning necessary
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forest: Disadvantages}

\begin{itemize}
  \item Often suboptimal for regression
  \item Hard to interpret, especially interactions
  \item Does not really optimize loss aggressively
  \item No real way to adapt to problem\\
  (see e.g. loss in GBM, kernel in SVM)
  \item Implementations sometimes memory-hungry
  \item Prediction can be slow
\end{itemize}

\end{vbframe}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) rpart vs. (bagged) knn}

  \begin{itemize}
    \item Goal: Compare performance of random forest against (bagged) stable and (bagged) unstable methods
    \item Algorithms:
    \begin{itemize}
      \item classification tree (\code{rpart})
      \item bagged classification tree using 50 bagging iterations (\code{bagged.rpart})
      \item k-nearest neighbors (\code{knn})
      \item bagged k-nearest neighbors using 50 bagging iterations (\code{bagged.knn})
      \item random forest with 50 trees (\code{rF})
    \end{itemize}
    \item Method to evaluate performance: 10-fold cross-validation
    \item Performance measure: mean missclassification error
    \end{itemize}

    \framebreak

    \begin{itemize}
    \item Datasets from \pkg{mlbench}:
    \end{itemize}

\begin{table}
\footnotesize
\begin{tabular}{p{1.5cm}p{2cm}p{0.5cm}p{0.5cm}p{5cm}}
Name & Kind of data &  n & p & Task\\
\hline
Glass & Glass identification data & 214 & 10 & Predict the type of glass on the basis of the chemical analysis of the glasses represented by the 10 features\\
Ionosphere & Radar data & 351 & 35 & Predict whether the radar returns show evidence of some type of structure in the ionosphere (\enquote{good}) or not (\enquote{bad}) \\
Sonar & Sonar data & 208 & 61 & Discriminate between sonar signals bounced off a metal cylinder (\enquote{M}) and those bounced off a cylindrical rock (\enquote{R})\\
Waveform & Artificial data & 100 & 21 & Simulated 3-class problem which is considered to be a difficult pattern recognition problem. Each class is generated by the waveform generator.\\
\hline
\end{tabular}
\end{table}

  \framebreak

  \begin{center}
  \includegraphics[height = 7.2cm, keepaspectratio]{figure_man/bm_stable_vs_unstable.jpg}
  \end{center}

  \framebreak

  Bagging knn does not improve performance because:

  \begin{itemize}
    \item knn is stable w.r.t. perturbations
    \item Each bootstrap sample contains about 63\% unique observations
        $$ \P(\text{obs. drawn}) = \left(1 - \frac{1}{n}\right)^n = \ \stackrel{n \to \infty}{\longrightarrow} \ 1 - \frac{1}{e} \approx 0.63$$
    \item In a 2-class problem classification may only change if for a test case the nearest neighbor in the learning set is \textbf{not} in at least half of the bootstrap samples.
    \item But probability that an observation is drawn into the bootstrap sample is 63\% which is greater than 50\%.
    \end{itemize}

\end{vbframe}

\endlecture
