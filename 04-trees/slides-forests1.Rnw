% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{14}{Bagging and Random Forests}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Ensemble methods}

% \begin{itemize}

% \item A \enquote{\bf{base learner}} is often referred to as \enquote{weak learner}.
% \enquote{\bf{Weak learners}} (e.g. decision trees) are learning algorithms that should perform (slightly) better than random guessing, e.g. in case of a balanced  classification problem the misclassification rate should be (slightly) better than 0.5.
% \item The linear combination of the base learners potentially expands the hypothesis space.
% \end{itemize}

% \framebreak
% Common ensemble methods:
  \begin{itemize}
\item Ensemble methods combine the predictions of several base learners and combine them into an aggregated estimator.
  \item Homogeneous ensembles (multiple models of same base learner)
    \begin{itemize}
    \item Bagging: Fit models on bootstrapped versions of training data
    \item Boosting: runs sequentially: each model on reweighted data version / the previous residuals so
      it improves the errors of the previous round
    \end{itemize}
  \item Heterogeneous ensembles (different base learners)
    \begin{itemize}
      \item Fit different base learners on the same data or different \enquote{views} of the same data.
        Then learn how to aggregate their predictions, often with a 2nd-layer model.
    \end{itemize}
  \end{itemize}

\framebreak

General homogeneous approach (often it works like this but not always)
  \begin{itemize}
  \item A \enquote{base learner} is selected and fitted multiple times to either resampled or reweighted versions
  of the original data.
  \item The base learner is applied to either resampled or reweighted versions of the original dataset.
  % \item his results in $M$ prediction functions $g^{(1)}(x),\dots,g^{(M)}(x)$.
  This results in $M$ prediction functions $\bl{1},\dots,\bl{M}$.
  \item These $M$ function are aggregated, usually in a linear fashion.

  This results in the following final prediction function:
$$f(x) = \sum_{m=1}^M \betam \blm$$
with coefficients $\betai{1},\dots,\betai{M}$.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Bagging}
  \begin{itemize}
    \item Bagging is based on {\bf B}ootstrap {\bf Agg}regation.
    \item Proposed by Breiman (1996).
    \item Train on multiple bootstrap samples of data $\D$, then combine:

    \begin{enumerate}
      \item Create $M$ bootstrap samples of size $n$.
      \item Fit the base learner on each of the $M$ bootstrap samples.
      \item Aggregate the predictions of the $M$ estimators via averaging or majority voting.
    \end{enumerate}

    \item $M$: main hyperparameter, affects Monte-Carlo approximation error.
    \item Interpretability of the model becomes harder.
  \end{itemize}

\framebreak

\begin{algorithm}[H]
  \small
  \setstretch{1.15}
  \caption*{Bagging algorithm}
  \begin{algorithmic}[1]
    \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
    \For {$m = 1 \to M$}
      \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
      \State Train base learner on $\D^{[m]}$, obtain model $\blm$
    \EndFor
    \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
    $$
    f(x) = \frac{1}{M} \sum_{m=1}^M \blm
    $$
  \end{algorithmic}
\end{algorithm}

\framebreak

  \begin{itemize}
    \item Bagging reduces the variance of the estimator, but increases the bias in return.
    \item Bagging works best for unstable/high variance learners (learners where small perturbations in training set lead to larger changes in the prediction)

    \begin{itemize}
      \item Classification and regression trees
      \item Neural networks
      \item Piece-wise variable selection in the regression case, etc.
    \end{itemize}

    \item For stable estimation methods bagging might degrade performance
    \begin{itemize}
      \item k-nearest neighbor
      \item discriminant analysis
      \item naive Bayes
      \item linear regression
    \end{itemize}

  \end{itemize}
\end{vbframe}

\begin{vbframe}{Why does bagging work?}
\begin{itemize}
  \item Suppose we have a numerical dependent variable.
  \item The training datasets are given by $\D$ and base learner estimator by $f(x)$.
  \item The datasets are sampled independently from distribution $\P_{xy}$ (data generating process).
  \item The {\em theoretical} aggregated estimator is given by
    \begin{align*}
      f_{\text{A}} (x) &= \E_\D[f(x)].
    \end{align*}

  \framebreak

  \item Let x,y be a random sample from $\P_{xy}$ but independent of $\D$. The average error of the normal
    $f(x)$ is then
    $e = E_\D E_{xy} [(y - f(x))^2]$
    and of the aggregated estimator
    $e_A = E_{xy} [(y - f_A(x))^2]$.
 \item It follows:
   $$ e = E_\D E_{xy} [(y - f(x))^2] = E_{xy}[y^2] - 2 E_{xy}[y f_A] + E_{\D} E_{xy} [f^2(x)] $$

 \item And we apply Jensen's inequality to $e$:
   $$ e = E_{xy} E_\D [(y - f(x))^2] \geq E_{xy} (E_\D[y - f(x)])^2 = E_{xy} [(y - f_A(x)])^2] = e_A $$
 $$\qquad\qquad\qquad\qquad = E_{xy}[y^2] - 2 E_{xy}[y f_A] + E_{xy} [f_A^2(x)] $$

 \framebreak
 \item The difference between $e$ and $e_A$ is $E_{\D} E_{xy} [f^2(x)] \geq \E_{xy} [f_A^2(x)] $

 \item The more unstable $f(x)$, the more error reduction we obtain.
 \item But the bagging estimator only approximates the theoretical $f_A$ (bootstrap), we therefore suffer from approximation error (bias) by using the empirical distribution function instead of the true data generating process and only perform $M$ bootstrap iterations instead of an infinite number.
\item Bagging does not necessarily lead to an improved classifier.
\begin{itemize}
\item Example: binary outcome, $y = 1$ for all values of $x$
\item Consider random classifier $f$ with $\text{P}(\fx = 1) = 0.4$
(independent of $x$)
\item Prediction error for $f$ is 0.6
\item Prediction error for the bagging estimator is 1
\end{itemize}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Random Forests}
  \begin{itemize}
  \item Modification of bagging for trees.
  \item Proposed by Breiman (2001).
  \item Construction of bootstrapped {\bf decorrelated} trees
  \item The variance of the bagging prediction depends on the correlation between the trees $\rho$
      \begin{align*}
        \rho \sigma^2 + \frac{1-\rho}{M} \sigma^2,
      \end{align*}
      where $\sigma^2$ describes the variance of a tree.
  \end{itemize}
\begin{itemize}
\item[$\Rightarrow$] Reduce correlation by randomization in each split.
  Instead of all $p$ features, draw $mtry \le p$ random split candidates.
\item[$\Rightarrow$] Trees are expanded as much as possible, without aggressive early stopping or pruning,
  to increase variance.
\end{itemize}

  \framebreak

  \begin{algorithm}[H]
  \caption*{Random Forest algorithm}
  \begin{algorithmic}[1]
  \State {\bf Input: }A dataset $\D$ of $n$ observations, number $M$ of trees
  in the forest, number $mtry$ of variables to draw for each split
  \For {$m = 1 \to M$}
  \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
  \State Grow tree $\blm$ using $\D^{[m]}$
  \State For each split only consider $mtry$ randomly selected features
  \State Grow tree without early stopping or pruning
\EndFor
\State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to predict on new data.
\end{algorithmic}
\end{algorithm}

\framebreak

\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/forest.png}\end{center}

\framebreak

\begin{itemize}
  \item The following values are recommended for $mtry$:
    \begin{itemize}
    \item Classification: $\lfloor \sqrt{p} \rfloor$
    \item Regression: $\lfloor p/3 \rfloor$
    \end{itemize}

  \item Out-Of-Bag error: On average ca. 1/3 of points are not drawn.
    \begin{align*}
      \P(\text{Obs. not drawn}) &= \left(1 - \frac{1}{n}\right)^n
      \ \stackrel{n \to \infty}{\longrightarrow} \ \frac{1}{e} \approx
      \Sexpr{round(exp(-1), digits = 2)}.
    \end{align*}

  To compute the OOB error, each observation $x$ is predicted only with those trees that did not use $x$ in their fit.
   \item The OOB error is similar to cross-validation estimation. It can also be used for a quicker model selection.
  \end{itemize}

\framebreak
% This pic has been created with powerpoint. To get a good quality pic
% I marked all the elements of the pic and copied them into IrfanView,
% where I can save these elements as a jpg.
\begin{center}
\includegraphics{figure_man/rF_oob_error.jpg}
\end{center}

  \end{vbframe}
\endlecture
