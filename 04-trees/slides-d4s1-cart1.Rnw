% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@


\lecturechapter{13}{Trees}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Trees - Introduction}

Can be used for classification, regression (and much more!)

\begin{blocki}{Zoo of tree methodologies}
\item AID (Sonquist and Morgan, 1964)
\item CHAID (Kass, 1980)
\item CART (Breiman et al., 1984)
\item C4.5 (Quinlan, 1993)
\item Unbiased Recursive Partitioning (Hothorn et al., 2006)
\end{blocki}

\end{vbframe}

\begin{vbframe}{CART}
  \begin{itemize}
    \item Classification and Regression Trees, introduced by Breiman
    \item Binary splits are constructed top-down
    \item Only constant prediction in each leaf
    \begin{figure}
    \centering
      \includegraphics[width= 8cm, height = 5.5cm]{figure_man/labelling_of_tree.png}
    \end{figure}
    \item In the greedy top-down construction, features and split points are selected by exhaustive search.
    \item For each node, one iterates over all features, and for each feature over all split points.
    \item The best feature and split point, which make both created child nodes most pure, measured by a split criterion, are selected.
    \item The procedure then is applied to the child nodes in a recursive manner.
    \end{itemize}
\end{vbframe}


\begin{vbframe}{CART}
\begin{itemize}
\item Trees divide the feature space $\Xspace$ into rectangles and fit simple models (e.g: ~constant) in these:
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(x \in R_m),
  \end{align*}
  where $M$ rectangles $R_m$ are used. $c_m$ is a predicted numerical response, a class label or a class
  distribution.
\end{itemize}

\pagebreak
\textbf{Example for Classification:} Iris-Data
\centering
<<results='hide', fig.height=2.4>>=
# Trees on Iris Data
data("iris")

iristask = makeClassifTask(data = iris[,-(1:2)], target = "Species")
rpart = makeLearner("classif.rpart")
rpart = setHyperPars(rpart, cp = 0, minbucket = 4, maxdepth = 1) #Illustration
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@

<<results='hide', fig.height=3>>=
fancyRpartPlot(model$learner.model, sub = "")
@

\pagebreak
\textbf{Example for Classification:} Iris-Data
<<results='hide', fig.height=2.4>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 4, maxdepth = 2) #2
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@

<<results='hide', fig.height=3>>=
fancyRpartPlot(model$learner.model, sub = "")
@

\pagebreak
\textbf{Example for Classification:} Iris-Data
<<results='hide', fig.height=2.4>>=
rpart = setHyperPars(rpart, cp = 0, minbucket = 4, maxdepth = 3) #3
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@

<<results='hide', fig.height=3>>=
fancyRpartPlot(model$learner.model, sub = "")
@

\pagebreak
\textbf{Example for Regression:}
\vspace{0.5cm}
\begin{columns}[T,onlytextwidth]
\column{0.2\textwidth}
<<out.width='\\textwidth'>>=
modForrester = makeSingleObjectiveFunction(
  name = "Modification Forrester et. all function",
  fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
  par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
  noisy = TRUE
)
set.seed(9)
design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
design$y = modForrester(design)
ordered.design = design[order(design$x),]
rownames(ordered.design) = NULL
kable(ordered.design, digits = 3)
@

\hspace{0.5cm}
\column{0.7\textwidth}
\includegraphics[height = 0.55\textheight]{figure_man/regression_tree}
\end{columns}
\vspace{0.5cm}
Data points (red) were generated from the underlying function (black):

$ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $

\pagebreak
\textbf{Example for Regression:}
<<fig.height=5>>=
regr.task = makeRegrTask(data = design, target = "y")
regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
regr.model = train(regr.rpart, regr.task)
fancyRpartPlot(regr.model$learner.model, sub="")
@
\end{vbframe}


%there were commented frames here in the original file

\begin{vbframe}{CART: Split criteria}

  Let $\Np \subseteq \D$ be a parent node with two child nodes $\Nl$ and $\Nr$.

  Dividing all of the data with respect to the split variable $\xj$ at split point $t$, leads to the following half-spaces:

  \begin{align*}
    \Nl(j,t) &= \{ (x,y) \in \Np: \xj \leq t \} \text{ and } \Nr(j,t) = \{ (x,y) \in \Np: \xj > t \}.
  \end{align*}

  Assume we can measure the impurity of the data in node $\Np$ (usually the label distribution) with function $I(\Np)$.
  This function should return an \enquote{average quantity per observation}.

  Potential splits created in a node $\Np$ are then evaluated via impurity reduction:

    $$  I(\Np) - \frac{|\Nl|}{|\Np|} I(\Nl) - \frac{|\Nr|}{|\Np|} I(\Nr) $$

  $|\Np|$ means number of data points contained in (parent) node $\Np$.

  \framebreak

  \begin{itemize}
  \item {\bf Continuous targets:} mean-squared error / variance

  $$I(\Np) = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} (y - \bar{y}_\Np)^2$$
  with $\bar{y}_\Np = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} y$.

  \vspace{0.3cm}

  Hence, the best prediction in a potential leaf $\Np$ is the mean of the contained y-values, i.e. impurity here is variance of y-values.

  \vspace{0.3cm}

  We can also obtain this by considering:
  \begin{align*}
    \min_{j,t} \left(\min_{c_1} \sum_{(x,y) \in \Nl} (y -
        c_1)^2 + \min_{c_2} \sum_{(x,y) \in \Nr} (y - c_2)^2
    \right).
  \end{align*}
  The inner minimization is solved through:
  $\hat{c}_1 = \bar{y}_1$ and $\hat{c}_2 = \bar{y}_2$

  \framebreak

  \item {\bf Categorical targets ($\mathbf{K}$ categories):} \enquote{Impurity Measures}
    \begin{itemize}
    \item Gini index:
      $$I(\Np) = \sum_{k\neq k'} \pikN_k \pikN_{k'} = \sum_{k=1}^g \pikN_k(1-\pikN_k)$$
    \item misclassification error:
      $$I(\Np) = 1 - \max_k \pikN_k$$
    \item Shannon entropy:
      $$I(\Np) = -\sum_{k=1}^g \pikN_k \log \pikN_k \ ,$$
    \end{itemize}
    where $\pikN_k$ corresponds to the relative frequency of category $k$ of the response.
  \end{itemize}

\framebreak

<<results='hide', fig.height=5>>=
Colors = colorspace::rainbow_hcl(3)
par(mar = c(5.1, 4.1, 0.1, 0.1))
p = seq(0, 1, length.out = 200)
entropy = function(p) (p * log(p) + (1 - p) * log(1 - p))/(2 * log(0.5))
gini = function(p) 2 * p * (1-p)
missclassification = function(p) (1 - max(p, 1 - p))
plot(p, entropy(p), type = "l", col = Colors[2], ylab = "", ylim = c(0, 0.6))
lines(p, gini(p), col = Colors[1])
lines(p, sapply(p, missclassification), col = Colors[3])
legend("topright", c("Gini Index", "Entropy", "Missclassification Error"),
       col = Colors[1:3], lty = 1)
@

\end{vbframe}


\begin{vbframe}{Impurity Measures}
\begin{itemize}
\item In general the three proposed splitting criteria are quite similar.
\item Entropy and Gini index are more sensitive to changes in the node probabilities.
\item \textbf{Example:} two-class problem with 400 obs in each class and two possible splits:
\end{itemize}
\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
\begin{center}
\textbf{Split 1:} \\
\vspace{0.25cm}
<<>>=
class = as.factor(c(rep(0,400), rep(1,400)))
x1 = as.factor(c(rep(0,300), rep(1,400), rep(0,100)))
x2 = as.factor(c(rep(0,600), rep(1,200)))
tab = table(x1, class)
tab2 = table(x2, class)
rownames(tab) = c("Left node", "Right node")
rownames(tab2) = c("Left node", "Right node")
kable(tab, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\column{0.5\textwidth}
\begin{center}
\textbf{Split 2:} \\
\vspace{0.25cm}
<<>>=
kable(tab2, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\end{columns}

\framebreak
\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
\begin{center}
\textbf{Split 1:} \\
\vspace{0.25cm}
<<>>=
kable(tab, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\column{0.5\textwidth}
\begin{center}
\textbf{Split 2:} \\
\vspace{0.25cm}
<<>>=
kable(tab2, row.names = TRUE, col.names = c("class A", "class B"))
@
\end{center}
\end{columns}

\begin{itemize}
\item Both splits produce a misclassification rate of $\frac{200}{800}=0.25$
\item Split 2 produces a pure node and is probably preferable.
\item The average node impurity after a split based on $x_1$ is $0.375$ (Gini) or $0.406$ (Entropy) and $\frac{1}{3}$ (Gini) or $0.344$ (Entropy) after a split based on $x_2$.
% Gini: 6/8 * 2 * 1/3 * 2/3
% entropy: 6/8 * ((1/3 * log(1/3) + 2/3 * log(2/3)) / (2 * log(0.5)))
\item Both criteria prefer split 2 and \textit{choose} the result with a pure node.
\end{itemize}
\framebreak
\begin{itemize}
\item For metric features the exact split points can be ambiguous.
\item If the classes of the response (for classification trees) are completely separated regarding the value range of the feature, a split can be done anywhere between the extreme values of the feature in the classes and the impurity measures stay the same.
\item Look again at the Iris data and the classes \textit{setosa} and \textit{versicolor}:
\end{itemize}
<<results='hide', fig.height=3>>=
# Trees on Iris Data
iristask = makeClassifTask(data = iris[,-(1:2)], target = "Species")

rpart = makeLearner("classif.rpart")
rpart = setHyperPars(rpart, cp = 0, minbucket=4, maxdepth=1) #Illustration
model = train(rpart, iristask)
# Plot
plotLearnerPrediction(rpart, iristask, gridsize = 300, cv = 0) +
  pers_theme +
  ggtitle("Iris Data")
@
\end{vbframe}

\begin{vbframe}{Monotone feature transformations}

Monotone transformations of one or several features will not change the value of the impurity measure, neither the structure of the tree (just the numerical value of the split point).
\vspace{0.5cm}
\begin{columns}[T]
\column{0.49\textwidth}
Original data
<<>>=
x = c(1,2,7,10,20)
y = c(1,1,0.5,10,11)
data = t(data.frame(x = x, y = y))
kable(data)
@

\column{0.49\textwidth}
Data with log-transformed $x$
<<fig.align='right'>>=
log.x = log(x)
data = t(data.frame(log.x, y))
rownames(data) = c("log(x)", "y")
kable(data,digits = 1)
@
\end{columns}
\vspace{0.5cm}
\centering
\includegraphics[width = \textwidth]{figure_man/monotone_trafo.png}
\end{vbframe}

\begin{vbframe}{CART: Stopping-Criteria}
  \begin{itemize}
    \item Minimal number of observations per node, for a split to be tried
    \item Minimal number of observations that must be contained in a leaf
    \item Minimal increase in goodness of fit that must be reached for a split to be
      tried
    \item Maximum number of levels for your tree
  \end{itemize}
\end{vbframe}

\begin{vbframe}{CART: Overfitting}
  \begin{itemize}
  \item The CART-Algorithm could just continue until there is a single observation in each node

  \item [$\Rightarrow$] Complexity (and hence the danger of overfitting)
  increases with the number of splits / levels / leafs
  \end{itemize}
\end{vbframe}

\begin{vbframe}{CART: Categorical Predictors}
  \begin{itemize}
  \item For a nominal scaled feature with $Q$ categories,
  there are $2^{Q-1}-1$ possible partitions of the $Q$ values into two groups:
    \begin{itemize}
    \item There are $2^Q$ ways to assign $Q$ distinct values to the left or right node.
    \item Two of these configurations lead to an empty node, while the other one contains all observations.
    Discarding these configurations leads to $2^Q -2$ possible partitions.
    \item Symmetry halves the number of possible partitions: $\frac{1}{2}(2^Q - 2) = 2^{Q-1} - 1$
    \end{itemize}
    $\Rightarrow$ computations become prohibitive for large values of $Q$
  \item But for regression with squared loss and binary classification shortcuts exist.
  \end{itemize}

  \pagebreak

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of 1-outcomes for each category of the feature.
  \item Sort the categories according to these proportions.
  \item The feature can then be treated as if it were an ordered categorical feature ($Q-1$ possible splits).
  \end{enumerate}

  \vspace{0.3cm}

<<fig.height=3.4>>=
set.seed(1234)
# generate data (one categorcal variable with 4 categories, 0-1 response)
data = data.frame(category = sample(c("A", "B", "C", "D"), size = 150,
  replace = TRUE, prob = c(0.2, 0.1, 0.4, 0.3)),
  y = sample(c(0,1), size = 150, replace = TRUE, prob = c(0.3, 0.7)))

# calculates proportion of 1-outcomes and plot
subset.data = subset(data, y == 1)
plot.data = data.frame(prop.table(table(subset.data$category)))
colnames(plot.data) = c("Category", "Frequency")
p1 = ggplot(data = plot.data, aes(x = Category, y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Frequency of class 1") + xlab("Category of feature")

# sort by proportions
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Frequency), y = Frequency, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Frequency of class 1") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")


# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Frequency))
vline.level = 'C'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@

\pagebreak

  \begin{itemize}
  \item This procedure obtains the optimal split for entropy and Gini index.
  \item This result also holds for regression trees (with squared error loss) -- the categories are ordered by increasing mean of the outcome (see next slide).
  \item The proofs are not trivial and can be found here:
    \begin{itemize}
    \item for 0-1 responses:
      \begin{itemize}
      \item Breiman, 1984, Classification and Regression Trees.
      \item Ripley, 1996, Pattern Recognition and Neural Networks.
      \end{itemize}
    \item for continuous responses:
      \begin{itemize}
      \item Fisher, 1958, On grouping for maximum homogeneity.
      \end{itemize}
    \end{itemize}
  \item Such simplifications are not known for multiclass problems.
  %\item The Algorithm prefers categorical variables with a large value
  %of categories $Q$
  \end{itemize}

\pagebreak

For continuous responses, in each node:
  \begin{enumerate}
  \item Calculate the mean of the outcome in each category.
  \item Sort the categories by increasing mean of the outcome.
  \item The feature can then be treated as if it were an ordered categorical feature ($Q-1$ possible splits).
  \end{enumerate}

\vspace{0.3cm}

<<fig.height=3.5>>=
set.seed(1234)
# generate data (one categorcal variable with 4 categories, 0-1 response)
data = rbind(data.frame(category = "A", y = runif(30, 5, 7.5)),
  data.frame(category = "B", y = runif(15, 6, 12)),
  data.frame(category = "C", y = runif(60, 5, 20)),
  data.frame(category = "D", y = runif(45, 1, 6)))

# calculate the mean of the outcome in each category
plot.data = aggregate(y ~ category, data = data, FUN = mean)
colnames(plot.data) = c("Category", "Mean")

# plot the categories wrt the mean of the outcome in each category
p1 = ggplot(data = plot.data, aes(x = Category, y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ggtitle("1)") + ylab("Mean of outcome") + xlab("Category of feature")

# sort by increasing mean of the outcome
p2.pre = ggplot(data = plot.data, aes(x = reorder(Category, Mean), y = Mean, fill = Category)) +
  geom_bar(stat = "identity")  + theme(legend.position = "none") +
  ylab("Mean of outcome") + xlab("Category of feature")
p2 = p2.pre + ggtitle("2)")

# decision tree to draw a vertical line where the spit is being made
mod = rpart::rpart(y ~., data = data)
lvls = levels(reorder(plot.data$Category, plot.data$Mean))
vline.level = 'B'
p3 = p2.pre +  geom_vline(xintercept = which(lvls == vline.level) - 0.5, col = 'red', lwd = 1, linetype = "dashed") +
  ggtitle("3)")
grid.arrange(p1, p2, p3, ncol = 3)
@
\end{vbframe}

\endlecture
