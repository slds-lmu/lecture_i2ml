\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
    \begin{beamercolorbox}[center]{postercolumn}
    
    \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{
    \begin{myblock}{Classification}
						Given a \textbf{classification problem:}

						\begin{eqnarray*} 
              & x \in \Xspace \quad & \text{feature vector}\\ 
              & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\ 
              & \D = \Dset & \text{observations of $x$ and $y$} 
            \end{eqnarray*}
						
						\vspace*{1ex}
						
						\textbf{Classification:} to construct $g$ discriminant functions: $f_1(\xv), \ldots, f_g(\xv)$, 
            so that we choose our class as $h(\xv) = \argmax_{k \in \gset} \fkx[k]$
						
						\begin{codebox}
							\textbf{Linear Classifier}
						\end{codebox}
						If discriminants $\fkx$ can be written as affine linear functions, 
            possibly through a rank-preserving, monotone transformation $g$:
            $$g(\fkx) = \bm{w}_k^\top \xv + b_k,$$
            we will call the classifier \textbf{linear}.

            \vspace*{1ex}
            
            If there exists a linear classifier that perfectly separates the classes of some dataset, the data are called \textbf{linearly separable}.
						
            \vspace*{1ex}

            \textbf{Note: }Linear classifiers can represent \textbf{non-linear} decision boundaries in the original input space if we use derived features like higher order interactions, polynomial features, etc.

			    \vspace*{1ex}
			
						\begin{codebox}
							 \textbf{Binary classification}
						\end{codebox}
            Only 2 classes, can use a single discriminant function $\fx = f_{1}(\xv) - f_{2}(\xv)$.
          \end{myblock}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myblock}{Generative Approach}
\begin{codebox}
\textbf{Generative approach} models $\pdfxyk$, usually by making some
\end{codebox}

\begin{codebox}
assumptions about the structure of these distributions and employs the
\end{codebox}

\begin{codebox}
Bayes theorem:
  \end{codebox}
\hspace*{1ex}$\pikx = \postk \propto \pdfxyk \pik$. It allows the computation of \hspace*{1ex}$\pikx$.

\begin{codebox}
  Examples:
\end{codebox}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
    \item Linear discriminant analysis (LDA)
  \end{itemize}
					\end{myblock}\vfill
          }
        \end{minipage}
          \end{beamercolorbox}
        \end{column}
	
	\begin{column}{.31\textwidth}
	\begin{beamercolorbox}[center]{postercolumn}
	
	\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{}
  
  \begin{itemize}[$\bullet$]    
  \setlength{\itemindent}{+.3in}
    \item Quadratic discriminant analysis (QDA)
    \item Naive Bayes
  \end{itemize}
  
  \vspace*{1ex}
  
  \begin{codebox}
  \textbf{Linear Discriminant Analysis (LDA): }follows a generative approach,
  \end{codebox}
  
  \begin{codebox}
  each class density is modeled as a \emph{multivariate Gaussian} with equal
  \end{codebox}
  
  \begin{codebox}
  covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$.
  \end{codebox}
  
  \hspace*{1ex}Parameters $\theta$ are estimated in a straight-forward manner by estimating \hspace*{1ex}$\hat{\pi}_k$, $\hat{\mu}_k$, $\hat{\Sigma}$.
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
      \item Each class fit as a Gaussian distribution over the feature space
      \item Different means but same covariance for all classes
      \item Rather restrictive model assumption.
  \end{itemize}

  \vspace*{1ex}

  \begin{codebox}
  \textbf{Quadratic Discriminant Analysis (QDA): }is a direct generalization
  \end{codebox}
  
  \begin{codebox}
  of LDA, where the class densities are now Gaussians with unequal 
  \end{codebox}
  
  \begin{codebox}
  covariances $\Sigma_k$.
  \end{codebox}
  
  \hspace*{1ex}Parameters $\theta$ are estimated in a straight-forward manner by estimating \hspace*{1ex}$\hat{\pi}_k$, $\hat{\mu}_k$, $\hat{\Sigma_k}$.
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
      \item Covariance matrices can differ over classes.
      \item Yields better data fit but also requires estimation of more parameters.
  \end{itemize}
  
  \vspace*{1ex}
  
  \begin{codebox}
  \textbf{Naive Bayes classifier: }A "naive" conditional independence assumption
  \end{codebox}

  \begin{codebox}
  is made: the features given the category y are conditionally independent of
  \end{codebox}
  
  \begin{codebox}
  each other
  \end{codebox}
  
  \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
      \item Covariance matrices can differ over both classes but assumed to be diagonal.
      \item Assumption of uncorrelated features. Often performs well despite this usually wrong assumption.
      \item Easy to deal with mixed features (metric and categorical)
  \end{itemize}
  \end{myblock}
  
  \begin{myblock}{Discriminant Approach}
  \begin{codebox}
  \textbf{Discriminant approach: }tries to optimize the discriminant functions
  \end{codebox}
  \end{myblock}
  }
  \end{minipage}
		\end{beamercolorbox}
	\end{column}
	
	
	\begin{column}{.31\textwidth}
	\begin{beamercolorbox}[center]{postercolumn}
	
	\begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{}
    
  \begin{codebox}
  directly, usually via empirical risk minimization:
  \end{codebox}
    
  $ \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.$
  
  \begin{codebox}
    Examples:
  \end{codebox}
    
    \begin{itemize}[$\bullet$]     
    \setlength{\itemindent}{+.3in}
    \item Logistic/Softmax regression
    \item KNN
    \end{itemize}
  
  \begin{codebox}
  \textbf{Logistic Regression}
  \end{codebox}
  Directly modeling the posterior probabilities $\pix$ of the labels is \textbf{logistic regression}.
  
  Encode $y \in \{0, 1\}$ and use ERM. Then model: \[ \pix = \post = \theta^T x\].
  To avoid predicted probabilities $\pix \not\in [0,1]$, logistic regression \enquote{squashes} the estimated linear scores $\theta^T x$ to \hspace*{1ex}$[0,1]$ through the \textbf{logistic function} $s$:
  $$ \pix = \frac{\exp\left(\theta^Tx\right)}{1+\exp\left(\theta^Tx\right)} = \frac{1}{1+\exp\left(-\theta^Tx\right)} = s\left(\theta^T x\right).$$
  The inverse $s^{-1}(\pi) = \log\left(\frac{\pi}{1 - \pi}\right)$ is called \textbf{logit} or \textbf{log odds}.

  \begin{codebox}
  \textbf{Cross Entropy/Bernoulli/Log-loss}
  \end{codebox}
  Minimizing it refers to maximizing the probabilities of logistic regression.

  \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
      \item $\Yspace = \setzo$ with score function: $\Lyf = - y \cdot f + \log(1 + \exp(f))$ 
      \item $\Yspace = \setmp$ with score function: $\Lyf = \log(1+\exp(-yf))$
      \item $\Yspace = \setzo$ with probability function: $\Lpiy = - y \log \left(\pi\right) - (1 - y) \log \left(1 - \pi\right)$
      \item $\Yspace = \setmp$ with probability function: $\Lpiy = - \frac{1 + y}{2} \log\left(\pi\right) - \frac{1 - y}{2} \log\left(1 - \pi\right)$
  \end{itemize}

  \begin{codebox}
  \textbf{Softmax}
  \end{codebox}
  A generalization of the logistic function. It \enquote{squashes} a g-dimensional real-valued vector $z$ to a vector of the same dimension,
  with every entry in the range [0, 1] and all entries adding up to 1.

  \emph{Softmax} is defined on a numerical vector $z$: $ s_k(z) = \frac{\exp(z_k)}{\sum_{j}\exp(z_j)} $.
  
\end{myblock}
}

\end{minipage}
\end{beamercolorbox}
\end{column}

\end{columns}
\end{frame}
\end{document}
