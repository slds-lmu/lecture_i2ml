\documentclass{beamer}
% These are loaded from relative file paths which depend on where the cheatsheets are stored
% Currently these are at two different directory levels, so we can reduce this copypasta 
% once they are always at the same level
\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{../style/beamerposter}
% usepackage{beamerthemeNAME} is equivalent to usetheme{NAME}
% But usepackage works with relative file paths, usetheme does not (as easily at least)
\mode<presentation>{\usepackage{../style/beamerthememlr}}
\input{../style/preamble-cheatsheets}

\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-trees.tex}

\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{

					\begin{myblock}{ML Overview}

						\begin{codebox}
							\textbf{Machine Learning}
						\end{codebox}
            A computer program is said to learn from experience E with respect to some task T and some performance measure P, 
            if its performance on T, as measured by P, improves with experience E.
            \begin{itemize}[$\bullet$] 
              \setlength{\itemindent}{+.3in}
                          \item Supervised Learning (SL): Learns from labeled data
                          \item Unsupervised Learning: Learns from unlabeled data
                          \item Reinforcement Learning: Learns from feedback
                         \end{itemize}

              \begin{codebox}
                \textbf{Components of SL}
              \end{codebox}
              \textbf{Learning = Hypothesis Space + Risk + Optimization}
              \begin{itemize}[$\bullet$] 
                \setlength{\itemindent}{+.3in}
                \item \textbf{Hypothesis Space:} Defines and restricts what kind of model 
                $f$ can be learned from the data.
                \item \textbf{Risk (Loss + Regularization):} Quantifies how well a specific model performs on a given 
                data set.
                \item \textbf{Optimization:} Defines how to search for the best model in the 
                hypothesis space, i.e., the model with the smallest risk.
              \end{itemize}

\end{myblock}


\begin{myblock}{Data}

$$\D = \Dset \in \defAllDatasetsn$$
\begin{itemize}[$\bullet$] 
  \setlength{\itemindent}{+.3in}
  \item $\greekxi$: features, $y^{(i)}$: target
  \item $\Xspace$: input space, usually $\Xspace \subset \R^p$
  \item $\Yspace$: output / target space
  \item \(\xyi\) $\in \Xspace\times \Yspace$:  \(i\)-th observation
\end{itemize}

\begin{codebox} 
\textbf{Categorical Data Encoding:}
\end{codebox}
Expand the representation of a variable $x$
with $k$ mutually exclusive categories from a scalar 
to a length-$\tilde k$ vector with at most one 
element being 1, and 0 otherwise: $\bm{o}(x)_j =\I(x = j) \in \{0,1\}$.

\begin{itemize}[$\bullet$] 
  \setlength{\itemindent}{+.3in}
    \item \textbf{One-hot encoding}: $\tilde k = k$ dummies, so \textit{exactly 
    one} element is 1. \\
    E.g., $x \in \{ a, b, c\} \mapsto \bm{o}(x) = (x_a, x_b, x_c)$, with 
    $x_a = x_b = 0, x_c = 1$ and $\bm{o}(x) = (0, 0, 1)$ for $x = c$.
    \item \textbf{Dummy encoding}: $\tilde k = k - 1$ dummies, so 
    \textit{at most one} element is 1, removing the redundancy of one-hot 
    encoding. \\
    E.g., $x \in \{ a, b, c\} \mapsto \bm{o}(x) = (x_a, x_b)$ for reference 
    category $c$, with $x_a = x_b = 0$ and $\bm{o}(x) = (0, 0)$ for $x = c$.
\end{itemize} 

\end{myblock}\vfill
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

\begin{myblock}{ }
\begin{codebox}
  \textbf{Data-Generating Process: }
  \end{codebox}
  \begin{itemize}[$\bullet$] 
    \setlength{\itemindent}{+.3in}
    \item Assume the observed data $\D$ to be generated by a process that can
    be characterized by some probability distribution $\Pxy,$ defined on 
    $\Xspace \times \Yspace$.
    \item Assume data to be drawn \emph{i.i.d.} (\textbf{i}ndependent and \textbf{i}dentically 
      \textbf{d}istributed) from $\Pxy$. 
    \item Often, distributions are characterized by a parameter vector 
      $\thetav \in \Theta$. We then write $\pdfxyt$.
    \end{itemize} 
  \end{myblock}

  \begin{myblock}{Supervised Tasks}

      \begin{itemize}[$\bullet$] 
      \setlength{\itemindent}{+.3in}
                  \item \textbf{Regression:} $\Yspace \subseteq \mathbb{R}$ Numerical target
                  \item \textbf{Classification:} $\Yspace = \{C_1,...,C_g\}$ Categorical target
                 \end{itemize}


  \begin{codebox}
  \textbf{Goal:}
  \end{codebox}
  \begin{itemize}[$\bullet$] 
  \setlength{\itemindent}{+.3in}
  \item \textbf{Learning to predict}. Don't care how
        model is structured. Want an accurate predictor for new data.

    \item \textbf{Learning to explain}. Model is only a mean to 
        better understand the inherent relationship in the data.
        Might not use the learned model on new observations.
\end{itemize}

  \end{myblock}

  \begin{myblock}{Model and Parameter}
    A \textbf{model} (or \textbf{hypothesis}) 
    $f : \Xspace \rightarrow \R^g$
    is a function that maps feature vectors to predicted target values.
  
    The set of functions defining a specific model class is called a 
    \textbf{hypothesis space}: $\Hspace = \{f: f \text{ belongs to a certain functional family}\}$

    Usually construct the space as parametrized family of functions:
    $\Hspace = \{f_{\thetav}: f_{\thetav} \text{ belongs to a certain functional family parameterized by } \thetav\}$

  \end{myblock}

  \begin{myblock}{Learner}
    \textbf{Learner} is the algorithm for finding model $f$, is a mean of picking the best element from the hypothesis space $\Hspace$
  for given training data.
  It maps training data $\D \in \allDatasets$ plus a vector of hyperparameter control settings $\lamv \in \Lam$ to a model:
  $ \ind: \preimageInducerShort \rightarrow \Hspace$. 
  Practically, with parameters $\ind: \preimageInducerShort \rightarrow \Theta$.
\end{myblock}

}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

  \begin{myblock}{Losses and Risk Minimization}
  
  \begin{codebox}
  \textbf{Loss}	
  \end{codebox}
  \textbf{Loss function} $\Lxy$ quantifies the quality of the prediction $\fx$ of a single observation $\xv$:
    $L: \Yspace \times \R^g \to \R$.


  \begin{codebox}
  \textbf{Risk}
  \end{codebox}
  Theoretical \textbf{risk} associated with a certain hypothesis $f \in \Hspace$ measured by a loss function $\Lxy$ is the \textbf{expected loss}
  $$ \riskf := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy $$
  
  \textbf{Problem}: $\Pxy$ is unknown, thus minimizing the theoretical $\riskf$ directly is not feasible.

  \begin{codebox}
  \textbf{Empirical Risk}
  \end{codebox}
  Simply sum up all pointwise losses: $ \riskef = \sumin \Lxyi $.\\
  Can also be defined as an average loss: $ \riskeb(f) = \frac{1}{n}\sumin \Lxyi $.\\
  With parameters: $\risket = \sumin \Lxyit$.\\
  \textbf{Empirical Risk Minimization} (ERM): $\thetavh = \argmin_{\thetav \in \Theta} \risket$.

  \end{myblock}
  
  \begin{myblock}{Optimization}

  \begin{codebox} 
  \textbf{ERM optimization:}
  \end{codebox}
  $$\thetavh  = \argmin_{\thetav \in \Theta} \risket $$
  
  Global minimum $\thetavh$: 
  \[
  \forall \thetav \in \Theta :\quad \riske(\thetavh) \leq \risket .
  \]

  Local minimum $\thetavh$:
  \[
  \exists \epsilon > 0\; \forall \thetav \text{ with } \left\Vert\thetavh - \thetav\right\Vert < \epsilon: \quad \riske(\thetavh) \leq \risket
  \]

  \end{myblock}
  }
  
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}
