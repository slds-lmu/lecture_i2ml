\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-eval.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of supervised machine learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
          \begin{myblock}{Performance Evaluation}
            \begin{codebox}
							\textbf{Generalization Error:}
						\end{codebox}

            \textbf{Generalization error (GE)} is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.\\
           For a fixed model: $\GEfL := \E \left[ \Lyfhx) \right]$, i.e. the expected error the model makes for data $(\xv, y) \sim \Pxy$.
 If $(\xv, y) \in \Dtrain$, estimator will be biased via overfitting the training data.
 Thus, we estimate the GE using unseen data $(\xv, y) \in \Dtest$:
 $$\GEh(\fh, L) := \frac{1}{m}\sum_{(\xv, y) \in \Dtest} \left[ \Lyfhx \right]$$
$\Lyfhx$ indicates how good the target matches our prediction.

          \begin{codebox}
            \textbf{Inner vs. Outer Loss:}
          \end{codebox}
          Inner loss is used in learning and outer loss is used in evaluation. 
          Optimally inner loss should always match outer loss. 
          But this is not always possible because some losses are hard to optimize.

					\end{myblock}

          \begin{myblock}{Training and Test Error}
            \begin{codebox}
        \textbf{Training Error: }
      \end{codebox}
        Training error: Estimate GE on training set.
        
        \textbf{Problems of training error: }
        \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item Unreliable and overly optimistic estimator of future performance evaluation.
        \item There are interpolators, which are not necessarily good as they will also interpolate the noise.
        \item Goodness-of-fit measures like $R^2$, likelihood, AIC, BIC etc are based on the training error.
        \end{itemize}

        \begin{codebox}
          \textbf{Test Error: }
          \end{codebox}
        Test error: Estimate GE on test set. Good way to estimate future performance, given that test data is i.i.d. compared to training data.
          
          \textbf{Problems of test error:}
          \begin{itemize}[$\bullet$]
          \setlength{\itemindent}{+.3in}
          \item The estimator will suffer from high variance and be less reliable if the test set is too small.
          \item Sometimes the test set is large, but one of the two classes is small.
          \end{itemize}
          


      \end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
	
	\begin{column}{.31\textwidth}
  \begin{beamercolorbox}[center]{postercolumn}
  \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{

    \begin{myblock}{Overfitting vs. Underfitting }
      \textbf{Overfitting:}
      \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item The model fits the training data too well, so it also models patterns in the data that are not actually true, like noise.
        \item Small training error, at the cost of high test error.
       \item Low bias, high variance.
       \end{itemize}
       \textbf{Avoid overfitting:} Use less complex models, get more and better data, early stopping, regularization etc.
 
       \textbf{Underfitting:} 
       \begin{itemize}[$\bullet$]
         \setlength{\itemindent}{+.3in}
         \item The model is too simple to learn the underlying structure of the data.
         \item High training error and high test error.
        \item High bias, low variance.
        \end{itemize}

     \end{myblock}

    \begin{myblock}{Resampling}

      While $\GEh(\fh, L) = \frac{1}{m}\sum_{(\xv, y) \in \Dtest} \left[ \Lyfhx \right]$ is unbiased, with a small $m$ it has high variance.
      We have two options to decrease the variance:
      \begin{itemize}[$\bullet$]
      \setlength{\itemindent}{+.3in}
      \item Increase $m$.
      \item Compute $\GEh(\fh, L)$ for multiple test sets and aggregate them.
      \end{itemize}

        With a finite amount of data, increasing $m$ would mean to decrease the size of the training data.
        Thus, we focus on using multiple ($B$) test sets: 
        $$\JJ = \JJset.$$
        where we compute $\GEh(\fh, L)$ for each set and aggregate the estimates.
        These $B$ sets are generated through \textbf{resampling}.
            
            \begin{codebox}
            \textbf{Cross-validation: }
            \end{codebox}
            Split the data into $k$ roughly equally-sized partitions.
            Use each part once as test set and join the $k-1$ others for training,
            obtain $k$ test errors and average.
            For unbalanced data, \textbf{stratification} is used.
        
            \begin{codebox}
            \textbf{Bootstrapping: }
            \end{codebox}
            Randomly draw $B$ training sets of size $n$ with replacement from the original training set $\Dtrain$.
        
            \begin{codebox}
            \textbf{Subsampling: }
            \end{codebox}
              Repeated hold-out with averaging. Similar to bootstrap, but draws without replacement.
            
            \end{myblock}



  }
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  \begin{column}{.31\textwidth}
  \begin{beamercolorbox}[center]{postercolumn}
  \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{
            \begin{myblock}{Evaluation Metrics}
            % \end{myblock}
    % \begin{myblock}{}
    \begin{codebox}
    \textbf{Confusion Matrix: }
  \end{codebox}
  \begin{columns} 
    \begin{column}{0.5\textwidth} 
      Many metrics can be computed from this, e.g., precision (PPV) and recall (TPR). $F_1$ score balances 2 conflicts:
      \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item Maximize PPV.
        \item Maximize TPR.
        \end{itemize}
        $\rho_{F_1} = 2 \cdot \cfrac{\rho_{PPV} \cdot \rho_{TPR}}{\rho_{PPV} + \rho_{TPR}}$.
    \end{column} 
    \begin{column}{0.5\textwidth} 
      \begin{center}
        \scriptsize
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{cc||cc|c}
            & & \multicolumn{2}{c|}{\bfseries True Class $y$} & \\
            & & $+$ & $-$ & \\ 
            \hline \hline
            \bfseries Pred.     & $+$ & TP & FP & $\rho_{PPV} = \frac{\text{TP}}{\text{TP} + \text{FP}}$\\
                      $\yh$ & $-$ & FN & TN & $\rho_{NPV} = \frac{\text{TN}}{\text{FN} + \text{TN}}$\\
            \hline
            & & $\rho_{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$ & $\rho_{TNR} = \frac{\text{TN}}{\text{FP} + \text{TN}}$ & $\rho_{ACC} = \frac{\text{TP}+ \text{TN}}{\text{TOTAL}}$
        \end{tabular}
        \renewcommand{\arraystretch}{1}
        \end{center}
    \end{column} 
    \end{columns} 
    \end{myblock}
\begin{myblock}{Evaluation Plots}
    \begin{codebox}
    \textbf{Receiver Operating Characteristic (ROC) Curve: }
    \end{codebox}
      \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item Plots FPR vs. TPR
        \item Start with threshold $c = 1$ in $(0, 0)$; predict everything as negative.
        \item Iterate through all possible thresholds $c$ and proceed for each observation $x$ as follows:
        \begin{itemize}
          \item If $x$ is positive, move TPR $\frac{1}{n_+}$ up, as we have one TP more.
          \item If $x$ is negative, move FPR $\frac{1}{n_-}$ right, as we have one FP more.
        \end{itemize}
        \end{itemize}

          \textbf{ROC Curve Properties: }
          \begin{itemize}[$\bullet$]
            \setlength{\itemindent}{+.3in}
            \item The closer the curve to the top-left corner, the better.
            \item If ROC curves cross, a different model might be better in different parts of the ROC space. 
            \end{itemize}

            \begin{codebox}
              \textbf{Area Under the Curve (AUC): }
            \end{codebox}
            AUC $\in [0,1]$ is a single metric to evaluate scoring classifiers, independent of the chosen threshold.
            AUC = 1: perfect classifier; AUC = 0.5: random, non-discriminant classifier; AUC = 0: perfect, with inverted labels.

  %           \textbf{Mann-Whitney-U test:} A \textbf{non-parametric hypothesis test} on the difference in location between two samples $X_1$, $X_2$ of sizes $n_1$ and $n_2$.\\
  %           Test statistic estimates the probability of a random sample from $X_1$ ranking higher than one from $X_2$ ($R_1$ denoting the sum of ranks of the $x_{1,i}$):
  % $$ U = \frac{1}{n_1 n_2} \sum_{i = 1}^{n_1} \sum_{j = 1}^{n_2} \I[x_{1,i} > x_{2,j}] = R_1 - \cfrac{n_1(n_1 + 1)}{2}$$
  %           AUC is $U$ normalized to the unit square: $\text{AUC} = \cfrac{U}{\np \cdot \nn}.$

            \textbf{Partial AUC (pAUC):} Area under the ROC curve in a limited region of interest, e.g. TPR $> 0.8$ or FPR $< 0.2$.\\

\textbf{Multi-class AUC:} $\text{AUC}(k ~|~ \ell)$ for classes $k$ (pos) and $\ell$ (neg).
$$\text{AUC}_{MC} = \tfrac{1}{g(g - 1)} \sum_{k \neq \ell} \text{AUC}(k | \ell) \in [0, 1].$$

\begin{codebox}
  \textbf{Precision-Recall (PR) Curves: }
\end{codebox}
Slightly changed ROC plot. Simply plot precision and recall, instead of TPR-FPR.
  \end{myblock}
  }
  \end{minipage}
  \end{beamercolorbox}
  \end{column}

\end{columns}
\end{frame}
\end{document}
