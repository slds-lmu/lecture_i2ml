\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-eval.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
          \begin{myblock}{Performance Evaluation}
            \begin{codebox}
							\textbf{Generalization Error:}
						\end{codebox}

            \textbf{Generalization error (GE)} is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.\\
           For a fixed model: $\GEfL := \E \left[ \Lyfhx) \right]$, i.e. the expected error the model makes for data $(\xv, y) \sim \Pxy$.
 If $(\xv, y) \in \Dtrain$, estimator will be biased via overfitting the training data.
 Thus, we estimate the GE using unseen data $(\xv, y) \in \Dtest$:
 $$\GEh(\fh, L) := \frac{1}{m}\sum_{(\xv, y) \in \Dtest} \left[ \Lyfhx \right]$$
$\Lyfhx$ indicates how good the target matches our prediction.

          \begin{codebox}
            \textbf{Inner vs. Outer Loss:}
          \end{codebox}
          Inner loss is used in learning and outer loss is used in evaluation. 
          Optimally inner loss should always match outer loss. 
          But this is not always possible because some losses are hard to optimize.

					\end{myblock}

          \begin{myblock}{Training and Test Error}
            \begin{codebox}
        \textbf{Training Error: }
      \end{codebox}
        Training error: average error over the same data set fitted on.
        
        \textbf{Problems of training error: }
        \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item Unreliable and overly optimistic estimator of future performance evaluation.
        \item There are interpolators, which are not necessarily good as they will also interpolate the noise.
        \item Goodness-of-fit measures like $R^2$, likelihood, AIC, BIC etc are based on the training error.
        \end{itemize}

        \begin{codebox}
          \textbf{Test Error: }
          \end{codebox}
          Test is a good way to estimate future performance evaluation, given that the test data is i.i.d. compared to the data applied to the model.
          
          \textbf{Problems of test error:}
          \begin{itemize}[$\bullet$]
          \setlength{\itemindent}{+.3in}
          \item The estimator will suffer from high variance and be less reliable if the test set is too small.
          \item Sometimes the test set is large, but one of the two classes is small.
          \end{itemize}
          
          \begin{codebox}
            \textbf{Overfitting vs. Underfitting: }
            \end{codebox}

      \end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
	
	\begin{column}{.31\textwidth}
  \begin{beamercolorbox}[center]{postercolumn}
  \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{

    \begin{myblock}{ }

      \textbf{Overfitting:}
      \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item The model fits the training data too well that it also models patterns in the data that are not actually true, like noise.
        \item Small training error, at cost of test high error.
       \item Low bias, high variance.
       \end{itemize}
       \textbf{Avoid overfitting:} Use less complex models, get more and better data, early stopping, regularization etc.
 
       \textbf{Underfitting:} 
       \begin{itemize}[$\bullet$]
         \setlength{\itemindent}{+.3in}
         \item The model is too simple to learn the underlying structure of the data.
         \item High training error and high test error.
        \item High bias, low variance.
        \end{itemize}

     \end{myblock}

    \begin{myblock}{Resampling}

      While $\GEh(\fh, L) = \frac{1}{m}\sum_{(\xv, y) \in \Dtest} \left[ \Lyfhx \right]$ is unbiased, with a small $m$ it has high variance.
      We have two options to decrease the variance:
      \begin{itemize}[$\bullet$]
      \setlength{\itemindent}{+.3in}
      \item Increase $m$.
      \item Compute $\GEh(\fh, L)$ for multiple test sets and aggregate them.
      \end{itemize}

        With a finite amount of data, increasing $m$ would mean to decrease the size of the training data.
        Thus, we focus on using multiple ($B$) test sets: 
        $$\JJ = \JJset.$$
        where we compute $\GEh(\fh, L)$ for each set and aggregate the estimates.
        These $B$ sets are generated through \textbf{resampling}.
            
            \begin{codebox}
            \textbf{Cross-validation: }
            \end{codebox}
            Split the data into $k$ roughly equally-sized partitions.
            Use each part once as test set and join the $k-1$ others for training,
            obtain $k$ test errors and average.
            For unbalanced data, \textbf{stratification} is used.
        
            \begin{codebox}
            \textbf{Bootstrapping: }
            \end{codebox}
            Randomly draw $B$ training sets of size $n$ with replacement from the original training set $\Dtrain$.
        
            \begin{codebox}
            \textbf{Subsampling: }
            \end{codebox}
              Repeated hold-out with averaging, a.k.a. monte-carlo CV. Similar to bootstrap, but draws without replacement.
            
            \end{myblock}

            \begin{myblock}{Evaluation Curves}
            \end{myblock}

  }
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  \begin{column}{.31\textwidth}
  \begin{beamercolorbox}[center]{postercolumn}
  \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{

    \begin{myblock}{}
    \begin{codebox}
    \textbf{Confusion Matrix: }
  \end{codebox}
  \begin{columns} 
    \begin{column}{0.5\textwidth} 
      \textbf{$F_1$ score} balances 2 conflicts:
      \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item Maximize positive predictive value.
        \item Maximize true positive rate.
        \end{itemize}
        $\rho_{F_1} = 2 \cdot \cfrac{\rho_{PPV} \cdot \rho_{TPR}}{\rho_{PPV} + \rho_{TPR}}$.
    \end{column} 
    \begin{column}{0.5\textwidth} 
      \begin{center}
        \scriptsize
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{cc||cc|c}
            & & \multicolumn{2}{c|}{\bfseries True Class $y$} & \\
            & & $+$ & $-$ & \\ 
            \hline \hline
            \bfseries Pred.     & $+$ & TP & FP & $\rho_{PPV} = \frac{\text{TP}}{\text{TP} + \text{FP}}$\\
                      $\yh$ & $-$ & FN & TN & $\rho_{NPV} = \frac{\text{TN}}{\text{FN} + \text{TN}}$\\
            \hline
            & & $\rho_{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$ & $\rho_{TNR} = \frac{\text{TN}}{\text{FP} + \text{TN}}$ & $\rho_{ACC} = \frac{\text{TP}+ \text{TN}}{\text{TOTAL}}$
        \end{tabular}
        \renewcommand{\arraystretch}{1}
        \end{center}
    \end{column} 
    \end{columns} 

    \begin{codebox}
    \textbf{Receiver Operating Characteristic (ROC): }
    \end{codebox}
      \begin{itemize}[$\bullet$]
        \setlength{\itemindent}{+.3in}
        \item Rank test observations on decreasing score.
        \item Start with $c = 1$ in $(0, 0)$; predict everything as negative.
        \item Iterate through all possible thresholds $c$ and proceed for each observation $x$ as follows:
        \begin{itemize}
          \item If $x$ is positive, move TPR $\frac{1}{n_+}$ up, as we have one TP more.
          \item If $x$ is negative, move FPR $\frac{1}{n_-}$ right, as we have one FP more.
        \end{itemize}
        \end{itemize}

          \textbf{ROC Properties: }
          \begin{itemize}[$\bullet$]
            \setlength{\itemindent}{+.3in}
            \item The closer the curve to the top-left corner, the better.
            \item If ROC curves cross, a different model might be better in different parts of the ROC space. 
            \end{itemize}

            \begin{codebox}
              \textbf{Area Under the Curve (AUC): }
            \end{codebox}
            AUC $\in [0,1]$ is a single metric to evaluate scoring classifiers, independent of the chosen threshold.
            AUC = 1: perfect classifier; AUC = 0.5: random, non-discriminant classifier; AUC = 0: perfect, with inverted labels.

            \textbf{Mann-Whitney-U test:} A \textbf{non-parametric hypothesis test} on the difference in location between two samples $X_1$, $X_2$ of sizes $n_1$ and $n_2$.\\
            Test statistic estimates the probability of a random sample from $X_1$ ranking higher than one from $X_2$ ($R_1$ denoting the sum of ranks of the $x_{1,i}$):
  $$ U = \frac{1}{n_1 n_2} \sum_{i = 1}^{n_1} \sum_{j = 1}^{n_2} \I[x_{1,i} > x_{2,j}] = R_1 - \cfrac{n_1(n_1 + 1)}{2}$$
            AUC is $U$ normalized to the unit square: $\text{AUC} = \cfrac{U}{\np \cdot \nn}.$

            \textbf{Partial AUC (pAUC):} Area under the ROC curve in a limited region of interest, e.g. TPR > 0.8 or FPR < 0.2.\\

\textbf{Multi-class AUC:} $\text{AUC}(k ~|~ \ell)$ for classes $k$ (pos) and $\ell$ (neg).
$$\text{AUC}_{MC} = \tfrac{1}{g(g - 1)} \sum_{k \neq \ell} \text{AUC}(k | \ell) \in [0, 1].$$

\begin{codebox}
  \textbf{Precision-Recall (PR) Curves: }
\end{codebox}
Slightly changed ROC plot. Simply plot precision and recall, instead of TPR-FPR.
  \end{myblock}
  }
  \end{minipage}
  \end{beamercolorbox}
  \end{column}

\end{columns}
\end{frame}
\end{document}
