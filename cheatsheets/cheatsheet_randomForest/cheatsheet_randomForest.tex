\documentclass{beamer}
% These are loaded from relative file paths which depend on where the cheatsheets are stored
% Currently these are at two different directory levels, so we can reduce this copypasta 
% once they are always at the same level
\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{../../style/beamerposter}
% usepackage{beamerthemeNAME} is equivalent to usetheme{NAME}
% But usepackage works with relative file paths, usetheme does not (as easily at least)
\mode<presentation>{\usepackage{../../style/beamerthememlr}}
\input{../../style/preamble-cheatsheets}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Bagging Ensembles}
						\begin{codebox}
			\textbf{Basic Idea:}
						\end{codebox}
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
                        \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation
                        \item It's an \textbf{ensemble method}, i.e., it combines many models into one 
        big \enquote{meta-model}
                        \item The constituent models of an ensemble are called \textbf{base learners}
                        \item All base learners are of the same type. The only difference between the models is the data they are trained on.
                        \end{itemize}
				
						\begin{codebox}
			\textbf{Bagging: }
						\end{codebox}
						Base learners $\blm, m = 1, \dots, M$ are trained on $M$ \textbf{bootstrap} samples of training data $\D$. \textbf{Aggregate} the predictions of the $M$ fitted base learners to get the \textbf{ensemble model} $\fMh$.
						
						\includegraphics[width=1.1\columnwidth]{img/img_1.jpg}
						
						\begin{codebox}
			\textbf{Note: }
						\end{codebox}
						Gains performance by reducing the variance of predictions, but (slightly) increases the bias: it reuses training data many times, so small mistakes can get amplified and Works best if base learners' predictions are only weakly correlated.
						
             \end{myblock}
             
        \begin{myblock}{Random Forests}
        \begin{codebox} \textbf{Introduction: }
        \end{codebox}
        Modification of bagging for trees proposed by Breiman (2001):
\end{myblock}\vfill
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

  \begin{myblock}{ }
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Tree baselearners on bootstrap samples of the data
  \item Uses \textbf{decorrelated} trees by randomizing splits (see below)
  \item Tree baselearners are usually fully expanded, without aggressive early stopping or pruning, to \textbf{increase variance of the ensemble}
  \end{itemize}
  
  \begin{codebox}
  \textbf{Out of bag Error/ Out of bag estimate:}
  \end{codebox}
  With the RF it is possible to obtain unbiased estimates of generalization error directly during training, based on the out-of-bag observations for each tree:
  
  \begin{center}
  \includegraphics[width=0.9\columnwidth]{img/img_2.PNG}
  \end{center}
  
  \end{myblock}
  
  \begin{myblock}{Feature Importance}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Single trees are highly interpretable and Random Forests as ensembles of trees lose this feature
  \item Contributions of the different features to the model are difficult to evaluate
  \item Way out: variable importance measures
  \item Basic idea: by how much would performance of the random forest decrease if a specific feature were removed or rendered useless?
    \end{itemize}
  
  \hspace*{1ex}
  
  \begin{algorithm}[H]
  \small
  \caption{Measure based on improvement in split criterion}
  \begin{algorithmic}[0]
  \For{features $x_j$, $j = 1$ to $p$}
  \For{tree base learners $\blmh$, $m = 1$ to $M$}
  \State {Find all nodes $\Np$ in $\blmh$ that use $x_j$.} 
  \State {Compute improvement in splitting criterion achieved by them.}
  \State {Add up these improvements.}
  \EndFor
  \State {Add up improvements over all trees to get feature importance of $x_j$.}
  \EndFor
  \end{algorithmic}
  \end{algorithm}

  
  \end{myblock}
}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{ }
  \begin{algorithm}[H]
  \small
  \caption{Measure based on permutations of OOB observations}
  \begin{algorithmic}[0]
  \State While growing tree, pass down OOB observations and record predictive accuracy.
  \State Permute OOB observations of $j$-th feature. This destroys the association between the target and the permuted $j$-th feature.
  \State Pass down the permuted OOB observations and evaluate predictive accuracy again.
  \State The decrease of performance induced by permutation is averaged over all trees and is used as a measure for the importance of the $j$-th variable.
  \end{algorithmic}
  \end{algorithm}
  
  \end{myblock}
  \begin{myblock}{Random Forest: Proximities}
  A measure of similarity ("closeness" or "nearness") of observations derived from random forests
  
  \begin{codebox} 
  \textbf{Basic Idea: }	
  \end{codebox}
  
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item The proximity between two observations $\xi$ and $\xi[j]$ is calculated by measuring the number of times that these two observations are placed in the same terminal node of the same tree of random forest, divided by the number of trees in the forest
  \item The proximity of observations $\xi$ and $\xi[j]$ can be written as $\operatorname{prox}\left(\xi, \xi[j]\right)$
    \item The proximities form an intrinsic similarity measure between pairs of observations
  \end{itemize}
  
  \begin{codebox}
  \textbf{Usage of random forest proximities: }
  \end{codebox}
  Imputing missing data, locating outliers, identifying mislabeled data, visualizing the forest.
  
  \end{myblock}
  
  \begin{myblock}{Synopsis}
  \textbf{Hypothesis Space:}
  Random forest models are (sums of) step functions over rectangular partitions of (subspaces of) $\Xspace$.
  \vspace*{1ex}
  
  \textbf{Risk:}
  Like trees, random forests can use any kind of loss function for regression or classification.
  \vspace*{1ex}
  
  \textbf{Optimization:}
  Exhaustive search over all (randomly selected!) candidate splits in each node of each tree to minimize the empirical risk in the child nodes.
  
  \end{myblock}
  }
\end{minipage}
\end{beamercolorbox}
\end{column}

\end{columns}
\end{frame}
\end{document}
