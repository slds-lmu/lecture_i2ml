\documentclass{beamer}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{I2ML :\,: BASICS} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ \invisible{x} % Package description in header
	% The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}
	

	
\begin{document}
\begin{frame}[fragile]{}
\vspace{-8ex}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First Column begin
%-------------------------------------------------------------------------------
% Data
%-------------------------------------------------------------------------------
\begin{myblock}{Data}
 $\Xspace \subseteq \R^p$ : $p$-dimensional \textbf{feature space} / input space\\ 
Usually we assume categorical features to be numerically encoded.\\
%sually we assume $\Xspace \equiv \R^p$, but sometimes, dimensions may be \\  
%bounded (e.g., for categorical or non-negative features.)    \\

$\Yspace$ : \textbf{target space} \\ 
e.g.: $\Yspace = \R$ for regression, $\Yspace = \setzo$ or $\Yspace = \setmp$ for binary classification, $\Yspace = \gset$ for multi-class classification with $g$ classes\\

$\xv = \xvec \in \Xspace$ : \textbf{feature vector} / covariate vector\\ 
 
$y \in \Yspace$ : \textbf{target variable} / output variable \\
Concrete samples are called labels \\

$\xyi \in \Xspace\times \Yspace$ : $i$ -th \textbf{observation} / sample / instance / example\\

$\allDatasets = \defAllDatasets$ : \textbf{set of all finite data sets} \\

$\allDatasetsn = \defAllDatasetsn \subseteq \allDatasets$ : \textbf{set of all finite data sets of size $n$} \\

$\D = \Dset \in \allDatasetsn $ : \textbf{data set} of size $n$.
An n-tuple, a family indexed by $\{1, \dots, n\}$. 
We use $\D_n$ to emphasize its size.\\
 
$\Dtrain$, $\Dtest \subseteq \D$ : \textbf{data sets for training and testing} \\ 
Often: $\D = \Dtrain ~ \dot{\cup} ~ \Dtest$\\
 

$\P_{xy}$ : \textbf{joint probability distribution on} $\Xspace \times \Yspace$ \\


\underline{Classification}\\

$o_k(y) = \I(y = k) \in \{0,1\}$: multiclass one-hot encoding, if $y$ is class k\\ 
We write $\bold{o}(y)$ for the g-length encoding vector and $o_k^{(i)} =  o_k(\yi)$\\

$\pi_k = \P(y = k)$:\textbf{ prior probability} for class $k$ \\
In case of binary labels we might abbreviate: $\pi = \P(y = 1)$.
  
\end{myblock}
%-------------------------------------------------------------------------------
% Model and Learner
%-------------------------------------------------------------------------------
\begin{myblock}{Model and Learner}
    \textbf{Model} / Hypothesis: $f : \Xspace \rightarrow \R^g$ maps features to predictions, often parametrized by $\thetav \in \Theta$ (then we write $f_{\thetav}(\xv)$ or $f(\xv | \thetav)$). \\

% $\fx$ or $\fxt \in \R$ or $\R^g$ : prediction function (\textbf{model}) \\ %learned from data
% \hspace*{1ex}We might suppress $\thetav$ in notation. \\

$\Theta \subseteq \R^d$ : \textbf{parameter space} \\
  
$\thetav = (\theta_1, \theta_2, ..., \theta_d) \in \Theta$: model \textbf{parameter} vector\\
Some models may traditionally use different symbols. \\

$\Hspace = \{f : \Xspace \rightarrow \R^g ~|~ f \text{ belongs to a certain functional family}\}$ : \\ \textbf{Hypothesis space} -- set of functions to which we restrict learning
				
\end{myblock}\vfill
% End First Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Second Column
\begin{myblock}{} \vspace{-4ex}

\textbf{Learner} / Inducer $\inducer: \preimageInducerShort \rightarrow \Hspace$  takes a training set  $\Dtrain \in \allDatasets$, produces model $f : \Xspace \rightarrow \R^g$, with hyperparam. configuration $\bm{\lambda} \in \bm{\Lambda}$.\\
We also write $\inducer: \preimageInducerShort \rightarrow \Theta$ or $\ind_{\bm{\lambda} }: \allDatasets \rightarrow \Theta$ \\

$\bm{\Lambda} = \bm{\Lambda_1} \times \bm{\Lambda_2} \times ... \times \bm{\Lambda_}{\ell} \subseteq \R^{\ell}$: %, where $\bm{\Lambda_j} = (a_j, b_j), \quad a_j, b_j \in \R,$ \\$j = 1, 2, ... , \ll$ : 
\textbf{hyperparameter space} \\
$\bm{\Lambda_j} $ are usually bounded real or integer intervals or a finite categorical set\\
% $\R$, intervals in $\R$ or intervals in $\N$\\

$\bm{\lambda} = (\lambda_1, \lambda_2, ..., \lambda_{\ell}) \in \bm{\Lambda}$ : \textbf{hyperparameter configuration} \\



% $\LLt$ and $\llt = \log\LLt$ : \textbf{likelihood} and \textbf{log-likelihood} for \\ parameter $\thetav$ \\
% These are based on a statistical model.\\

$r = y - \fx$ or $r^{(i)} = \yi - \fxi$ : ($i$-th) \textbf{residual} in regression\\

%????????????????????????????????


\underline{Classification}\\

$\pikx : \Xspace \rightarrow [0,1]$ \textbf{probability prediction} for class $k$, approximates $\postk$;
% $\postk \in [0, 1]$: \textbf{posterior probability} for class $k$, \\ given $\xv$ (
for binary we abbreviate with $\pix$ for $\post$.\\
 
$\fkx: \Xspace \rightarrow \R$: \textbf{scoring} / discriminant \textbf{function} for class $k$;\\
for binary we use $\fx = f_{1}(\xv) - f_{2}(\xv)$\\
 
$\hx: \Xspace \rightarrow \Yspace$ : \textbf{hard label function};\\ 
% that maps class scores / probabilities to discrete classes.
Typically created by $h(\xv) = \argmax \limits_{k \in \gset} \fkx$ or \\ $h(\xv) = \argmax \limits_{k \in \gset} \pikx$  \\

$\yf$ or $\yfi$: \textbf{margin} for ($i$-th) observation in binary classification\\%\\ classification (with $\Yspace = \{-1, 1\}$). \\

$c \in \R$, s.t. $h(\xv) := [\pix \geq c]$ or $h(\xv) := [\fx \geq c]$: \textbf{threshold} for hard label assignment in binary case (common: $c = 0$ for scoring, $c = 0.5$ for probabilistic classifiers)

% \hrule width7cm
\vspace{1cm}

$\yh$, $\fh$, $\hh$, $\pikxh$, $\pixh$ and $\thetah$ \\
The hat symbol denotes \textbf{learned} functions and parameters.

\end{myblock}

%-------------------------------------------------------------------------------
% Loss and Risk 
%-------------------------------------------------------------------------------
\begin{myblock}{Loss, Risk and ERM}
  $L: \Yspace \times \R^g \to \R^+_0$ : \textbf{loss function}: 
 Quantifies "quality" $\Lxy$ of prediction $\fx$ (or $\Lpixy$ of prediction $\pix$) for true $y$. \\

\textbf{(Theoretical) risk:} $\risk:  \Hspace \to \R $, 
$\riskf = \E_{(\xy \sim \Pxy)}[\Lxy]$
  
\textbf{Empirical risk:} $\riske:  \Hspace \to \R $, 
% summed loss of model over data.
$\riskef = \sumin \Lxyi$,  analogously:
$\riske : \Theta \to \R$; $\risket = \sumin \Lxyit$\\
  
\textbf{Empirical risk minimization (ERM)}:
% -- figuring out which model $\fh$ has the smallest summed loss. \\ 
$\thetavh \in \argmin \limits_{\thetav \in \Theta} \risket$ \\
% $= \argmin \limits_{\thetav \in \Theta} \sumin \Lxyit,$ \\

\textbf{Bayes-optimal model}: $\fbayes = \argmin_{f: \Xspace \rightarrow \R^g}  
\riskf \\

\textbf{Regularized risk}: $\riskr: \Hspace
\to \R, \riskrf = \riskef + \lambda \cdot J(f)$ with regularizer $J(f)$, 
complexity control parameter $\lambda > 0$ (analogous for $\thetav$).

\end{myblock}





% End Second Column					
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Third Column#


%-------------------------------------------------------------------------------
% Regression Losses 
%------------------------------------------------------------------------------- 
\begin{myblock}{Regression Losses}
  \textbf{L2 loss / squared error:} 
\begin{itemize}    
  \setlength{\itemindent}{+.3in}
  \item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
  \item Convex and differentiable, non-robust against outliers
  % \item Tries to reduce large residuals (loss scaling quadratically)
  \item Optimal constant model: $\fxh = \meanin \yi =
  \bar{y}$
  \item Optimal model over $\Pxy$ for unrestricted $\Hspace$: $\fxh = \E[y | \xv]$
  % \item $\fxh = \text{mean of } y | \bm{x}$
\end{itemize}

\vspace*{1ex}
%        \includegraphics[width=1\columnwidth]{img/reg_loss.PNG}


  \textbf{L1 loss / absolute error:} 
\begin{itemize}
\setlength{\itemindent}{+.3in}
  \item $\Lxy = |y-\fx|$
  \item Convex and more robust, non-differentiable
  \item Optimal constant model: $\fxh = \text{med}(y^{(1)}, \ldots, y^{(n)})$
  \item Optimal model over $\Pxy$ for unrestricted $\Hspace$: $\fxh = \text{med} [y | \xv]$
  % \item \textcolor{orange}{Optimal model for unrestricted $\Hspace$: $\fxh = \meanin \text{med}(\yi | \xi)$}
  % \item $\fxh = \text{median of } y | \bm{x}$     
\end{itemize}
  %\includegraphics[width=1.03\columnwidth]{img/reg_loss_2.PNG} 
\end{myblock}

%-------------------------------------------------------------------------------
% Classification Losses 
%------------------------------------------------------------------------------- 

\begin{myblock}{Classification Losses}

% \textbf{0-1 loss} \\
% $\Lhxy = [y \neq \hx]$ ~ for $\Yspace = \setzo$ \\
\textbf{0-1-loss (binary case)}\\
$L (y, h(\xv)) = \I(y \neq h(\xv))$\\
$L (y, \fx) = \I( y\fx < 0)$ for $\Yspace = \setmp$ \\ 
Discontinuous, results in NP-hard optimization\\
%Optimal constant model: $h(\xv) \in \argmax \limits_{j \in {0,1}} \sumin \I(\yi = j) $\\

\textbf{Brier score (binary case)} \\
$\Lpixy = (\pix - y)^2$ for $\Yspace = \setzo$ \\
Least-squares on probabilities\\
%Optimal constant model: $\pixh = \bar{y}$\\


\textbf{Log-loss / Bernoulli loss / binomial loss (binary case)}\\
$\Lpixy = -y \log(\pix) - (1-y) \log(1-\pix)$ for $\Yspace = \setzo$ \\
$\Lpixy = \log(1 + (\frac{\pix}{1-\pix})^{-y})$ for $\Yspace = \setmp$ \\
%Optimal constant model: $\pixh = \bar{y}$\\

Assuming a logit-link $\pix = \exp(\fx) / ( 1+\exp(\fx))$:\\
$\Lxy = -y \cdot \fx + \log(1 + \exp(\fx))$ for $\Yspace = \setzo$ \\
$\Lxy = \log(1 + \exp(- y \cdot \fx))$ for $\Yspace = \setmp$ \\
Penalizes confidently-wrong predictions heavily\\

\textbf{Brier score (multi-class case)} \\
$\Lpixy =  \sumkg (\pikx - o_k(y))^2$ \\
%Optimal constant model: $\pixh = \left(\meanin o_1^{(i)}, \meanin o_g^{(i)}\right)$ \\

\textbf{Log-loss (multi-class case)} \\
$ \Lpixy =  - \sumkg o_k(y) \log(\pikx)$ \\  %\\
%Optimal constant model: $\pixh = \left(\meanin o_1^{(i)}, \meanin o_g^{(i)}\right)$ \\

\underline{Optimal constant models}\\[-1ex]
0-1-loss: $h(\xv) \in \argmax \limits_{j \in {0,1}} \sumin \I(\yi = j) $\\
Brier and log-loss (binary): $\pixh = \bar{y}$\\
Brier and log-loss (multiclass): $\pixh = \left(\meanin o_1^{(i)}, \dots, \meanin o_g^{(i)}\right)$ 

%\textcolor{orange}{ADD PROPERTIES OF LOSSES}

\end{myblock}
%-------------------------------------------------------------------------------
% Classification 
%------------------------------------------------------------------------------- 

%\begin{myblock}{Classification}
% 				    We want to assign new observations to known categories according to criteria learned from a training set.  
%             \vspace*{1ex}
%             

%$y \in \Yspace = \gset : $ categorical output variable (label)\\ 

%\textbf{Classification} usually means to construct $g$ \textbf{discriminant functions}:
  
%$f_1(\xv), \ldots, \fgx$, so that we choose our class as \\ $h(\xv) = \argmax_{k \in \gset} \fkx$ \\

%\textbf{Linear Classifier:} functions $\fkx$ can be specified as linear functions\\

% \hspace*{1ex}\textbf{Note: }All linear classifiers can represent non-linear decision boundaries \hspace*{1ex}in our original input space if we include derived features. For example: \hspace*{1ex}higher order interactions, polynomials or other transformations of x in \hspace*{1ex}the model.

%\textbf{Binary classification: }If only 2 classes ($\Yspace = \setzo$ or  $\Yspace = \setmp$) exist, we can use a single discriminant function $\fx = f_{1}(\xv) - f_{2}(\xv)$.  \\


% \textbf{Generative approach }models $\pdfxyk$, usually by making some assumptions about the structure of these distributions and employs the Bayes theorem: 
% $\pikx = \postk \propto \pdfxyk \pik$. \\ %It allows the computation of \hspace*{1ex}$\pikx$. \\
% \textbf{Examples}: Linear discriminant analysis (LDA), Quadratic discriminant analysis (QDA), Naive Bayes\\
% 
% \textbf{Discriminant approach }tries to optimize the discriminant functions directly, usually via empirical risk minimization:\\ 
% $ \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.$\\
% \textbf{Examples}: Logistic/softmax regression, kNN


%\end{myblock}

%-------------------------------------------------------------------------------
% HRO - Components of Learning 
%-------------------------------------------------------------------------------          
%\begin{myblock}{Components of Learning}

%\textbf{Learning = Hypothesis space + Risk + Optimization} \\
%\phantom{\textbf{Learning}} \textbf{= }$ \Hspace + \risket + \argmin_{\thetav \in \Theta} 
%\risket$

% 
% \textbf{Learning &= Hypothesis space &+ Risk  &+ Optimization} \\
% &= $\Hspace &+ \risket &+ \argmin_{\thetav \in \Theta} \risket$
% 
% \textbf{Hypothesis space: } Defines (and restricts!) what kind of model $f$
% can be learned from the data.
% 
% Examples: linear functions, decision trees
% 
% \vspace*{0.5ex}
% 
% \textbf{Risk: } Quantifies how well a model performs on a given
% data set. This allows us to rank candidate models in order to choose the best one.
% 
% Examples: squared error, negative (log-)likelihood
% 
% \vspace*{0.5ex}
% 
% \textbf{Optimization: } Defines how to search for the best model, i.e., the model with the smallest {risk}, in the hypothesis space.
% 
% Examples: gradient descent, quadratic programming


%\end{myblock}
% End Third Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			  }
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
\end{columns}

\end{frame}
\end{document}
