\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
%\input{../../latex-math/ml-lm.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Regression Losses}
						\begin{codebox}
			\textbf{Basic Idea (L2 loss/ squared error):}
						\end{codebox}
						
						\begin{itemize}[$\bullet$]     
						\setlength{\itemindent}{+.3in}
              \item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$
              \item Convex and differentiable
              \item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large)   
              \item Risk minimizer $\fxbayes = \E_{y|x} \left[y ~|~ \xv \right]$
              \item Optimal constant model $\fxh = \frac{1}{n} \sumin \yi$
            \end{itemize}

            \vspace*{1ex}
            \includegraphics[width=0.8\columnwidth]{img/reg_loss.PNG}

\begin{codebox}
\textbf{Basic Idea (L1 loss/ absolute error):}
\end{codebox}

\begin{itemize}[$\bullet$]     \setlength{\itemindent}{+.3in}
\item $\Lxy = |y-\fx|$
\item Convex and more robust
\item No derivatives for $ = 0$, $y = \fx$, optimization becomes harder
\item Risk minimizer $\fxbayes = \text{med}_{y|x} \left[y ~|~ \xv \right]$
\item Optimal constant model $\fxh = \text{med}(\yi)$    
  \end{itemize}

\includegraphics[width=0.8\columnwidth]{img/reg_loss_2.PNG} 

      \end{myblock}

  }
			\end{minipage}
		\end{beamercolorbox}
	\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

\begin{myblock}
  {Linear Regression Models}
\begin{codebox}
    \textbf{Risk (corresponding to \textbf{L2 Loss}):}
\end{codebox} 
    \[\risket = \operatorname{SSE}(\thetav) = \sumin \Lxyit = \sumin \left(\yi - \thetav^T \xi\right)^2\]
    Via normal equations $\pd{\risket}{\thetav} = 0$ we get ordinary-least-squares (OLS) estimator
    $\thetavh = \olsest$

  \begin{codebox}
  \textbf{Risk (corresponding to \textbf{L1 Loss}):}
  \end{codebox}
  \[
    \risket = \sumin \Lxyit = \sumin \left|\yi - \theta^T \xi\right| \qquad
    \]
  L1 loss is harder to optimize, but the model is less sensitive to outliers
  
  \begin{codebox}
  \textbf{Summary:}
  \end{codebox}
  \textbf{Hypothesis Space:} Set of all linear functions in $\bm{\theta}$:
  $$\Hspace = \{ \theta_0 + \thetav^T \xv\ |\ (\theta_0, \thetav) \in \mathbb{R}^{p+1} \}$$

  \vspace*{1ex}
  
  \textbf{Risk:} Any regression loss function.
  
  \vspace*{1ex}
  
  \textbf{Optimization:} Direct analytic solution for L2 loss, numerical optimization for L1 and others.
  \end{myblock}
  
}
\end{minipage}
\end{beamercolorbox}
\end{column}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{Polynomial Regression Models}
  
    Linear regression models can be made more flexible by using \emph{polynomials} $x_j^d$ -- or any other \emph{derived features} like $\sin(x_j)$ or $(x_j \cdot x_k)$ -- as additional features. The optimization and risk of the learner remain the same.\vspace*{1ex}
    Only the hypothesis space of the learner changes and includes linear functions of the derived features as well. e.g.:
      \begin{align*}
    f(\xi) &= \theta_0 + \sum^d_{k=1} \theta_{1k} \left(\xi_1\right)^k + \sum^d_{k=1} \theta_{2k} \left(\xi_2\right)^k + \dots
    \end{align*}
    
    Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:
    
    %\vspace*{1ex}
    \includegraphics[width=0.9\columnwidth]{img/poly_reg.PNG}
  

  The higher $d$ is, the more \textbf{capacity} the learner has to learn complicated functions of $x$, but
  this also increases the danger of \textbf{overfitting}
  
  \vspace*{1ex}
  
  \end{myblock}
  
  
}

\end{minipage}
\end{beamercolorbox}
\end{column}

\end{columns}
\end{frame}
\end{document}
