\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Regression Losses}
						\begin{codebox}
			\textbf{Basic Idea (L2 loss/ squared error):}
						\end{codebox}
						
						\begin{itemize}[$\bullet$]     
						\setlength{\itemindent}{+.3in}
              \item $\Lxy = (y-\fx)^2$ or $\Lxy = 0.5 (y-\fx)^2$.
              \item Convex and differentiable.
              \item Tries to reduce large residuals (if residual is twice as large, loss is 4 times as large).
              \item Risk minimizer $\fxbayes = \E_{y|x} \left[y ~|~ \xv \right]$.
              \item Optimal constant model $\fxh = \frac{1}{n} \sumin \yi$.
            \end{itemize}

            \vspace*{1ex}
            \includegraphics[width=0.8\columnwidth]{img/reg_loss.PNG}

\begin{codebox}
\textbf{Basic Idea (L1 loss/ absolute error):}
\end{codebox}

\begin{itemize}[$\bullet$]     \setlength{\itemindent}{+.3in}
\item $\Lxy = |y-\fx|$.
\item Convex and more robust.
\item No derivatives for $ = 0$, $y = \fx$, optimization becomes harder.
\item Risk minimizer $\fxbayes = \text{med}_{y|x} \left[y ~|~ \xv \right]$.
\item Optimal constant model $\fxh = \text{med}(\yi)$.
  \end{itemize}

\includegraphics[width=0.8\columnwidth]{img/reg_loss_2.PNG} 

      \end{myblock}

  }
			\end{minipage}
		\end{beamercolorbox}
	\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

\begin{myblock}
  {Linear Regression Models}

  Predict $y \in \R$ as \textbf{linear} combination of features:
  $$\yh = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p.$$
    
  \begin{align*}
    \hat \yv = \Xmat \thetav = 
        \left(
        \begin{smallmatrix}
            1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
            1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
            \vdots & \vdots & & \vdots \\
            1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
        \end{smallmatrix}
        \right)
        \left(
        \begin{smallmatrix}
            \theta_0 \\ \theta_1 \\ \vdots \\ \theta_p
        \end{smallmatrix}
        \right)
        &=
        \left(
        \begin{smallmatrix}
            \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
            \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
            \vdots \\
            \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
        \end{smallmatrix}
        \right)
    \end{align*}
 
    $\Xmat \in \R^{n \times (p + 1)}$ is the \textbf{design matrix}.

\begin{codebox}
    \textbf{Risk (corresponding to \textbf{L2 Loss}):}
\end{codebox} 
    \[\risket = \operatorname{SSE}(\thetav) = \sumin \Lxyit = \sumin \left(\yi - \thetav^T \xi\right)^2\]
    Via normal equations $\pd{\risket}{\thetav} = 0$ we get ordinary-least-squares (OLS) estimator
    $\thetavh = \olsest$.

  \begin{codebox}
  \textbf{Risk (corresponding to \textbf{L1 Loss}):}
  \end{codebox}
  \[
    \risket = \sumin \Lxyit = \sumin \left|\yi - \theta^T \xi\right| \qquad
    \]
  L1 loss is harder to optimize, but the model is less sensitive to outliers.
  
  \begin{codebox}
  \textbf{Summary:}
  \end{codebox}
  \textbf{Hypothesis Space:} Set of all linear functions in $\bm{\theta}$:
  $$\Hspace = \{ \theta_0 + \thetav^T \xv\ |\ (\theta_0, \thetav) \in \mathbb{R}^{p+1} \}$$

  \vspace*{1ex}
  
  \textbf{Risk:} Any regression loss function.
  
  \vspace*{1ex}
  
  \textbf{Optimization:} Direct analytic solution for L2 loss, numerical optimization for L1 and others.
  \end{myblock}
  
}
\end{minipage}
\end{beamercolorbox}
\end{column}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{Polynomial Regression Models}
  
   The linear model with \textbf{basis functions} $\phi_j$:
    \begin{equation*} \label{true_lm}
      \fx = \theta_0 + \sumjp \theta_j \phi_j (x_j)
      = \theta_0 + \theta_1  \phi_1 \left( x_1 \right)
      + \dots + \theta_p \phi_p \left(x_p \right)
    \end{equation*}

    Design matrix:
\begin{align*}
  \Xmat =
  \left(
    \begin{smallmatrix}
        1 & \textcolor{black}{\phi_1} (x^{(1)}_1) & \ldots &
        \textcolor{black}{\phi_p} (x^{(1)}_p) \\
        % 1 & \textcolor{black}{\phi_2} (x^{(2)}_1) & \ldots &
        % \textcolor{black}{\phi_2} (x^{(2)}_p) \\
        \vdots & \vdots & & \vdots \\
        1 & \textcolor{black}{\phi_1} (x^{(n)}_1) & \ldots &
        \textcolor{black}{\phi_p} (x^{(n)}_p) \\
    \end{smallmatrix}
    \right)
\end{align*}

Simple \& flexible choice for basis functions: \textbf{$d$-polynomials} gives us \textbf{polynomial regression}:\\
Map $x_j$ to (weighted) sum of its monomials up to order $d \in \N$
$$ \phi^{(d)}: \R \rightarrow \R, x_j \mapsto \sum_{k = 1}^d \beta_k x_j^k$$

    Models of different \emph{complexity}, i.e. of different polynomial order $d$, are fitted to the data:
    
    %\vspace*{1ex}
    \includegraphics[width=0.9\columnwidth]{img/poly_reg.PNG}
  

  The higher $d$ is, the more \textbf{capacity} the learner has to learn complicated functions of $x$, but
  this also increases the danger of \textbf{overfitting}.
  
  \vspace*{1ex}
  
  \end{myblock}
  
  
}

\end{minipage}
\end{beamercolorbox}
\end{column}

\end{columns}
\end{frame}
\end{document}
