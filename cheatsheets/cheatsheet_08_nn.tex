\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-trees.tex}
\input{../latex-math/ml-nn.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{

					\begin{myblock}{NN Overview}
      \textbf{Deep learning} is a subfield of machine learning that uses neural networks with many layers to learn complex patterns in data.\\
      \textbf{Representation learning:} DL automates feature engineering.
      \begin{codebox} 
        \textbf{Hidden Layers:}
        \end{codebox}
      \begin{itemize}[$\bullet$] 
        \setlength{\itemindent}{+.3in}
          \item Each layer adds degree of non-linearity that can learn more abstract representations.
          \item Output of hidden units serves as input for units in next layer.
          \item Too many hidden layers or too many units per layer cause overfitting.
      \end{itemize}

\end{myblock}


\begin{myblock}{Perceptron}

The \textbf{perceptron} is a single artificial neuron and basic computational unit of neural networks.
It is restricted to learn only linear decision boundaries.
A neural network is built by combination of multiple perceptrons.\\
The perceptron is a weighted sum of input values, transformed by $\tau$:
$$\fx=\tau(w_1x_1+\ldots+w_px_p+b)=\tau(\wtw^\top\xv+b).$$

\begin{codebox} 
  \textbf{Structure:}
  \end{codebox}
A neuron performs a 2-step computation:
\begin{itemize}[$\bullet$] 
  \setlength{\itemindent}{+.3in}
  \item \textbf{Affine Transformation:} weighted sum of inputs plus bias: $z_{in}=w_1x_1+\ldots+w_px_p+b$.
  \item \textbf{Non-linear Activation:} a non-linear transformation: $\tau(z_{in})$.
\end{itemize}

Weights $\wtw$ are connected to edges from the input layer.\\
For an explicit graphical representation, we do a simple trick:
\begin{itemize}[$\bullet$] 
  \setlength{\itemindent}{+.3in}
  \item Add a constant feature to the inputs: $\xtil = (1,x_1, \ldots, x_p)^\top$
  \item Absorb the bias into the weight vector: $\tilde{\wtw} = (b,w_1, \ldots, w_p)^\top $
\end{itemize}

A \textbf{forward pass}: input vector being "fed" to neurons on the left followed by a sequence of computations performed from left to right. 

\begin{codebox} 
  \textbf{Activation function:}
  \end{codebox}
A single neuron represents different functions depending on the choice of activation function $\tau$.

\begin{itemize}[$\bullet$] 
  \setlength{\itemindent}{+.3in}
  \item Identity function gives us the simple linear regression: $\fx=\tau(\wtw^\top\xv)=\wtw^\top\xv$
  \item Logistic function gives us the logistic regression: $\fx=\tau(\wtw^\top\xv)=\frac{1}{1+\text{exp}(-\wtw^\top\xv)}$
\end{itemize}

\begin{codebox} 
  \textbf{Hypothesis space:}
  \end{codebox}
  $\Hspace = \{f: \R^p \rightarrow \R | \fx = \tau(\sumjp w_jx_j+b), \wtw \in \R^p, b\in \R\}$
  

\end{myblock}\vfill
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

\begin{myblock}{ }
  
    If $\tau$ is the logistic sigmoid or identity function, $\Hspace$ corresponds to the hpothesis space of logistic or linear regression, respectively.
    
    \begin{codebox} 
      \textbf{Optimization:}
      \end{codebox}
    Minimize the empirical risk $\riske = \meanin \Lxyi$.\\
    Regression: L2 loss. Binary classification: cross-entropy loss.
    \begin{itemize}[$\bullet$] 
      \setlength{\itemindent}{+.3in}
        \item For a single neuron and both choices of $\tau$, the loss function is convex.
        \item The global optimum can be found with an iterative algorithm like GD.
        \item A single neuron with logistic sigmoid function trained with the Bernoulli loss yields the same result as logistic regression when trained until convergence.
    \end{itemize} 
  \end{myblock}

  \begin{myblock}{Single hidden layer networks}
  \textbf{Single hidden layer networks} have a set of neurons in hidden layers and one or more output neurons.
Multiple inputs are simultaneously fed to the network.
Each neuron in the hidden layer performs a 2-step computation as a single neuron.
The final output of the network is then calculated by another 2-step computation performed by the neuron in the output layer.

  \textbf{Hidden Layer Activation Function:}\\
  \textbf{ReLU:} $\tau(v) = \max(0,v)$. \textbf{Signoid:} $\tau(v) = \frac{1}{1+\text{exp}(-v)}$.

\begin{codebox}
  \textbf{Multi-class Classification:}
  \end{codebox}

Multiple neurons in the output layer.
Each neuron will represent a specific class.
For $g$-class classification, $g$ output units: $\bm{f}=(f_1,\ldots,f_g)$.\\
$m$ hidden neurons $z_1,\ldots, z_m$ with $z_j = \sigma(\bm(W)_j^\top\xv), j=1,\ldots,m$.\\
$f_{in,k}=\bm{U}_k^\top\bm{z}, \bm{z}=(z_1,\ldots, z_m)\top, k=1,\ldots,g$.\\

Apply a \textbf{softmax} activation function to the output layer.
This gives us a probability distribution over g different possible classes:
$$f_{out,k}=\tau_k(f_{in,k})=\frac{\text{exp}(f_{in,k})}{\sum_{k'=1}^{g}f_{in,k'}}.$$
Derivative $\frac{\partial \tau(f_{in})}{\partial f_{in}} = \text{diag}(\tau(f_{in}))-\tau(f_{in})\tau(f_{in})^\top$.\\

  The loss function for a softmax classifier is
  $$\Lxy=\sumkg [y=k]\text{log}\(\frac{\text{exp}(f_{in,k})}{\sum_{k'=1}^{g}f_{in,k'}}\),
  [y=k] = \begin{cases}
    1, & \text{if} \quad y=k \\
    o, & \text{otherwise}
    \end{cases}$$
  
    This is equivalent to the cross-entropy loss when the label vector y is one-hot coded. 
    There is no analytical solution.
  \end{myblock}

}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{

  \begin{myblock}{ML FNNs}
  
  We allow an arbitrary amount $l$ of hidden layers as multi-layer (ML). 
  \textbf{Feedforward neural networks (FNNs)}: inputs are passed through the network from left to right, no feedback-loops are allowed.


  \begin{codebox}
  \textbf{Chain Structure:}	
  \end{codebox}
  $$\fx = \tau \circ \phi \circ \sigma^{(l)} \circ \phi^{(l)} \circ \sigma^{(l-1)} \circ \phi^{(l-1)} \circ \ldots \circ \sigma^{(1)} \circ \phi^{(1)}.$$
  $\sigma^{(i)}$: $i$-th hidden layer activation function; $\phi^{(i)}$: weighted sum of $i$-th layer;
  $\tau$ and $\phi$: corresponding components of the output layer.

  Each hidden layer has:
  \begin{itemize}[$\bullet$]
  \setlength{\itemindent}{+.3in}
  \item An associated weight matrix $\bm{W}^{(i)}$, bias $\bm{b}^{(i)}$ and activations $\bm{z}^{(i)}$.
  \item $\bm{z}^{(i)} = \sigma^{(i)}(\phi^{(i)}) = \sigma^{(i)}(\bm{W}^{(i)\top} \bm{z}^{(i-1)}+\bm{b}^{(i)})$ where $z^{(0)}=\xv$.
  \end{itemize}

  Without non-linear activations in the hidden layers, the network can only learn linear decision boundaries.

 
  \end{myblock}
  
  \begin{myblock}{Backpropagation}
    We would like to run ERM by GD on: $$\risket = \meanin\Lxyit.$$
    Backprop training of NNs runs in 2 alternating steps, for one $\xv$:
    \begin{itemize}[$\bullet$]
      \setlength{\itemindent}{+.3in}
      \item \textbf{Forward pass:} Inputs flow through model to outputs, then compute the observation loss..
      \item \textbf{Backward pass:} Loss flows backwards to update weights so error is reduced, as in GD.
      \end{itemize}

  \begin{codebox} 
  \textbf{Example:}
  \end{codebox}
  One hidden layer, logistic activations with L2 loss.

  Forward pass:\\
  $z_{i,in} = \bm{W}_i^\top \xv+b_i, \quad z_{i,out} = \sigma(z_{i,in}),$\\
  $f_{in} = \bm{u}^\top\bm{z}+c, \quad f_{out} = \tau(f_{in}), \quad \Lxy = \frac{1}{2}(y-f_{out})^2.$

  Backward pass:\\
  $\frac{\partial \Lxy}{\partial u_i} = \frac{\partial \Lxy}{\partial f_{out}}\frac{\partial f_{out}}{\partial f_{in}}\frac{\partial f_{in}}{\partial u_i}$\\
  $\frac{\partial \Lxy}{\partial f_{out}} = -(y-f_{out}), \quad \frac{\partial f_{out}}{\partial f_{in}} = \sigma(f_{in})Â·(1-\sigma(f_{in})), \quad \frac{\partial f_{in}}{\partial u_i} = z_{i,out}.$\\
  $u^{[new]}_i = u^{[old]}_i - \alpha \frac{\partial \Lxy}{\partial u_i}.$\\
  $\frac{\partial \Lxy}{\partial W_{ij}} = \frac{\partial \Lxy}{\partial f_{out}}\frac{\partial f_{out}}{\partial f_{in}}\frac{\partial f_{in}}{\partial z_{j,out}}\frac{\partial z_{j,out}}{\partial z_{j,in}}\frac{\partial z_{j,in}}{\partial W_{ij}}$\\
  $\frac{\partial f_{in}}{\partial z_{j,out}}=u_j, \quad \frac{\partial z_{j,out}}{\partial z_{j,in}}=\sigma(z_{j,in})Â·(1-\sigma(z_{j,in})), \quad \frac{\partial z_{j,in}}{\partial W_{ij}}=x_i.$\\
  $W^{[new]}_{ij} = W^{[old]}_{ij} - \alpha \frac{\partial \Lxy}{\partial W_{ij}}.$\\  
\end{myblock}
  }
  
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}