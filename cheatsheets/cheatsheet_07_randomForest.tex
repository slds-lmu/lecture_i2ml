\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-eval.tex}
\input{../latex-math/ml-trees.tex}
\input{../latex-math/ml-ensembles.tex}

\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Bagging Ensembles}

						\begin{codebox}
			\textbf{Basic Idea:}
						\end{codebox}
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
                        \item \textbf{Bagging:} \textbf{B}ootstrap \textbf{Agg}regation
                        \item \textbf{Ensemble method}: combines models into large \enquote{meta-model}
                        \item \textbf{Base learners (BLs)}: individual models of an ensemble
                        \item Homogeneous ensembles: all BLs are of the same model type\\ $\rightarrow$  Only training data varies due to bootstrap
                         \item \textbf{Bagging reduces prediction variance} by averaging over BL predictions with a mild \textbf{increase in bias} due to data reuse\\
                         $\rightarrow$ Performs best when BL predictions are only weakly correlated
                        \end{itemize}
            %Bagging improve performance by reducing the variance of predictions, but (slightly) increases the bias: training data is reused, so small mistakes can get amplified. Works best if BLs' predictions are only weakly correlated.
						
				

						\begin{codebox}
			      \textbf{Bagging Training: }
						\end{codebox}
						Train BL $\bl, m = 1, \dots, M$ on $M$ \textbf{bootstrap} samples of $\D$. 
						\begin{algorithm}[H]
              \small
              \caption{Bagging algorithm: Training}
              \begin{algorithmic}[1]
                \State Input: Dataset $\D$, type of BLs, number of bootstraps $M$
                \For {$m = 1 \to M$}
                  \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
                  \State Train BL on $\D^{[m]}$ to obtain model $\blh$
                \EndFor
              \end{algorithmic}
            \end{algorithm}

            \begin{codebox}
              \textbf{Bagging Aggregating: }
              \end{codebox}

            Average predictions of $M$ fitted BLs to obtain the ensemble model:
            \begin{align*}
              \hat{f}(\xv) &= \meanmM \left(\blfhx\right) \text{(regression / decision score, use $\hat{f}_k$ in multi-class)} \\[-1ex]
              \hxh &= \argmax_{k \in \Yspace} \summM \I \left(\bllhx = k\right) \text{(majority voting)} \\[-1ex]
            \pikxh &=
            \left\{
            \begin{aligned}
            & \meanmM \blphxk && \text{(probabilities through averaging)} \\[-1ex]
            & \meanmM \I \left(\bllhx = k \right) && \text{(probabilities through class frequencies)}
            \end{aligned}
            \right.
            \end{align*}

             \end{myblock}

             \begin{myblock}{Random Forests}
              \begin{codebox} \textbf{Introduction: }
              \end{codebox}
          
              \begin{itemize}[$\bullet$]     
                \setlength{\itemindent}{+.3in}
                \item Random Forests (RFs) use \textbf{bagging with CARTs} as BLs.
                \item \textbf{Random feature sampling} (\texttt{mtry}) decorrelates the BLs.
                \item \textbf{Fully expanded trees} further increase variability of trees.
                \end{itemize}

              \end{myblock}
             
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{ }      
      \textbf{Advantages:} Can handle missing values / categorical features; Easy to parallelize; Work well on high-dimensional / noisy data\\
      \textbf{Disadvantages:} Extrapolation problem; Harder to interpret; Memory-hungry implementation; Expensive prediction for large ensembles
        
      \begin{codebox} \textbf{Random Feature Sampling: }
      \end{codebox}
      \begin{itemize}[$\bullet$]     
        \setlength{\itemindent}{+.3in}
        \item For each node, randomly draw $\texttt{mtry} \le p$ features to find best split\\
        $\rightarrow$ More randomness $\Rightarrow$ lower correlation between trees

        \item Classification: $\texttt{mtry}$ $ = \lfloor \sqrt{p} \rfloor$; Regression: $\texttt{mtry}$ $ = \lfloor p/3 \rfloor$
        \end{itemize}

        \begin{codebox} \textbf{Note:}
        \end{codebox}
        \begin{itemize}[$\bullet$]     
          \setlength{\itemindent}{+.3in}
          \item Other hyperparameters: minimum node size and tree depth
          %\item RF usually use fully expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variability of each tree}.\\
              \item Performance improves with larger ensembles
    \item May overfit, but less prone than individual CARTs due to averaging
    \item Trees are trained independently $\Rightarrow$ increasing the number of trees reduces variance \textbf{without increasing overfitting}
        \end{itemize}
      \end{myblock}

      \begin{myblock}{Out-of-Bag (OOB)}
       \begin{algorithm}[H]
              \small
              \caption{Out-Of-Bag error estimation}
              \begin{algorithmic}[1]
                \State Input: $\oobm = \{i \in \nset | \xyi \notin \D^{[m]}\}$, $\blh \ \forall m \in \{1, \dots, M\}$
                \For {$i = 1 \to n$}
                  \State Compute the ensemble OOB prediction for observation $i$, e.g., for regression:
                  $$\fh^{(i)}_{\oob} =
                  \frac{1}{S_{\oob}^{(i)}} \summM
                  \I(i \in \oobm) \cdot \blfh(\xi) $$

                  $S_{\mathrm{OOB}}^{(i)} = \summM \I(i \in \oobm)$ is the nr. of trees where $i$-th observation is OOB.
                \EndFor
                \State Average losses over all observations: $\GEh_{\oob} = \meanin L(\yi, \fh^{(i)}_{\oob})$
            
              \end{algorithmic}
            \end{algorithm}

            \textbf{OOB Size:} The probability that an observation is out-of-bag (OOB) is:
            $$\P \left(i \in \oobm\right) = \left(1 - \frac{1}{n}\right)^n \stackrel{n \to \infty}{\longrightarrow} \frac{1}{e} \approx 0.37$$
            $\Rightarrow$ similar to holdout or 3-fold CV (1/3 validation, 2/3 training)

            \textbf{OOB Error Usage:}
Get first impression of RF performance; Select ensemble size; Evaluate different RF hyperparameter configurations.
\end{myblock}
}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{Feature Importance}

\begin{itemize}[$\bullet$]     
          \setlength{\itemindent}{+.3in}
    \item RF perform better but are less interpretable compared to a single tree
\item \textbf{Feature importance} mitigates this by quantifying performance drop when a feature is permuted or excluded
\end{itemize}
    %RFs improve accuracy by aggregating multiple decision trees but \textbf{lose interpretability} compared to a single tree. 
    %\textbf{Feature importance} mitigates this problem by asking how much does performance decrease, if feature is removed / rendered useless.

    \begin{algorithm}[H]
      \small
      \caption{Feature importance based on permutations}
      \begin{algorithmic}[1]
        \State Calculate $\GEh_{\oob}$ using set-based metric $\rho$
        \For{features $x_j$, $j = 1 \to p$}
        \For{Some statistical repetitions}
          \State {Distort feature-target relation: permute $x_j$ with $\psi_j$}
          \State Compute all $n$ OOB-predictions for permuted feature data, obtain all $\fh^{(i)}_{\oob,\psi_j}$
          \State Arrange predictions in $\bm{\hat{F}}_{\oob,\psi_j}$;
          Compute $\GEh_{\oob, j} = \rho(\yv, \bm{\hat{F}}_{\oob,\psi_j})$
        \State {Estimate importance of $j$-th feature: $\widehat{\text{FI}_j} = \GEh_{\oob,j} - \GEh_{\oob
        } $}
        \EndFor
        \State Average obtained $\widehat{\text{FI}_j}$ values over reps
        \EndFor
      \end{algorithmic}
      \end{algorithm}

    \begin{algorithm}[H]
      \small
      \caption{Feature importance based on impurity}
      \begin{algorithmic}[1]
        \For{features $x_j$, $j = 1 \to p$}
        \For{all models $\blh$, $m = 1 \to M$}
        \State {Find all splits in $\blh$ on $x_j$}
        \State {Extract improvement / risk reduction for these splits}
        \State {Sum them up}
        \EndFor
        \State {Add up improvements over all trees for FI of $x_j$}
        \EndFor
      \end{algorithmic}
      \end{algorithm}
  \end{myblock}

\begin{myblock}{Proximities}


\begin{itemize}[$\bullet$]     
          \setlength{\itemindent}{+.3in}
 \item \textbf{Proximities:} Similarity measure derived from co-occurrence in terminal nodes across trees.
  \item For observations $\xi$ and $\xi[j]$:
  \[
    \operatorname{prox}(\xi, \xi[j]) = \frac{\text{\# trees where } \xi \text{ and } \xi[j] \text{ share a leaf}}{\text{\# total trees}}
  \]
  \item Defines an intrinsic, forest-induced similarity between instances.
  \item \textbf{Applications:} Missing data imputation, outlier detection, identifying mislabeled samples, forest visualization.
  \end{itemize}

% \textbf{Proximities} measure of similarity ("closeness" or "nearness") of observations derived from random forests.
% The proximity $\operatorname{prox}\left(\xi, \xi[j]\right)$ between two observations $\xi$ and $\xi[j]$ is calculated by measuring the number of times that these two observations are placed in the same terminal node of the same tree of random forest, 
% divided by the number of trees in the forest.
% It forms an intrinsic similarity measure between pairs of observations.

% \textbf{Proximities usage: }Imputing missing data, locating outliers, identifying mislabeled data, visualizing the forest.

\end{myblock}
}

  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}
