\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-trees.tex}
\input{../latex-math/ml-ensembles.tex}

\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Bagging Ensembles}

						\begin{codebox}
			\textbf{Basic Idea:}
						\end{codebox}
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
                        \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
                        \item An \textbf{ensemble method} that combines many models into one 
        big \enquote{meta-model}.
                        \item The constituent models of an ensemble are called \textbf{base learners} (BLs).
                        \item Homogeneous ensembles, i.e., all base learners are of the same type.
                        \end{itemize}
            Bagging improve performance by reducing the variance of predictions, but (slightly) increases the bias: training data is reused, so small mistakes can get amplified. Works best if BLs' predictions are only weakly correlated.
						
				

						\begin{codebox}
			      \textbf{Bagging Training: }
						\end{codebox}
						Base learners $\blm, m = 1, \dots, M$ are trained on $M$ \textbf{bootstrap} samples of training data $\D$. 
						\begin{algorithm}[H]
              \small
              \caption{Bagging algorithm: Training}
              \begin{algorithmic}[1]
                \State Input: Dataset $\D$, type of BLs, number of bootstraps $M$
                \For {$m = 1 \to M$}
                  \State Draw a bootstrap sample $\D^{[m]}$ from $\D$
                  \State Train BL on $\D^{[m]}$ to obtain model $\blh$
                \EndFor
              \end{algorithmic}
            \end{algorithm}

            \begin{codebox}
              \textbf{Bagging Aggregating: }
              \end{codebox}

            Aggregate the predictions of $M$ fitted BLs to get the ensemble model:
            \begin{align*}
              \hat{f}(\xv) &= \meanmM \left(\blfhx\right) \text{(regression / decision score, use $\hat{f}_k$ in multi-class)} \\[-1ex]
              \hxh &= \argmax_{k \in \Yspace} \summM \I \left(\bllhx = k\right) \text{(majority voting)} \\[-1ex]
            \pikxh &=
            \left\{
            \begin{aligned}
            & \meanmM \blphxk && \text{(probabilities through averaging)} \\[-1ex]
            & \meanmM \I \left(\bllhx = k \right) && \text{(probabilities through class frequencies)}
            \end{aligned}
            \right.
            \end{align*}

             \end{myblock}

             \begin{myblock}{Random Forests}
              \begin{codebox} \textbf{Introduction: }
              \end{codebox}
          
              \begin{itemize}[$\bullet$]     
                \setlength{\itemindent}{+.3in}
                \item Random Forests (RFs) use \textbf{bagging with CARTs} as BLs.
                \item \textbf{Random feature sampling} decorrelates the BLs.
                \item \textbf{Fully expanded trees} further increase variability of trees.
                \end{itemize}

              \end{myblock}
             
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{ }      
      Advantages: Less preprocessing; Can handle missing value; Easy to parallelize; Work well on high-dimensional / noisy data.\\
      Disadvantages: Extrapolation problem; Hard to interpret; Memory-hungry implementation; Computationally demanding for prediction of large ensembles.
        
      \begin{codebox} \textbf{Random feature sampling: }
      \end{codebox}
      \begin{itemize}[$\bullet$]     
        \setlength{\itemindent}{+.3in}
        \item For each node, randomly draw $\texttt{mtry} \le p$ features to find the best split.
        \item Previous analysis was simplified. The more we decorrelate by this,
           the more random the trees become.
        \item Classification: $\texttt{mtry}$ $ = \lfloor \sqrt{p} \rfloor$; Regression: $\texttt{mtry}$ $ = \lfloor p/3 \rfloor$
        \end{itemize}

        \begin{codebox} \textbf{Note:}
        \end{codebox}
        \begin{itemize}[$\bullet$]     
          \setlength{\itemindent}{+.3in}
          \item Other hyperparameters: $\texttt{Min. nr. of obs.}$ in each decision tree node, $\texttt{depth of each tree}$.
          \item RF usually use fully expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variability of each tree}.\\
          \item RFs are usually better if ensemble is large.
          \item RFs can overfit but generally less prone to than individual CARTs.
          \item Since each tree is trained \textit{individually and without knowledge of previously trained trees}, 
          increasing $\texttt{ntrees}$ generally reduces variance \textbf{without increasing the chance of overfitting}!
        \end{itemize}
      \end{myblock}

      \begin{myblock}{Out-of-Bag (OOB)}
       \begin{algorithm}[H]
              \small
              \caption{Out-Of-Bag error estimation to evaluate different RF hyperparameter configurations}
              \begin{algorithmic}[1]
                \State Input: OOB observations $\oobm = \{i \in \nset | \xyi \notin \D^{[m]}\}$, tree $\blh \ \forall m \in \{1, \dots, M\}$
                \For {$i = 1 \to n$}
                  \State Compute the ensemble OOB prediction for observation $i$, e.g., for regression:
                  $$\fh^{(i)}_{\oob} =
                  \frac{1}{S_{\oob^{(i)}}} \summM
                  \I(i \in \oobm) \cdot \blfh(\xi) $$

                  $S_{\mathrm{OOB}}^{(i)} = \summM \I(i \in \oobm)$ is the nr. of trees where $i$-th observation is OOB.
                \EndFor
                \State Average losses over all observations: $\GEh_{\oob} = \meanin L(\yi, \fh^{(i)}_{\oob})$
            
              \end{algorithmic}
            \end{algorithm}

            \textbf{OOB Size:} The probability that an observation is out-of-bag (OOB) is:
            $$\P \left(i \in \oobm\right) = \left(1 - \frac{1}{n}\right)^n \stackrel{n \to \infty}{\longrightarrow} \frac{1}{e} \approx 0.37$$
            $\Rightarrow$ similar to holdout or 3-fold CV (1/3 validation, 2/3 training)
            
\end{myblock}
}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{ }
\textbf{OOB Error Usage:}
Get first impression of RF performance; Select ensemble size; Evaluate different RF hyperparameter configurations.
\end{myblock}

  \begin{myblock}{Feature Importance}

    RFs improve accuracy by aggregating multiple decision trees but \textbf{lose interpretability} compared to a single tree. 
    \textbf{Feature importance} mitigates this problem by asking how much does performance decrease, if feature is removed / rendered useless.

    \begin{algorithm}[H]
      \small
      \caption{Feature importance based on permutations}
      \begin{algorithmic}[1]
        \State Calculate $\GEh_{\oob}$ using set-based metric $\rho$
        \For{features $x_j$, $j = 1 \to p$}
        \For{Some statistical repetitions}
          \State {Distort feature-target relation: permute $x_j$ with $\psi_j$}
          \State Compute all $n$ OOB-predictions for permuted feature data, obtain all $\fh^{(i)}_{\oob,\psi_j}$
          \State Arrange predictions in $\bm{\hat{F}}_{\oob,\psi_j}$;
          Compute $\GEh_{\oob, j} = \rho(\yv, \bm{\hat{F}}_{\oob,\psi_j})$
        \State {Estimate importance of $j$-th feature: $\widehat{\text{FI}_j} = \GEh_{\oob,j} - \GEh_{\oob
        } $}
        \EndFor
        \State Average obtained $\widehat{\text{FI}_j}$ values over reps
        \EndFor
      \end{algorithmic}
      \end{algorithm}

    \begin{algorithm}[H]
      \small
      \caption{Feature importance based on impurity}
      \begin{algorithmic}[1]
        \For{features $x_j$, $j = 1 \to p$}
        \For{all models $\blh$, $m = 1 \to M$}
        \State {Find all splits in $\blh$ on $x_j$}
        \State {Extract improvement / risk reduction for these splits}
        \State {Sum them up}
        \EndFor
        \State {Add up improvements over all trees for FI of $x_j$}
        \EndFor
      \end{algorithmic}
      \end{algorithm}
  \end{myblock}

\begin{myblock}{Proximities}
\textbf{Proximities} measure of similarity ("closeness" or "nearness") of observations derived from random forests.
The proximity $\operatorname{prox}\left(\xi, \xi[j]\right)$ between two observations $\xi$ and $\xi[j]$ is calculated by measuring the number of times that these two observations are placed in the same terminal node of the same tree of random forest, 
divided by the number of trees in the forest.
It forms an intrinsic similarity measure between pairs of observations.

\textbf{Proximities usage: }Imputing missing data, locating outliers, identifying mislabeled data, visualizing the forest.

\end{myblock}
}

  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}
