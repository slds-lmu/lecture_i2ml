\documentclass{beamer}

\input{../style/preamble-cheatsheets}

\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-boosting.tex}
\input{../latex-math/ml-automl.tex}

\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Hyperparameters}
						\textbf{Hyperparameters} $\lambdav$ are parameters that are \emph{inputs} to the training problem, in which a learner $\inducer$ minimizes the empirical risk on a training data set in order to find optimal \textbf{model parameters} $\bm{\theta}$ which define the fitted model $\fh$.
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
                        \item not decided during training rather must be specified before the training
                        \item an \textbf{input} of the training
                        \item often control the complexity of a model
                        \item can influence any structural property of a model or computational part of the training process
                        \end{itemize}

						\begin{codebox}
			\textbf{Types of hyperparameters: }
						\end{codebox}
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
						    \item Real-valued parameters: Minimal error improvement in a tree to accept a split
						    \item Integer parameters: Neighborhood size $k$ for $k$-NN
						    \item Categorical parameters: Which distance measure for $k$-NN
						\end{itemize}
						\end{myblock}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%						
						\begin{myblock}{Hyperparameter Tuning}

\textbf{(Hyperparameter) Tuning} is the process of finding good model hyperparameters

\begin{codebox}
\textbf{A bi-level optimization problem:}
\end{codebox}
The well-known risk minimization problem to find $\hat f$ is \textbf{nested} within the outer hyperparameter optimization (also called second-level problem). \textbf{Nested hyperparameter tuning problem:}
$$ \min_{\lambdav \in \Lambda} \GEh{\Dtest}\left(\inducer(\Dtrain, \lambdav)\right) $$
  
  \begin{codebox}
\textbf{Components of a tuning problem:}
\end{codebox}
The dataset, the learner(tuned), the learnerâ€™s hyperparameters and their respective regions-of-interest over which optimization is done, the performance measure, a (resampling) procedure for estimating the predictive performance.

\begin{codebox}
\textbf{Tuning is hard:}
\end{codebox}

Because, tuning is derivative-free which is a black-box problem. Every evaluation is \textbf{expensive} and the answer we get from that evaluation is \textbf{not exact, but stochastic} in most settings. The space of hyperparameters we optimize over has a non-metric, complicated structure
\end{myblock}\vfill
						
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  
  \begin{myblock}{Basic Techniques}
  
  \begin{codebox}
  \textbf{Grid Search:}
  \end{codebox}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Tries all hyperparameter combinations
  \item For each hyperparameter a finite set of candidates is predefined and searches all possible combinations in arbitrary order
  \item Grid Search over 10x10 points:
    \end{itemize}
  
  \begin{center}
  \includegraphics[width=0.5\columnwidth]{img/grid_1.PNG}
  \end{center}
  
  \textbf{Note:} It is very easy to implement, all parameter types possible and parallelizing computation is trivial. However, it scales badly and is inefficient.
  
  \begin{codebox}
  \textbf{Random Search:}
  \end{codebox}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Small variation of Grid Search.
  \item Uniformly sample from the region-of-interest
  \item Random Search over 10x10 points:
    \end{itemize}
  
  \begin{center}
  \includegraphics[width=0.5\columnwidth]{img/rand.PNG}
  \end{center}	
  \textbf{Note:} Very easy to implement, all parameter types possible, trivial parallelization and an anytime algorithm and no discretization. But, it is also inefficient and scales badly.
  
  \end{myblock}
  
  \begin{myblock}{Tuning}
  \begin{codebox} \textbf{Problem of Tuning}	\end{codebox}
  Need to \textbf{select an optimal learner} without compromising the 
  \end{myblock}
  }
\end{minipage}
\end{beamercolorbox}
\end{column}
  
  \begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{}
  \textbf{accuracy of the performance estimate} for that learner. For this \textbf{untouched test set} is needed.
  \begin{codebox} \textbf{Train-validation-test}	\end{codebox}
  A 3-way split is the simplest method:
    During tuning, a learner is trained on the \textbf{training set}, evaluated on the  \textbf{validation set} and after the best model configuration $\lambda^\star$ is selected, we re-train on the joint (training+validation) set and evaluate the model's performance on the \textbf{test set}.\\

\begin{center}
             \includegraphics[width=1\columnwidth]{img/tuning_1.PNG}
               \end{center}

\hspace*{1ex}

If we want to tune over a set of candidate HP configurations $\lambda_i; i = 1, \dots$ with 4-fold CV in the inner resampling and 3-fold CV in the outer loop. The outer loop is visualized as the light green and dark green parts.

 \begin{codebox} \textbf{Nested Resampling}	\end{codebox}
 
 \begin{center}
             \includegraphics[width=0.9\columnwidth]{img/tuning_2.PNG}
               \end{center}
The outer loop is visualized as the light green and dark green parts. This is with 4-fold CV in the inner resampling and 3-fold CV in the outer loop.

                    \end{myblock}



}
\end{minipage}
\end{beamercolorbox}
\end{column}

\end{columns}
\end{frame}
\end{document}
