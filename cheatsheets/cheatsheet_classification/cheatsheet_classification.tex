\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{				
					\begin{myblock}{Classification}
						We want to assign new observations to known categories according to criteria learned from a training set.  
						\vspace*{1ex}
						\begin{codebox}
						    Assume we are given a \textbf{classification problem:}
						\end{codebox}
						\begin{eqnarray*} & x \in \Xspace \quad & \text{feature vector}\\ & y \in \Yspace = \gset \quad & \text{\emph{categorical} output variable (label)}\\ &\D = \Dset & \text{observations of $x$ and $y$} \end{eqnarray*}
						
						\vspace*{1ex}
						
						\begin{codebox}
							Classification usually means to construct $g$ discriminant functions:
						\end{codebox}
						\hspace*{1ex}$f_1(\xv), \ldots, \fgx$, so that we choose our class as $h(\xv) = \argmax_k \fkx$ \hspace*{1ex}for $k = 1, 2,\ldots, g$
						
						\vspace*{1ex}
						
						\begin{codebox}
							\textbf{Linear Classifier:}
						\end{codebox}
						\hspace*{1ex}If the functions $\fkx$ can be specified as linear functions, we will call \hspace*{1ex}the classifier a \emph{linear classifier}.\\
						
						\hspace*{1ex}\textbf{Note: }All linear classifiers can represent non-linear decision boundaries \hspace*{1ex}in our original input space if we include derived features. For example: \hspace*{1ex}higher order interactions, polynomials or other transformations of x in \hspace*{1ex}the model.
			
			    \vspace*{1ex}
			
						\begin{codebox}
							 \textbf{Binary classification: }If only 2 classes exist
						\end{codebox}
						\hspace*{1ex}We can use a single discriminant function $\fx = f_{1}(\xv) - f_{2}(\xv)$.
          \end{myblock}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{myblock}{Generative Approach}
\begin{codebox}
\textbf{Generative approach} models $\pdfxyk$, usually by making some
\end{codebox}

\begin{codebox}
assumptions about the structure of these distributions and employs the
\end{codebox}

\begin{codebox}
Bayes theorem:
  \end{codebox}
\hspace*{1ex}$\pikx = \postk \propto \pdfxyk \pik$. It allows the computation of \hspace*{1ex}$\pikx$.

\begin{codebox}
  Examples of \textbf{Generative} approach:
    \end{codebox}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
    \item Linear discriminant analysis (LDA)
  \end{itemize}
					\end{myblock}\vfill
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
	\begin{column}{.31\textwidth}
	\begin{beamercolorbox}[center]{postercolumn}
	
	\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{}
  
%  \begin{codebox}
%  Examples of \textbf{Generative} approach:
 %   \end{codebox}
  
  \begin{itemize}[$\bullet$]    
  \setlength{\itemindent}{+.3in}
    \item Quadratic discriminant analysis (QDA)
    \item Naive Bayes
  \end{itemize}
  
  \vspace*{1ex}
  
  \begin{codebox}
  \textbf{Linear Discriminant Analysis (LDA): }follows a generative approach,
  \end{codebox}
  
  \begin{codebox}
  each class density is modeled as a \emph{multivariate Gaussian} with equal
  \end{codebox}
  
  \begin{codebox}
  covariance, i. e. $\Sigma_k = \Sigma \quad \forall k$.
  \end{codebox}
  
  \hspace*{1ex}Parameters $\theta$ are estimated in a straight-forward manner by estimating \hspace*{1ex}$\hat{\pi}_k$, $\hat{\mu}_k$, $\hat{\Sigma}$.
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
      \item Each class fit as a Gaussian distribution over the feature space
      \item Different means but same covariance for all classes
      \item Rather restrictive model assumption.
  \end{itemize}

  \vspace*{1ex}

  \begin{codebox}
  \textbf{Quadratic Discriminant Analysis (QDA): }is a direct generalization
  \end{codebox}
  
  \begin{codebox}
  of LDA, where the class densities are now Gaussians with unequal 
  \end{codebox}
  
  \begin{codebox}
  covariances $\Sigma_k$.
  \end{codebox}
  
  \hspace*{1ex}Parameters $\theta$ are estimated in a straight-forward manner by estimating \hspace*{1ex}$\hat{\pi}_k$, $\hat{\mu}_k$, $\hat{\Sigma_k}$.
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
      \item Covariance matrices can differ over classes.
      \item Yields better data fit but also requires estimation of more parameters.
  \end{itemize}
  
  \vspace*{1ex}
  
  \begin{codebox}
  \textbf{Naive Bayes classifier: }A "naive" conditional independence assumption
  \end{codebox}

  \begin{codebox}
  is made: the features given the category y are conditionally independent of
  \end{codebox}
  
  \begin{codebox}
  each other
  \end{codebox}
  
  \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
      \item Covariance matrices can differ over both classes but assumed to be diagonal.
      \item Assumption of uncorrelated features. Often performs well despite this usually wrong assumption.
      \item Easy to deal with mixed features (metric and categorical)
  \end{itemize}
  
  \end{myblock}
  
  \begin{myblock}{Discriminant  Approach}
  \begin{codebox}
  \textbf{Discriminant approach: }tries to optimize the discriminant functions
  \end{codebox}
  \end{myblock}
  }
  \end{minipage}
		\end{beamercolorbox}
	\end{column}
	
	
	\begin{column}{.31\textwidth}
	\begin{beamercolorbox}[center]{postercolumn}
	
	\begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{}
    
  \begin{codebox}
  directly, usually via empirical risk minimization:
  \end{codebox}
    
  \hspace*{1ex}$ \fh = \argmin_{f \in \Hspace} \riske(f) = \argmin_{f \in \Hspace} \sumin \Lxyi.$
    
    \begin{codebox}
  Examples of \textbf{Discriminant} approach:
    \end{codebox}
     \hspace*{1ex}\textbf{Discriminant Approach: }models $p(y|\bm{x})$ directly. Example: 
     
  \begin{itemize}[$\bullet$]     \setlength{\itemindent}{+.3in}
    %\renewcommand{\labelitemi}{$\bullet$}
      \item Logistic/Softmax regression
      \item kNN
  \end{itemize}
  
  \vspace*{1ex}
  
  \begin{codebox}
  \textbf{Logistic Regression: }A \emph{discriminant} approach for directly modeling
  \end{codebox}
  
  \begin{codebox}
  the posterior probabilities $\pix$ of the labels is \textbf{logistic regression}
  \end{codebox}
  
  \hspace*{1ex}We focus on the binary case $y \in \{0, 1\}$. We then model: \[ \pix = \post = \theta^T x .\] 
  \hspace*{1ex}and this could result in predicted probabilities $\pix \not\in [0,1]$. To avoid \hspace*{1ex}this, logistic regression \enquote{squashes} the estimated linear scores $\theta^T x$ to \hspace*{1ex}$[0,1]$ through the \textbf{logistic function} $s$:
  \vspace*{1ex}
    \[ \pix = \frac{\exp\left(\theta^Tx\right)}{1+\exp\left(\theta^Tx\right)} = \frac{1}{1+\exp\left(-\theta^Tx\right)} = s\left(\theta^T x\right) \]
    
  \vspace*{1ex}
  
  \begin{codebox}
  \textbf{Cross Entropy loss: }Minimizing it refers to maximizing the
  \end{codebox}
  
  \begin{codebox}
  probabilities of logistic regression, where the labels are $\Yspace = \{0,1\}$
    \end{codebox}
  
  \hspace*{1ex}$\Lxy = y \theta^T x - \log[1 + \exp(\theta^T x)]$
  
  \vspace*{1ex}
  \begin{codebox}
  \textbf{Bernoulli Loss: }If we encode the labels with $\Yspace = \{-1,+1\}$ we can
  \end{codebox}
  
  \begin{codebox}
  simplify the loss function as: $\Lxy = \log[1 + \exp(-\yf] $
    \end{codebox}
  \hspace*{1ex}Bernoulli loss is equivalent to Cross Entropy loss encoded differently.
  
  \vspace*{1ex}
  \begin{codebox}
  \textbf{Softmax: }is a generalization of the logistic function. It \enquote{squashes}
  \end{codebox}
  
  \begin{codebox}
  a g-dimensional real-valued vector $z$ to a vector of the same dimension,
  \end{codebox}
  
  \begin{codebox}
  with every entry in the range [0, 1] and all entries adding up to 1.
  \end{codebox}
  
  \hspace*{1ex}\emph{Softmax} is defined on a numerical vector $z$: $$ s(z)_k = \frac{\exp(z_k)}{\sum_{j}\exp(z_j)} $$
  \end{myblock}
}
\end{minipage}
		\end{beamercolorbox}
	\end{column}
\end{columns}
\end{frame}
\end{document}
