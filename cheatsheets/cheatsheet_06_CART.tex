\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-trees.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of supervised machine learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Introduction to CART}

						% \begin{codebox}
							\textbf{CART -- \textbf{C}lassification \textbf{A}nd \textbf{R}egression \textbf{T}rees:}
						% \end{codebox}
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
                        \item Divide feature space into sub-regions.
                        \item Learn best constant prediction from training data for each region.
            \end{itemize}

            \textbf{Advantages:} Automatic feature selection; Fast and scales well for larger data; Less preprocessing; Can model discontinuities and non-linearities.\\
            \textbf{Disadvantages:} Predictions are step functions, not smooth, cannot fit linear trends well; Empirically not the best; High instability (variance). 			
          
          % \begin{codebox}
          \textbf{Binary Trees:}
          % \end{codebox}
          \begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
            \item Represent a top-down hierarchy with binary splits that contains:
                 root node, internal nodes, and terminal nodes (leaves).
            \item Nodes have relative relationships: parent nodes and child nodes. 
            \item Root nodes don't have parents -- leaves don't have children.
          \end{itemize}

      % \begin{codebox}
        \textbf{CARTs:} 
      % \end{codebox}
      \begin{itemize}[$\bullet$]     
        \setlength{\itemindent}{+.3in}
        \item Use the structure of a binary tree
        \item Binary splits are constructed top-down in a \emph{data optimal} way.
        \item Each split is a threshold decision for a single feature.
        \item Each node contains the training points which follow its path.
        \item Each leaf contains a constant prediction.
   \end{itemize}

      % \begin{codebox}
        \textbf{Trees as an additive model:}
      % \end{codebox}
      Divide the feature space $\Xspace$ with $M$ leaf nodes into \textbf{rectangular regions} $Q_m$:
      $$\fx = \sum_{m=1}^M c_m \I(x \in Q_m).$$
      $c_m$: predicted numerical response, class label or class distribution in respective leaf node.
\end{myblock}

\begin{myblock}{Growing a Tree}
  
  \textbf{Greedy optimization}: At each node $\Np$, find the locally optimal split by exhaustively evaluating all possible splits on all thresholds $t$ for all features $x_j$ in terms of empirical risk $\risk(\Np,j,t)$. Training data is then distributed to child nodes according to this split, which is never reconsidered.
  \begin{itemize}[$\bullet$]     
    \setlength{\itemindent}{+.3in}
    \item Start with an empty tree, a root node contains all data.
    \item Search for feature and split point that minimizes the empirical risk in child nodes --  makes label distribution more homogenous.
    \item Proceed recursively for each child node: select best split and divide data from parent node into left and right child nodes.
    \item Repeat until a stop criterion, e.g., until each leaf cannot be split.
    \item Greedy optimization does not guarantee to find the global optimum.
  \end{itemize}

\end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{Splitting Criteria}
    Use \textbf{empirical risk minimization:}

    $\Np \subseteq \D$: data contained in this node. $c_{\Np}$: predicted constant for $\Np$.\\
    The risk $\risk(\Np)$ for a node is:
    $\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} L(y, c_{\Np})$.\\
    The optimal constant is:
    $c_{\Np} = \argminlim_c \sum\limits_{(\xv, y) \in \Np} L(y, c)$

    A split w.r.t. feature $x_j$ at split point $t$ divides a parent node $\Np$ into 
    $$\Nl = \{ (\xv, y) \in \Np: x_j < t \} \text{ and } \Nr = \{ (\xv, y) \in \Np: x_j \geq t \}.$$
    Finding the best way to split $\Np$ into $\Nl, \Nr$ means solving
    $$\argmin_{j, t} \risk(\Np, j, t) = \argmin_{j, t} \risk(\Nl) +  \risk(\Nr)$$
    If use averages $\frac{1}{|\Np|}$, we have to reweight the terms to obtain a global average w.r.t. $\Np$ as the children have different sizes
    $$\bar{\risk}(\Np, j, t) = \frac{|\Nl|}{|\Np|} \bar{\risk}(\Nl) + \frac{|\Nr|}{|\Np|} \bar{\risk}(\Nr)$$

    % \begin{codebox}
    \textbf{Splitting criteria:}
    % \end{codebox}
    \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
    \item Regression trees --- $L_2$ loss.
    \item Classification trees --- Brier score.\\
    Minimize Brier score $\iff$ Minimize \textbf{Gini impurity}:
    $$I(\Np) = \sum_{k=1}^g \pikNh(1-\pikNh)$$
    \item Classification trees --- Bernoulli loss.\\
    Minimize Bernoulli loss $\iff$ Minimize \textbf{entropy impurity}:
    $$I(\Np) = -\sum_{k=1}^g \pikNh \log \pikNh$$
    \end{itemize}

    For classification, predicted probabilities in node $\Np$ are the class proportions in the node:
    $$ \pikNh = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} \I(y = k) $$
    Brier score and Bernoulli loss are more sensitive to changes in the node probabilities, and therefore often preferred over misclassification loss.
  \end{myblock}


}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{Split computation}
  
    % \begin{codebox}
    \textbf{Monotone feature transformations:}
    % \end{codebox}
    Only change numerical value of the split point, not the distribution into left and right child node.

    % \begin{codebox}
      \textbf{Categorical Features:}	
      % \end{codebox}
      
  % \end{myblock}  
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item A split on a categorical feature partitions the feature levels:
    $$x_j \in \{a,c,e\} \leftarrow \Np \rightarrow x_j \in \{b,d\} $$
  \item A feature with $m$ levels results in about $2^m$ different possible binary partitions ($2^{m-1} - 1$ because of symmetry and empty groups).
  \item For L2 regression and binary classification, clever shortcuts are available that treat the feature as ordinal and only require checking $m-1$ splits.
  \end{itemize}

  % % \begin{codebox}
  % \textbf{Continuous responses, in each node: }
  % % \end{codebox}
  
  % \begin{itemize}[$\bullet$]     
  % \setlength{\itemindent}{+.3in}
  % \item Calculate the mean of the outcome in each category.
  % \item Sort the categories by increasing mean of the outcome.
  % \end{itemize}

  % \begin{codebox}	
  \textbf{Missing Feature values: }	
  % \end{codebox}
  Use \textbf{surrogate splits} to define replacement splitting rules with a different feature, 
  that result in almost the same child nodes as the original split.

  \end{myblock}
  
  \begin{myblock}
    {Overfitting}

  \textbf{Reduce overfitting:}
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Use a less deep tree.
  \item Define different \textbf{stopping criteria}.
  \item \textbf{Pruning}: pre-pruning or post-pruning.
  \end{itemize}
  
  % \begin{codebox}
    \textbf{Cost-complexity pruning (CCP): }
  % \end{codebox}
  Post-pruning method to grow a large tree and remove least informative leaves.
  CCP is steered with regularization parameter $\alpha$ that penalizes number of leaves in a subtree
$$\riskr(T) = \sum_{m=1}^{|T|} \sum_{i: x^{(i)} \in Q_m} L(y^{(i)}, c_m) + \alpha |T|,$$
$|T|$: number of leaves of subtree $T$. $Q_m$: subset of the feature space, $m$-th terminal node. $c_m$: m-th node prediction.

CCP performs a greedy backward search:
\begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
\item Computes $\riskr(T)$ with a fixed $\alpha$ for all possible subtrees that can be created by replacing one internal node with a leaf.
\item By replacing a node we also eliminate all subsequent nodes.
\item Select the subtree with lowest risk and repeat the procedure.
\item Stop if pruning does not further reduce the risk.
\end{itemize}
Hyperparameter $\alpha$ is typically selected via cross-validation.

  \end{myblock}
  }
  
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}
