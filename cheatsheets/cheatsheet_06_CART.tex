\documentclass{beamer}
\newcommand \beameritemnestingprefix{}


\usepackage[orientation=landscape,size=a0,scale=1.4,debug]{beamerposter}
\mode<presentation>{\usetheme{mlr}}


\usepackage[utf8]{inputenc} % UTF-8
\usepackage[english]{babel} % Language
\usepackage{hyperref} % Hyperlinks
\usepackage{ragged2e} % Text position
\usepackage[export]{adjustbox} % Image position
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}


\input{../latex-math/basic-math.tex}
\input{../latex-math/basic-ml.tex}
\input{../latex-math/ml-trees.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\newlength{\columnheight} % Adjust depending on header height
\setlength{\columnheight}{84cm} 

\newtcolorbox{codebox}{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	hbox}

\newtcolorbox{codeboxmultiline}[1][]{%
	sharp corners,
	leftrule=0pt,
	rightrule=0pt,
	toprule=0pt,
	bottomrule=0pt,
	#1}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Introduction to CART}

						\begin{codebox}
							\textbf{CART - \textbf{C}lassification \textbf{A}nd \textbf{R}egression \textbf{T}rees}
						\end{codebox}
						\begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
                        \item Divide feature space into sub-regions.
                        \item Learn best constant prediction from training data for each region.
            \end{itemize}

            \textbf{Advantages:} Automatic feature selection; Fast and scales well larger data; Less preprocessing; Can model discontinuities and non-linearities.\\
            \textbf{Disadvantages:} Linear dependencies; Predictions are step functions, never smooth; Empirically not the best; High instability (variance). 			
          
          \begin{codebox}
          \textbf{Binary Trees}
          \end{codebox}
          \begin{itemize}[$\bullet$]     
            \setlength{\itemindent}{+.3in}
            \item Represent a top-down hierarchy with binary split that contains:
                 root node, internal nodes, and terminal nodes (leaves).
            \item Nodes have relative relationships: parent nodes and child nodes. 
            \item Root nodes don't have parents -- leaves don't have children.
          \end{itemize}

      \begin{codebox}
        \textbf{Classification Trees} - use the structure of a binary tree
      \end{codebox}
      \begin{itemize}[$\bullet$]     
        \setlength{\itemindent}{+.3in}
        \item Binary splits are constructed top-down in a \emph{data optimal} way.
        \item Each split is a threshold decision for a single feature.
        \item Each node contains the training points which follow its path.
        \item Each leaf contains a constant prediction.
   \end{itemize}

      \begin{codebox}
        \textbf{Trees as an additive model: }
      \end{codebox}
      Divide the feature space $\Xspace$ with $M$ leaf nodes into \textbf{rectangular regions}:
      $$\fx = \sum_{m=1}^M c_m \I(x \in Q_m).$$
      $c_m$: predicted numerical response, class label or class distribution in respective leaf node.
\end{myblock}

\begin{myblock}{Growing a Tree}
  
  Recursively applying \emph{greedy} optimization to each node $\Np$. 
  Greedy means \textbf{exhaustive search}: all possible splits of $\Np$ on all possible points $t$ for all features $x_j$ are compared in terms of empirical risk $\risk(\Np, j, t)$.
  Training data is then distributed to child nodes according to the optimal split.
  \begin{itemize}[$\bullet$]     
    \setlength{\itemindent}{+.3in}
    \item Start with an empty tree, a root node contains all data.
    \item Search for feature and split point that minimizes the empirical risk in child nodes --  makes label distribution more homogenous.
    \item Proceed recursively for each child node: select best split and divide data from parent node into left and right child nodes.
    \item Repeat until a stop criterion, e.g., until each leaf cannot be split.
  \end{itemize}

\end{myblock}
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{Splitting Criteria}
    Use \textbf{empirical risk minimization}.

    $\Np \subseteq \D$: data contained in this node. $c_{\Np}$: predicted constant for $\Np$.\\
    The risk $\risk(\Np)$ for a node is:
    $\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} L(y, c_{\Np})$.\\
    The optimal constant is:
    $c_{\Np} = \argminlim_c \sum\limits_{(\xv, y) \in \Np} L(y, c)$

    A split w.r.t. feature $x_j$ at split point $t$ divides a parent node $\Np$ into 
    $$\Nl = \{ (\xv, y) \in \Np: x_j < t \} \text{ and } \Nr = \{ (\xv, y) \in \Np: x_j \geq t \}.$$
    Finding the best way to split $\Np$ into $\Nl, \Nr$ means solving
    $$\argmin_{j, t} \risk(\Np, j, t) = \argmin_{j, t} \risk(\Nl) +  \risk(\Nr)$$
    If use averages $\frac{1}{|\Np|}$, we have to reweight the terms to obtain a global average w.r.t. $\Np$ as the children have different sizes
    $$\bar{\risk}(\Np, j, t) = \frac{|\Nl|}{|\Np|} \bar{\risk}(\Nl) + \frac{|\Nr|}{|\Np|} \bar{\risk}(\Nr)$$

    \begin{codebox}
    \textbf{Splitting criteria}\end{codebox}
    \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
    \item Regression trees --- $L_2$ loss.
    \item Classification trees --- Brier score.\\
    Minimize Brier score $\iff$ Minimize \textbf{Gini impurity}:
    $$I(\Np) = \sum_{k=1}^g \pikNh(1-\pikNh)$$
    \item Classification trees --- Bernoulli loss.\\
    Minimize Bernoulli loss $\iff$ Minimize \textbf{entropy impurity}:
    $$I(\Np) = -\sum_{k=1}^g \pikNh \log \pikNh$$
    \end{itemize}

    For classification, predicted probabilities in node $\Np$ are the class proportions in the node:
    $$ \pikNh = \frac{1}{|\Np|} \sum\limits_{(x,y) \in \Np} \I(y = k) $$
    Brier score and Bernoulli loss are more sensitive to changes in the node probabilities, and therefore often preferred than misclassification loss.
  \end{myblock}

  \begin{myblock}{Split computation}
  
    \begin{codebox}
    \textbf{Monotone feature transformations of feautures: }
    \end{codebox}
    Will only change the numerical value of the split point.

    \begin{codebox}
      \textbf{Categorical Features}	
      \end{codebox}
      
  \end{myblock}  
}
\end{minipage}
\end{beamercolorbox}
\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{column}{.31\textwidth}
\begin{beamercolorbox}[center]{postercolumn}
\begin{minipage}{.98\textwidth}
\parbox[t][\columnheight]{\textwidth}{
  \begin{myblock}{ }
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item A split on a categorical feature partitions the feature levels:
    $$x_j \in \{a,c,e\} \leftarrow \Np \rightarrow x_j \in \{b,d\} $$
    \item A feature with $m$ levels results in about $2^m$ different possible binary partitions ($2^{m-1} - 1$ because of symmetry and empty groups).
  \end{itemize}

  \begin{codebox}
  \textbf{$0-1$ responses, in each node: }
  \end{codebox}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Calculate the proportion of $1$-outcomes for each category.
  \item Sort the categories according to these proportions.
  \item Can then treat feature as ordinal, check at most $m-1$ splits.
  \end{itemize}

  \begin{codebox}
  \textbf{Continuous responses, in each node: }
  \end{codebox}
  
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Calculate the mean of the outcome in each category.
  \item Sort the categories by increasing mean of the outcome.
  \end{itemize}

  \begin{codebox}	
  \textbf{Missing Feature values: }	\end{codebox}
  Use \textbf{surrogate splits} to define replacement splitting rules, with a different feature 
  that result in almost the same child nodes as original split.

  \end{myblock}
  
  \begin{myblock}
    {Overfitting}

  Reduce overfitting:
  \begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
  \item Use a less deep tree.
  \item Define different \textbf{stopping criteria}.
  \item \textbf{Pruning}: pre-pruing or post-pruning.
  \end{itemize}
  
  \begin{codebox}
    \textbf{Cost-complexity pruning (CCP): }
  \end{codebox}
  A post-prunig method to grow a large tree and remove the least informative leaves.
  CCP is steered with a regularization parameter $\alpha$ that penalizes the number of leaves in a sub tree
$$\riskr(T) = \sum_{m=1}^{|T|} \sum_{i: x^{(i)} \in Q_m} L(y^{(i)}, c_m) + \alpha |T|,$$
$|T|$: number of leaves of sub tree $T$. $Q_m$: subset of the feature space, $m$-th terminal node. $c_m$: m-th node prediction. $T_0$: the complete tree.

CCP performs a greedy backward search:
\begin{itemize}[$\bullet$]     
  \setlength{\itemindent}{+.3in}
\item Computes $\riskr(T)$ with a fixed $\alpha$ for all possible sub trees that can be created by replacing one internal node with a leaf.
\item By replacing a node we also eliminate all subsequent nodes.
\item Select the sub tree with lowest risk and repeat the procedure.
\item Stop if pruning does not further reduce the risk.
\end{itemize}
Hyperparameter $\alpha$ is typically selected via cross-validation.

  \end{myblock}
  }
  
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  
  
\end{columns}
\end{frame}
\end{document}
