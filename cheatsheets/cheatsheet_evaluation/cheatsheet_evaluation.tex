\documentclass{beamer}

\input{../../style/preamble-cheatsheets}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{I2ML :\,: CHEAT SHEET} % Package title in header, \, adds thin space between ::
\newcommand{\packagedescription}{ % Package description in header
	The \textbf{I2ML}: Introduction to Machine Learning course offers an introductory and applied overview of "supervised" Machine Learning. It is organized as a digital lecture.
}

\begin{document}
\begin{frame}[fragile]{}
\begin{columns}
	\begin{column}{.31\textwidth}
		\begin{beamercolorbox}[center]{postercolumn}
			\begin{minipage}{.98\textwidth}
				\parbox[t][\columnheight]{\textwidth}{
					\begin{myblock}{Performance Evaluation}

						\begin{codebox}
							\textbf{Performance Evaluation} refers to estimating performance on new
						\end{codebox}
						
						\begin{codebox}
							data.
						\end{codebox}
						
					    \vspace*{1ex}
						
						\begin{codebox}
							\textbf{Different levels of randomness:}
						\end{codebox}
						
						\begin{itemize}[$\bullet$]
						  \setlength{\itemindent}{+.3in}
              \item The sample can be too small, then our estimator will be of high variance or if the sample could not be from the distribution of interest, then our estimator will be biased.
               \item Many learning algorithms are stochastic. Example: Random forest, Stochastic gradient descent
            \end{itemize}
						
						\vspace*{1ex}
	
						\begin{codebox}
							 \textbf{Metrics: inner vs. outer loss}
						\end{codebox}
						Inner loss is used in learning and outer loss is used in evaluation. Optimally inner loss should always match outer loss. But this is not always possible because some losses are hard to optimize. 
				
					\end{myblock}
					%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
					\begin{myblock}{Simple Metrics}
					\begin{codebox}
							\textbf{Metrics for Label based prediction methods: }Accuracy, MCE, Costs,
						\end{codebox}
						\begin{codebox}
							 Confusion matrix, F1 measure, ROC curve, Precision, Recall etc.
						\end{codebox}
						
						\vspace*{1ex}
						
						\begin{codebox}
						\textbf{Metrics for Probabilistic prediction methods: }Brier Score, Log-Loss 
                        \end{codebox}
                        \begin{codebox}
						etc. \vspace*{1ex}
                        \end{codebox}
                     \includegraphics[width=1.05\columnwidth]{img/table_metric.PNG}
						\vspace*{1ex}
					\end{myblock}\vfill
				}
			\end{minipage}
		\end{beamercolorbox}
	\end{column}
	
	
	\begin{column}{.31\textwidth}
  \begin{beamercolorbox}[center]{postercolumn}
  \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{
    \begin{myblock}{Train and Test Error}
        \begin{codebox}
    \textbf{Training Error: }estimated by the averaging error over the same data
    \end{codebox}
    \begin{codebox}
    set we fitted on
    \end{codebox}
    \hspace*{1ex}\textbf{Problems of training error: }
    
    \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
    \item Unreliable and overly optimistic estimator of future Performance Evaluation
    \item There are interpolators - interpolating splines, interpolating Gaussian processes - they are not necessarily good as they will also interpolate the noise
    \item Goodness-of-fit measures like $R^2$, likelihood, AIC, BIC etc are based on the training error
    \end{itemize}
    
    \vspace*{1ex}
    
    \begin{codebox}
    \textbf{Test Error: }the test is a good way to estimate future Performance
    \end{codebox}
    
    \begin{codebox}
     Evaluation, given that the test data is i.i.d. compared to the data we will 
    \end{codebox}
   
    \begin{codebox}
    see when we apply the model.
    \end{codebox}
    
    \hspace*{1ex}\textbf{Problems of test error:}
    
    \begin{itemize}[$\bullet$]
    \setlength{\itemindent}{+.3in}
    \item The estimator will suffer from high variance and be less reliable if the test set is too small
    \item Sometimes the test set is large, but one of the two classes is small
    \end{itemize}
    
    \vspace*{1ex}
    
    \begin{codebox}
    \textbf{Holdout Splitting: }It is a tool for estimating future Performance 
    \end{codebox}
    
    \begin{codebox}
     Evaluation. All of the models produced during that phase of  
    \end{codebox}
    
    \begin{codebox}
     evaluation are intermediate results.
    \end{codebox}
    
    \hspace*{1ex}
    \vspace*{1ex}	
    \includegraphics[width=1\columnwidth]{img/test_error.png}
    \end{myblock}
  }
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
  
  \begin{column}{.31\textwidth}
  \begin{beamercolorbox}[center]{postercolumn}
  \begin{minipage}{.98\textwidth}
  \parbox[t][\columnheight]{\textwidth}{
    \begin{myblock}{ }
    \begin{codebox}
    \textbf{Generalization error: }is a measure of how accurately an algorithm is
    \end{codebox}
    
    \begin{codebox}
    able to predict outcome values for previously unseen data
    \end{codebox}
    
    \vspace*{1ex}
    
    \begin{codebox}
    \textbf{Overfitting: }happens when our algorithm starts modelling patterns in
    \end{codebox}
    \begin{codebox}
    the data that are not actually true in the real world, e.g., noise in the
    \end{codebox}
    \begin{codebox}
    training data
    \end{codebox}
    
    \vspace*{1ex}
    
    \begin{codebox}	
    \textbf{Avoiding Overfitting: }Use less complex models, get more and better
    \end{codebox}
    
    \begin{codebox}	
    data, early stopping, regularization etc.
    \end{codebox}
   
    \end{myblock}
    
    \begin{myblock}{Resampling}
    The aim is to assess the Performance Evaluation of learning algorithm. Uses the data more efficiently and repeatedly splits in train and test, then average results.
    \vspace*{1ex}
    
    \begin{codebox}
    \textbf{Cross-validation: }Split the data into $k$ roughly equally-sized partitions.
    \end{codebox}
    
    \begin{codebox}
    Use each part once as test set and join the $k-1$ others for training, 
    \end{codebox}
    
    \begin{codebox}
    obtain $k$ test errors and average.
    \end{codebox}
    
    \vspace*{1ex}
    
    \begin{codebox}
    \textbf{Bootstrapping: }Randomly draw $B$ training sets of size $n$ with
    \end{codebox}
    \begin{codebox}
    replacement from the original training set $\Dtrain$
      \end{codebox}
      
    \vspace*{1ex}
    
    \begin{codebox}
    \textbf{Subsampling: }Repeated hold-out with averaging, a.k.a. monte-carlo 
    \end{codebox}
    
    \begin{codebox}
    CV. Similar to bootstrap, but draws without replacement
    \end{codebox}
    
    \end{myblock}
  }
  \end{minipage}
  \end{beamercolorbox}
  \end{column}
\end{columns}
\end{frame}
\end{document}
