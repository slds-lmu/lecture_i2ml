% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{999}{Code for Chapter 3}
\lecture{intro to ML}

\begin{vbframe}{Bias-Variance of holdout}

  Experiment:
  \begin{itemize}
    \item Data: simulate spiral data (sd = 0.1) from the \texttt{mlbench} package.
    \item Learner: CART (\texttt{classif.rpart} from \texttt{mlr}).
    \item Goal: estimate real performance of a model with $|\Dtrain| = 500$.
    \item Get the "true" estimator by repeatedly sampling 500 observations from the simulator,
      fit the learner, then evaluate on a really large number of observation.
    \item Analyse different types of holdout and subsampling (= repeated holdout), with different split rates:
    \begin{itemize}
    \item Sample $\D$ with $|\D| = 500$ and use split-rate $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
    \item Estimate performance on $\Dtest$ with $|\Dtest| = 500 \cdot (1 - s)$.
    \item Repeat the samping of $\D$ 50 times and the splitting with $s$ 50 times ($\Rightarrow$ 2500 experiments for each split-rate).
    \end{itemize}
  \end{itemize}

\framebreak

Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

<<>>=
load("rsrc/holdout-biasvar.RData")
@

<<echo = FALSE, fig.height = 5>>=
ggd1 = melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = ddply(ggd1, "split", summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = ddply(ggd1, c("split", "rep"), summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = ddply(ggd2, "split", summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)

ggd$type = as.factor(ggd$type)
pl1 = ggplot(ggd, aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
#pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))

gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)

pl2 = ggplot(gmse, aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)

grid.arrange(pl1 + theme_minimal(), pl2 + theme_minimal(), layout_matrix = rbind(1,1,2))
@


% \framebreak
%
%   \begin{itemize}
%     \item The training error decreases with smaller training set size as it is easier for the model to learn the underlying structure in smaller training sets perfectly.
%     \item The test error (its bias) decreases with increasing training set size as the model generalizes better with more data, however, the variance increases as the test set size decreases at the same time.
%     \item The variance of the test error should decrease if we repeat the hold-out more often. %(here 10 vs. 100 repetitions):
%
% <<echo = FALSE, cache = TRUE, eval = FALSE, out.width="0.85\\textwidth", fig.height=3>>=
% res = rbind(cbind(res.rpart, repetitions = 100), cbind(res.rpart.small, repetitions = 20))
% res$repetitions = as.factor(res$repetitions)
%
% p1 = ggplot(data = subset(res, measure == "1"), aes(x = percentage, y = mmce)) +
%   geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = repetitions), width = 0.025, position = position_dodge(width = 0.01)) +
%   geom_line(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   geom_point(aes(colour = repetitions), position = position_dodge(width = 0.01)) +
%   ylab("Test error") +
%   xlab("Training set percentage") +
%   theme_minimal()
% p1
% @
%
% \end{itemize}

\end{vbframe}



\begin{vbframe}{Benchmarking in \texttt{mlr}}

\begin{itemize}
\item In a benchmark experiment different learning methods are applied to one or several data sets with the aim to compare and rank the algorithms with respect to one or more performance measures.
\item It is important that the train and test sets are synchronized, i.e. all learning methods see the same data splits so that they are better comparable.
\end{itemize}

We will first create a list of tasks and a list of learners:

<<echo = TRUE>>=
data("BostonHousing", "mtcars", "swiss", package = c("mlbench", "datasets"))
tasks = list(
  makeRegrTask(data = BostonHousing, target = "medv"),
  makeRegrTask(data = swiss, target = "Fertility"),
  makeRegrTask(data = mtcars, target = "mpg")
)
learners = list(
  makeLearner("regr.rpart"),
  makeLearner("regr.randomForest"),
  makeLearner("regr.lm")
)
@


\framebreak

The \texttt{benchmark} function from \texttt{mlr} allows you to compare all tasks and learners w.r.t. one or more measures based on the resampling method specified in the \texttt{resamplings} argument:

<<echo = -1, cache = TRUE>>=
set.seed(1)
(bmr = benchmark(learners, tasks, resamplings = cv10, measures = mlr::mse))
@

\end{vbframe}

\begin{vbframe}{Benchmarking in \texttt{mlr}: Access Data}

<<echo = TRUE, cache = TRUE>>=
head(getBMRAggrPerformances(bmr, as.df = TRUE), 3)
head(getBMRPerformances(bmr, as.df = TRUE), 3)
head(getBMRPredictions(bmr, as.df = TRUE), 3)
@

\end{vbframe}

\begin{vbframe}{Visualizing Performances}

Inspect the distribution of the performance measures using box plots:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse)
@

\framebreak

The plot is a \texttt{ggplot2} object that can be changed (names, colors, etc.):

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRBoxplots(bmr, measure = mlr::mse, style = "violin") +
  aes(color = learner.id)
@

\framebreak

Visualize the aggregated performance values as dot plot:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr)
@

\framebreak

Plot the rankings of the aggregated performance values:

<<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
plotBMRSummary(bmr, trafo = "rank")
@

\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%          d3s4 ROC
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{ROC Analysis in R}
\begin{itemize}
  \item \texttt{generateThreshVsPerfData} calculates one or several performance measures for a sequence of decision thresholds from 0 to 1.
  \item It provides S3 methods for objects of class \texttt{Prediction}, \texttt{ResampleResult}
and \texttt{BenchmarkResult} (resulting from  \texttt{predict.WrappedModel}, \texttt{resample}
or \texttt{benchmark}).
  \item \texttt{plotROCCurves} plots the result of \texttt{generateThreshVsPerfData} using \texttt{ggplot2}.
  \item More infos \url{http://mlr-org.github.io/mlr-tutorial/release/html/roc_analysis/index.html}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
\scriptsize
<<echo=TRUE, message = FALSE>>=
set.seed(1)
# get train and test indices
n = getTaskSize(sonar.task)
train.set = sample(n, size = round(2/3 * n))
test.set = setdiff(seq_len(n), train.set)

# fit and predict
lrn = makeLearner("classif.lda", predict.type = "prob")
mod = train(lrn, sonar.task, subset = train.set)
pred = predict(mod, task = sonar.task, subset = test.set)
@
\normalsize
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
We calculate fpr, tpr and compute error rates:

\scriptsize
<<echo = TRUE>>=
df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
@
\normalsize
\begin{itemize}
  \item \texttt{generateThreshVsPerfData} returns an object of class \texttt{ThreshVsPerfData},
which contains the performance values in the \texttt{\$data} slot.
  \item By default, \texttt{plotROCCurves} plots the performance values of the first two measures passed
to \texttt{generateThreshVsPerfData}.
  \item The first is shown on the x-axis, the second on the y-axis.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Example 1: Single predictions}
\scriptsize
<<echo = TRUE, fig.align="center", fig.width = 5, fig.height = 5, out.width="0.55\\textwidth">>=
df = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
@
\normalsize

\framebreak

The corresponding area under curve auc can be calculated by

\scriptsize
<<echo = TRUE>>=
performance(pred, auc)
@

\normalsize
\texttt{plotROCCurves} always requires a pair of performance measures that are plotted against
each other.

\framebreak

If you want to plot individual measures vs. the decision threshold, use

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="0.9\\textwidth">>=
plotThreshVsPerf(df)
@
\normalsize
\end{vbframe}


\begin{vbframe}{Example 2: Benchmark Experiment}
\scriptsize
<<>>=
options(width = 200)
@
<<echo = TRUE>>=
lrn1 = makeLearner("classif.randomForest", predict.type = "prob")
lrn2 = makeLearner("classif.rpart", predict.type = "prob")

cv5 = makeResampleDesc("CV", iters = 5)

bmr = benchmark(learners = list(lrn1, lrn2), tasks = sonar.task,
  resampling = cv5, measures = list(auc, mmce), show.info = FALSE)
bmr
@
\normalsize

Calling \texttt{generateThreshVsPerfData} and \texttt{plotROCCurves} on the \texttt{BenchmarkResult}
produces a plot with ROC curves for all learners in the experiment.

\framebreak

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
df = generateThreshVsPerfData(bmr, measures = list(fpr, tpr, mmce))
plotROCCurves(df)
@
\framebreak

\scriptsize
<<echo = TRUE, fig.align="center", fig.height = 4, fig.width = 8, out.width="\\textwidth">>=
plotThreshVsPerf(df)
@
\end{vbframe}







\endlecture
