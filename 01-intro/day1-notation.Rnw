% Introduction to Machine Learning
% Day 1

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@
% Load all R packages and set up knitr
<<setup-r, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{1}{Notation and definitions}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Fundamental definitions and notation}

\begin{itemize}
  \item $\Xspace$ $p$-dim. input space, usually we assume $\Xspace = \R^p$, but
    categorical features can occur, too
\item $\Yspace$ target space,  \\
  e. g. $\Yspace = \R$, $\Yspace = \lbrace 0, 1 \rbrace$, $\Yspace = \lbrace -1, 1 \rbrace$, $\Yspace = \gset$, $\Yspace = \lbrace \textrm{label}_1 \ldots \textrm{label}_g \rbrace$
\item $x = \xvec \in \Xspace$ observation
\item $y \in \Yspace$ dependent variable (target, label, output)
\item $\P_{xy}$ joint probability distribution on $\Xspace \times \Yspace$
\item $\pdfxy$ or $\pdfxyt$ joint pdf for $x$ and $y$
\end{itemize}

\remark{
This lecture is mainly developed from a frequentist perspective. If parameters appear behind the |, this is
for better reading, and does not imply that we condition on them in a Bayesian sense (but this notation
would actually make a Bayesian treatment simple).
So formally, $p(x | \theta)$ should be read as $p_\theta(x)$ or $p(x, \theta)$ or $p(x; \theta)$.
}

\framebreak

\begin{itemize}
\item $\D = \Dset$ data set with $n$ observations
\item $\xyi$ $i$-th observation
\item $\Dtrain$, $\Dtest$ data for training and testing, often $\D = \Dtrain \dot{\cup} ~ \Dtest$
\item $\fx$ or $\fxt \in \R$ prediction function learnt on data, we might suppress $\theta$ in notation
\item $\hx$ or $\hxt \in \Yspace$ discrete prediction for classification (see later)
\item $\theta \in \Theta$ model parameters\\
  (some models may traditionally use different symbols)
\item $\Hspace$ hypothesis space, $f$ lives here, restricts the functional form of $f$
\item $\eps = y - \fx$ or $\epsi = \yi - \fxi$, residual in regression
\item $\yf$ or $\yfi$, margin for binary classification with  $\Yspace = \{-1, 1\}$ (see later)
\end{itemize}

\framebreak

\begin{itemize}
\item $\pikx = \postk$, posterior probability for class $k$, given $x$, in case of binary labels we might abbreviate
  $\pix = \post$
\item $\pi_k = \P(y = k)$, prior probability for class $k$, in case of binary labels we might abbreviate
  $\pi = \P(y = 1)$
\item $\LLt$ and $\llt$, Likelihood and log-Likelihood for a parameter $\theta$, based on a statistical model
\item $\fh$, $\hh$, $\pikxh$, $\pixh$ and $\thetah$, learned functions and parameters
% \item $\phi(x | \mu, \sigma^2)$, $\Phi(x | \mu, \sigma^2)$, pdf and cdf of the univariate normal distribution,
  % with $\phi(x)$ and $\Phi(x)$ for the N(0,1) standard normal case,
  % and $\phi(x | \mu, \Sigma)$ (or $\phi_p(x)$) for the (standard) multivariate case in $p$ dimensions

\end{itemize}

\lz

\remark{
With a slight abuse of notation we write random variables, e.g., $x$ and $y$, in lowercase, as normal
variables or function arguments. The context will make clear what is meant.
}

\framebreak



In the simplest case we have i.i.d. data $\D$, where the input and output space are both real-valued and one-dimensional.

\vspace{0.5cm}

<<fig.height=4>>=
qplot(wt, mpg, data = mtcars) + xlab("x") + ylab("y")
@

\framebreak

Design matrix and intercept term:

\begin{minipage}{0.45\textwidth}
$$
X  = \mat{x_1^{(1)} & \cdots & x_p^{(1)} \\
          \vdots    & \vdots & \vdots \\
          x_1^{(n)} & \cdots & x_p^{(n)}}
$$
\end{minipage}
\begin{minipage}{0.45\textwidth}
$$
X  = \mat{1      & x_1^{(1)} & \cdots & x_p^{(1)} \\
          \vdots & \vdots    & \vdots & \vdots \\
          1      & x_1^{(n)} & \cdots & x_p^{(n)}}
$$
\end{minipage}

\begin{itemize}
  \item $\xjb = \xjvec$ j-th observed feature vector.
  \item $\ydat = \yvec$ vector of target values.
  \item The right design matrix demonstrates the trick to encode the intercept via an additional
    constant-1 feature, so the feature space will be $(p+1)$-dimensional.
    This allows to simplify notation, e.g., to write $\fx = \theta^T x$, instead
    of $\fx = \theta^T x + \theta_0$.
\end{itemize}



\end{vbframe}

\begin{vbframe}{Binary label coding}

\remark{
Notation in binary classification can be sometimes confusing because of different coding styles,
and as we have to talk about predicted scores, classes and probabilities.
}

\lz

A binary variable can take only two possible values.
For probability / likelihood-based model derivations a 0-1-coding, for geometric / loss-based models
the -1+1-coding is often preferred.

\begin{itemize}
\item $\Yspace = \{0, 1\}$. Here, the approach often models $\pix$, the posterior probability for class 1 given $x$.
  Usually, we then define $\hx = [\pix \geq 0.5] \in \Yspace$.
\item $\Yspace = \{-1, 1\}$. Here, the approach often models $\fx$, a real-valued score from $\R$ given x.
  Usually, we define $\hx = \sign(\fx) \in \Yspace$, and we interpret
  |\fx| as \enquote{confidence} for the predicted class $\hx$.
\end{itemize}

\lz


\end{vbframe}
