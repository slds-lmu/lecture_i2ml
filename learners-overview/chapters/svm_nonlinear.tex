\begin{frame}{nonlinear SVM -- method summary}

\footnotesize

% \maketag{SUPERVISED} 
\maketag{CLASSIFICATION} \maketag[50]{REGRESSION} \maketag{NONPARAMETRIC} 
\maketag{BLACK-BOX}

\medskip

\highlight{General idea}
\begin{itemize}
  \item Move \textbf{beyond linearity} by mapping data to 
  transformed space where they are linearly separable
  \item \textbf{Kernel trick} %\textcolor{blue}{(based on Mercer's theorem,  existence of reproducing kernel Hilbert space)}: 
  \begin{itemize}
    % \item Replace two-step operation feature map $\phi$ $\leadsto$ inner product 
    % by \textbf{kernel} $k: \Xspace \times \Xspace \rightarrow \R$, s.t.
    % $\scp{\phix}{\phixt} = \kxxt$
    \item No need for explicit construction of feature maps
    \item Replace inner product of feature map $\phi$ by \textbf{kernel}: $\scp{\phix}{\phixt} = \kxxt$
  \end{itemize}
  %\item Loss of interpretability through nonlinear feature map
\end{itemize}


\medskip

% \operatorname{sign}(\mathbf{w} \cdot \Phi(\mathbf{x})+b)

\highlight{Hypothesis space} ~~
% $\Hspace = \left \{ \fx = \sumin \alpha_i \yi k(\xi, \xv)  + \theta_0 ~|~
% \theta_0, \alpha_i \in \R ~ \forall i \right \} $
%\textcolor{blue}{$\Hspace = \{ \operatorname{sign}(\sumin \alpha_i \yi k(\xi, \xv)  + \theta_0) |\ (\theta_0, \thetab) \in \R^{p+1} \} $}
%$\left \{ \fx = \sumin \alpha_i \yi  k(\xi, \xv)   +     \theta_0 ~|~ \alpha_i \geq 0 ~ \forall i \right \}$

$\Hspace  = \left \{\fx ~:~ \fx = \sign \left( \sumin \alpha_i \yi k(\xi, \xv) + \theta_0 \right) ~|~ \alpha_i \geq 0,  \sumin \alpha_i \yi = 0 \right \}$\\
$\Rightarrow$ \textbf{Note:} Non-SVs have $\alpha_i = 0$ as they do not affect the hyperplane

% \langle \phi \left( \xi \right), \phi(\xv) \rangle
\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.24\textwidth}
          \centering
  \includegraphics[width=0.8\textwidth]{
  figure/circles_ds.png} \\
  \tiny{Nonlinear problem in original space} 
    \end{column}
    \begin{column}{0.75\textwidth}
          \centering
  \includegraphics[width=0.9\textwidth, trim=0 30 0 50, clip]{
  figure/circles_feature_map.png} \\
  \tiny{Mapping to 3D space and subsequent linear separation -- implicitly handled by kernel in nonlinear SVM}
    \end{column}
\end{columns}

% \begin{minipage}[t]{0.33\textwidth}
%   \centering
%   \includegraphics[width=0.5\textwidth]{
%   figure/circles_ds.png} \\
%   \tiny{Nonlinear problem in original space} 
% \end{minipage}
% \begin{minipage}[t]{0.66\textwidth}
%   \centering
%   \includegraphics[width=0.9\textwidth, trim=0 30 0 0, clip]{
%   figure/circles_feature_map.png} \\
%   \tiny{Mapping to 3D space and subsequent linear separation -- implicitly 
%   handled by kernel in nonlinear SVM}
% \end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{nonlinear SVM -- method summary}

\footnotesize

\highlight{Dual problem} ~~ \textbf{Kernelize} dual (soft-margin) SVM problem, 
replacing all inner products by kernels:
$$\max_{\alphav} \sumin \alpha_i - \frac{1}{2}\sumin \sumjn
\alpha_i\alpha_j\yi y^{(j)} \textcolor{blue}{k(\xi, \xi[j])}, ~~ \text{s.t. } ~~ 
0 \le \alpha_i \le C, ~~ \sumin \alpha_i \yi = 0.
$$

\medskip

\highlight{Hyperparameters} ~~ Cost $C$ of margin violations, kernel 
hyperparameters (e.g., width of RBF kernel)

\medskip

\begin{columns}[T, totalwidth=\textwidth]
    \begin{column}{0.59\textwidth}
        
\highlight{Interpretation as basis function approach}
  \begin{itemize}
    \item \textbf{Representer theorem:} solution of dual soft-margin SVM problem is
    $\thetab = \sum_{j=1}^n \beta_j \phi (\xi[j] )$ \\
    \item Sparse, weighted sum of \textbf{basis functions}\\
    $\rightarrow \beta_j = 0$ 
    for non-SVs
    \item Result: \textbf{local} model with smoothness depending on kernel
  \end{itemize}
    \end{column}
        \begin{column}{0.4\textwidth}
        \centering
  \includegraphics[width=\textwidth, trim=50 100 50 150, clip]{
  figure/svm_rbf_as_basis.png} \\
  \tiny{RBF kernel as mixture of Gaussian basis functions, forming
  bumpy, nonlinear decision surface to discern red and green points}
    \end{column}
\end{columns}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{nonlinear SVM -- Implementation \& Practical hints}

\footnotesize

\highlight{Common kernels}

\begin{itemize}
  \item \textbf{Linear} kernel: dot product of given observations ~~ 
  $\Rightarrow \kxxt = \xv^\top \xtil$ ~~ $\Rightarrow$ linear SVM
  \item \textbf{Polynomial} kernel of degree $d \in \N$: monomials (i.e., 
  feature interactions) up to $d$-th 
  order ~~$\Rightarrow 
  \kxxt = \left(\xv^\top \xtil + b \right)^d, ~ b \geq 0$
  \item \textbf{Radial basis function (RBF)} kernel: infinite-dimensional 
  feature space, allowing for perfect separation of all finite 
  datasets ~~ $\Rightarrow \kxxt = \exp \left( -\gamma \| \xv - \xtil \|_2^2 
  \right )$ with 
  bandwidth parameter $\gamma > 0$
\end{itemize}
 
\medskip

 \highlight{Tuning}
 
 \begin{itemize}
  \item High sensitivity w.r.t. hyperparameters, especially those of kernel
  ~~ $\Rightarrow$ \textbf{tuning} very important
  \item For RBF kernels, use \textbf{RBF sigma heuristic} to determine 
  bandwidth
\end{itemize}

  \medskip

\highlight{Implementation} 
\begin{itemize}
  \item \textbf{R:} \texttt{mlr3} learners \texttt{LearnerClassifSVM} /
  \texttt{LearnerRegrSVM}, calling \texttt{e1071::svm()} with nonlinear kernel (\texttt{libSVM} interface),
  \texttt{kernlab::ksvm()} allowing custom kernels
  \item \textbf{Python:} \texttt{sklearn.svm.SVC} from package 
  \texttt{scikit-learn} / package \texttt{libSVM}
\end{itemize}

\end{frame}
% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------

\begin{frame}{SVM -- Pro's \& Con's}

\begin{columns}[T, totalwidth=\textwidth]
  \begin{column}{0.5\textwidth}
    \highlight{Advantages}
    \footnotesize
    \begin{itemize}
      % \positem High \textbf{accuracy}
      \positem Often \textbf{sparse} solution (w.r.t. observations)
      \positem Robust against overfitting (\textbf{regularized}); especially in 
      high-dimensional space
      \positem \textbf{Stable} solutions (w.r.t. changes in train data)\\
      $\rightarrow$ Non-SV do not affect decision boundary
      \positem Convex optimization problem \\
      $\rightarrow$ local minimum $\hat{=}$ global minimum
      %\positem \textbf{memory efficient} (only use non-SVs)
    \end{itemize}
    
    % \highlight{Advantages (nonlinear SVM)}
    % \begin{itemize}
    %    \positem Can learn \textbf{nonlinear decision boundaries}
    %    \positem \textbf{Very flexible} due to custom kernels \\
    %    $\rightarrow$ RBF kernel yields local model \\
    %    $\rightarrow$ kernel for time series, strings etc.
    % \end{itemize}
  \end{column}

  \begin{column}{0.5\textwidth}
    \highlight{Disadvantages}
    \footnotesize
    \begin{itemize}
      \negitem \textbf{Long} training times $\rightarrow O(n^2 p + n^3)$
      %\negitem \textbf{Limited scalability} to larger data sets 
      %\textcolor{blue}{\textbf{??}}
      \negitem Confined to \textbf{linear model}
      \negitem Restricted to \textbf{continuous features}
      \negitem Optimization can also fail or get stuck
      % \negitem Poor \textbf{interpretability}
      %\negitem No handling of \textbf{missing} data
    \end{itemize}

    %     \highlight{Disadvantages (nonlinear SVM)}
    % \begin{itemize}
    %    \negitem Poor \textbf{interpretability} due to complex kernel
    %    \negitem \textbf{Not easy tunable} as it is highly important to choose the right kernel (which also introduces further hyperparameters)
    % \end{itemize}
  \end{column}
\end{columns}



\begin{columns}[b, totalwidth=\textwidth]
  \begin{column}{0.5\textwidth}    
    \highlight{Advantages (nonlinear SVM)}
    \begin{itemize}
       \positem Can learn \textbf{nonlinear decision boundaries}
       \positem \textbf{Very flexible} due to custom kernels \\
       $\rightarrow$ RBF kernel yields local model \\
       $\rightarrow$ kernel for time series, strings etc.
    \end{itemize}
  \end{column}

  \begin{column}{0.5\textwidth}

        \highlight{Disadvantages (nonlinear SVM)}
    \begin{itemize}
       \negitem Poor \textbf{interpretability} due to complex kernel
       \negitem \textbf{Not easy tunable} as it is highly important to choose the right kernel (which also introduces further hyperparameters)
    \end{itemize}
  \end{column}
\end{columns}

% \conclbox{Very accurate solution for high-dimensional data that is linearly 
% separable}

\end{frame}

% \begin{frame}{nonlinear SVM -- Pro's \& Con's}

% \footnotesize

% \begin{columns}[onlytextwidth]
%   \begin{column}{0.5\textwidth}
%     \highlight{Advantages}
%     \footnotesize
%     \begin{itemize}
%       %\positem high \textbf{accuaracy}
%       \positem Can learn \textbf{nonlinear decision boundaries}
%       \positem Often \textbf{sparse} solution (w.r.t. observations)
%       \positem Robust against overfitting (\textbf{regularized}); especially in 
%       high-dimensional space 
%       \item \textbf{Stable} solutions (w.r.t. changes in train data)\\
%       $\rightarrow$ Non-SV do not affect decision boundary
%       \positem \textbf{Very flexible} due to custom kernels \\
%       $\rightarrow$ kernel for time series, strings etc.
%     \end{itemize}
%   \end{column}

%   \begin{column}{0.5\textwidth}
%     \highlight{Disadvantages}
%     \footnotesize
%     \begin{itemize}
%       \negitem \textbf{Costly} implementation; long training times
%       %\negitem does not scale well to \textbf{larger data sets} 
%       %\textcolor{blue}{\textbf{??}}
%       \negitem Poor \textbf{interpretability} due to complex kernel
%       %\item[$\textbf{\textcolor{gray!80}{-}}$] very memory-intensive
%       \negitem \textbf{Not easy tunable} as it is highly important to choose the 
%       right kernel
%       %\negitem No handling of \textbf{missing} data
%     \end{itemize}
%   \end{column}
% \end{columns}

% \vfill

% \small

% \conclbox{nonlinear SVMs perform very well for nonlinear separable data, but are 
% hard to interpret and need a lot of tuning.}

% \end{frame}
