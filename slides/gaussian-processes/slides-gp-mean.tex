\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-gp}

\newcommand{\titlefigure}{figure_man/post-variance.png} %does not fit
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Mean Functions for Gaussian Processes}
\lecture{Introduction to Machine Learning}


\begin{vbframe}{The Role of Mean Functions}

\begin{itemize}
  \item It is common but by no means necessary to consider GPs with a zero-mean function 
  $$
    m(\xv) \equiv 0
  $$
  \item Note that this is not necessarily a drastic limitation, since the mean of the posterior process is not confined to be zero 
  $$
    \bm{f}_* | \Xmat_*, \Xmat, \bm{f} \sim \mathcal{N}(\Kmat_{*}^{T}\Kmat^{-1}\bm{f}, \Kmat_{**} - \Kmat_*^T \Kmat ^{-1}\Kmat_*).
  $$
  \item Yet there are several reasons why one might wish to explicitly model a mean function, including interpretability, convenience of expressing prior informations, ... 
  \item When assuming a non-zero mean GP prior $\gp$ with mean $m(\xv)$, the predictive mean becomes 
  $$
    m(\Xmat_*) + \Kmat_*\Kmat_y^{-1}\left(\bm{y} - m(\Xmat)\right)
  $$
  while the predictive variance remains unchanged. 
  
  \framebreak
  
  \item Gaussian processes with non-zero mean Gaussian process priors are also called Gaussian processes with trend.  
\vspace{.3cm}

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/gp-sample/gp-sample-1-1.pdf}
\end{figure}

\framebreak


\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/gp-sample/gp-sample-2-1.pdf}
\end{figure}

\framebreak
 


\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/gp-sample/gp-sample-2-2.pdf}
\end{figure}

 \framebreak
 
 
\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/gp-sample/gp-sample-2-3.pdf}
\end{figure}

\framebreak

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/gp-sample/gp-sample-2-4.pdf}
\end{figure}

\framebreak


\item In practice it can often be difficult to specify a fixed mean function
\item In many cases it may be more convenient to specify a few fixed basis functions, whose coefficients, $\bm{\beta}$, are to be inferred from the data
\item Consider 
$$
  g(\xv) = b(\xv)^\top \bm{\beta} + \fx, \text{   where } \fx \sim \mathcal{GP} \left(0, k(\xv, \tilde \xv)\right)
$$
\item This formulation expresses that the data is close to a global linear model with the residuals being modelled by a GP. 
\item For the estimation of $g(\xv)$ please refer to \emph{Rasmussen, Gaussian Processes for Machine Learning, 2006}

\end{itemize}


\end{vbframe}


\endlecture
\end{document}
