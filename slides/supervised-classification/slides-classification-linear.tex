\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Linear Classifiers
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/linear_boundary.png
}{% Learning goals, wrapped inside itemize environment
  \item Linear classifier
  \item Linear decision boundaries
  \item Linear separability
}

\framebreak


\begin{vbframe}{Linear Classifiers}

Important subclass of classification models. 

\lz

Definition: 
If discriminant(s) $\fkx$ can be written as affine linear function(s)
(possibly through a rank-preserving, monotone transformation $g$):

$$
  g(\fkx) = \bm{w}_k^\top \xv + b_k,
$$

we will call the classifier \textbf{linear}. 

\vfill

\begin{itemize}
\item $\bm{w}_k$ and $b_k$ do not necessarily refer to parameters $\thetav_k$, although they often coincide; discriminant simply must be writable in an affine-linear way 
\item reasons for the transformation is that we only care about the position of the decision boundary
\end{itemize}


\end{vbframe}

  
\begin{vbframe}{linear decision boundaries}
  
We can also easily show that the decision boundary between classes $i$ and $j$ is a hyperplane. For every $\xv$ where there is a tie in scores: 

\begin{eqnarray*}
  \fkx[i] &=& \fkx[j] \\
  g(\fkx[i]) &=& g(\fkx[j]) \\
  \bm{w}_i^\top \xv + b_i &=& \bm{w}_j^\top \xv + b_j \\
  \left(\bm{w}_i - \bm{w}_j\right)^\top \xv + \left(b_i - b_j\right) &=& 0 
\end{eqnarray*}

This represents a \textbf{hyperplane} separating two classes:

\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit?usp=sharing
\includegraphics[width=0.32\textwidth]{figure_man/linear_boundary.png} 
\end{center}
\end{vbframe}

\begin{vbframe}{Example: 2 Classes with Centroids}

\begin{itemize}
\item Model binary problem with centroid $\muk$ per class as "parameters"

\item  Don't really care how the centroids are estimated; \\
could use class means, but the following doesn't depend on it

\item Classify point $\xv$ by assigning it to class $k$ of nearest centroid

\end{itemize}

\lz

\begin{center}
\includegraphics[width=0.75\textwidth]{figure/nearest_centroid_classifier.png} 
\end{center}

\end{vbframe}

\begin{vbframe}{Example: 2 Classes with Centroids}

Let's calculate the decision boundary:
$$
d_1 = ||\xv - \muk[1]||^2 = \xv^\top \xv - 2 \xv^\top \muk[1] + \muk[1]^\top \muk[1]
= \xv^\top \xv - 2 \xv^\top \muk[2] + \muk[2]^\top \muk[2] = ||\xv - \muk[2]||^2 = d_2
$$

Where $d$ is measured using Euclidean distance. This implies:
$$
-2 \xv^\top \muk[1] + \muk[1]^\top \muk[1]
= -2 \xv^\top \muk[2] + \muk[2]^\top \muk[2]
$$

Which simplifies to:
$$
2 \xv^\top (\muk[2] - \muk[1]) =\muk[2]^\top \muk[2] - \muk[1]^\top \muk[1]
$$

Thus, it's a linear classifier!

\end{vbframe}


\begin{vbframe}{linear separability}
If there exists a linear classifier that perfectly separates the classes of some dataset, the data are called \textbf{linearly separable}.

\vspace{1cm}

\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit?usp=sharing
\includegraphics{figure_man/linear_separability-1.png} 
\end{center}

\end{vbframe}


\begin{vbframe}{feature transformations}
Note that linear classifiers can represent \textbf{non-linear} decision boundaries in the original input space if we use derived features like higher order interactions, polynomial features, etc.

\lz

\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit?usp=sharing
\includegraphics{figure_man/linear_separability-2.png} 
\end{center}

\lz

Here we used absolute values to find suitable derived features.

\end{vbframe}

\endlecture

\end{document}
