\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Discriminant Analysis
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/disc_analysis-qda_1
}{% Learning goals, wrapped inside itemize environment
  \item LDA and QDA construction principle based on
    generative approach
  \item How are their parameters estimated
  \item Linear and quadratic decision boundaries
}

\begin{vbframe}{Linear discriminant analysis}

\begin{small}
Generative approach, following Bayes' theorem:

$$\pikx \approx \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j}$$

Assume that distribution $\pdfxyk$ per class is  \textbf{multivariate Gaussian}:

$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma^{-1} (\xv - \muk)\right)
$$

with \textbf{equal covariance structure}, so $\Sigma_k = \Sigma \quad \forall k$
\end{small}

\vspace{-0.9em}
\begin{center}
\includegraphics[width=0.60\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-lda_2.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Univariate example}
\begin{small}
\begin{itemize}
\item Classify a new person as male or female based on their height\\
  (naive toy example, unrealistic in many ways)
\item We will compute in the true DGP, so we assume we know all distributions and their params; we use the LDA setup
\end{itemize}
\begin{center}
\includegraphics[width=0.95\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-1.png}
\end{center}
\centerline{Optimal separation is located at the intersection (= decision boundary)!}
\end{small}
\end{vbframe}

\begin{vbframe}{Univariate example: equal class sizes}
\begin{small}
Let's compute posterior probability that a 172 cm tall person is male 
\begin{center}
\includegraphics[width=0.85\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-2.png}
\end{center}
Assuming equal class sizes, prior probs $\pik$ cancel out (since $\pi_{man} = \pi_{woman}$):
$$
\P(y = \text{man} \mid \xv) = \frac{p(\xv \mid y = \text{man})}{p(\xv \mid y = \text{man}) + p(\xv \mid y = \text{woman})} = \frac{0.0135}{0.0135 + 0.088} = 0.133
$$
\end{small}
\end{vbframe}

\begin{vbframe}{Univariate example: unequal class sizes}
\begin{small}
For unequal class sizes (e.g., $\pi_{woman} = 2\pi_{man}$), the prior probs matter and cause a shift of the decision boundary towards the smaller class
\begin{center}
\includegraphics[width=0.86\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-3.png}
\end{center}
\begin{align*}
\P(y = \text{man} \mid \xv) &= \frac{p(\xv \mid y = \text{man}) \pi_{man}}{p(\xv \mid y = \text{man}) \pi_{man} + p(\xv \mid y = \text{woman}) \pi_{woman}}\\
&=\frac{0.0135 \cdot \tfrac{1}{3}}{0.0135 \cdot \tfrac{1}{3} + 0.088 \cdot \tfrac{2}{3}} = 0.0712
\end{align*}

\end{small}

\end{vbframe}

\begin{vbframe}{LDA as linear classifier}
Because of the equal covariance structure of all class-specific Gaussians, the decision boundaries of LDA are always linear

\begin{center}
\includegraphics[width=\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_db-lda.png}
\end{center}

\end{vbframe}

\begin{vbframe}{LDA as linear classifier}

Can easily prove this by showing that posteriors can be written as affine-linear functions - up to rank-preserving transformation:

$$
  \pikx = \frac{\pik \cdot \pdfxyk }{\pdfx} = \frac{\pik \cdot \pdfxyk}{\sumjg \pi_j \cdot \pdfxyk[j]}
$$

As the denominator is the same for all classes we only need to consider 
$$\pik \cdot \pdfxyk$$ 
and show that this can be written as a linear function of $\xv$.

\end{vbframe}

\begin{vbframe}{LDA as linear classifier}
\begin{footnotesize}
\begin{eqnarray*}
& \pik \cdot \pdfxyk \\
\propto& \textcolor{orange}{\pik} \exp\left(\textcolor{blue}{- \frac{1}{2} \xv^T\Sigma^{-1}\xv} \textcolor{orange}{- \frac{1}{2} \muk^T \Sigma^{-1} \muk} + \xv^T \textcolor{purple}{\Sigma^{-1} \muk} \right) \\
=& \exp\left(\textcolor{orange}{\log \pik - \frac{1}{2} \muk^T \Sigma^{-1} \muk} + \xv^T \textcolor{purple}{\Sigma^{-1} \muk} \right) \exp\left(\textcolor{blue}{- \frac{1}{2} \xv^T\Sigma^{-1}\xv}\right) \\
=& \exp\left(\textcolor{orange}{w_{0k}} + \xv^T \textcolor{purple}{\bm{w}_k}\right) \exp\left(\textcolor{blue}{- \frac{1}{2} \xv^T\Sigma^{-1}\xv}\right)\\
\propto& \exp\left(w_{0k} + \xv^T \bm{w}_k\right) 
\end{eqnarray*}

by defining
$w_{0k} := \log \pik  - \frac{1}{2} \muk^T \Sigma^{-1} \muk$ $\quad$ and $\bm{w}_k := \Sigma^{-1} \muk$.

\lz

By finally taking the log, we can write our transformed scores as linear:  
$$ \fkx =  w_{0k} + \xv^T \bm{w}_k $$


\begin{itemize}
\item The above is a little bit \enquote{lax} so lets carefully check
\item We left out several (pos) multiplicative constants
\item $\exp\left(- \frac{1}{2} \xv^T\Sigma^{-1}\xv\right)$ contains $\xv$ but is the same for all classes
\item $\log(a t + b $ is still isotonic for $a > 0$
\end{itemize}
\end{footnotesize}



\end{vbframe}


\begin{vbframe}{Quadratic discriminant analysis}

Doesn't assume equal covariances $\Sigma_k$ per class, so generalizes LDA:

$$
\pdfxyk = \frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma_k|^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\xv - \muk)^T \Sigma_k^{-1} (\xv - \muk)\right)
$$

$\Rightarrow$ Better data fit but \textbf{requires estimation of more parameters} ($\Sigma_k$)!

\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.9\textwidth, clip=true, trim={0 75 0 45}]{figure/disc_analysis-qda_2.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Univariate Example with QDA}
\begin{small}
Different covariance matrices lead to multiple classification rules:
\begin{itemize}
  \item $x < 159.6$ is being assigned to class \textit{man}.
  \item $159.6 < x < 175.5$ is being assigned to class \textit{woman}.
  \item $x > 175.5$ is being assigned to class \textit{man}.
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_univariate-4.png}
\end{center}
$\Rightarrow$ The separation function is quadratic, we learn a curved decision boundary\\
(in 1D a little bit weird, as we learn an interval)
\end{small}
\end{vbframe}

\begin{vbframe}{QDA decision boundaries}

\vspace{-2em}
\begin{small}
\begin{eqnarray*}
\pikx &\propto& \pik \cdot \pdfxyk \\
&\propto& \pik |\Sigma_k|^{-\frac{1}{2}}\exp(- \frac{1}{2} \xv^T\Sigma_k^{-1}\xv - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk )
\end{eqnarray*}

Taking log, we get a quadratic discriminant function in $x$:

$$ \log \pik - \frac{1}{2} \log |\Sigma_k| - \frac{1}{2} \muk^T \Sigma_k^{-1} \muk + \xv^T \Sigma_k^{-1} \muk - \frac{1}{2} \xv^T\Sigma_k^{-1}\xv $$


Allowing for curved decision boundaries:

\vspace{-0.5em}
\begin{center}
\includegraphics[width=0.68\textwidth, clip=true, trim={0 0 0 20}]{figure/disc_db-qda.png}
\end{center}
\end{small}

\end{vbframe}

\begin{vbframe}{Parameter estimation}

Parameters $\thetav$ are estimated in a straightforward manner by:\\
\begin{equation*}
\begin{aligned}
\pikh &= \frac{n_k}{n},\text{ where $n_k$ is the number of class-$k$ observations} \\
\mukh &= \frac{1}{n_k}\sum_{i:\yi = k} \xi \\
\Sigmah_k &= \frac{1}{n_k - 1} \sum_{i: \yi = k} (\xi - \mukh) (\xi - \mukh)^T \quad \quad \text{   (QDA)} \\
\Sigmah &= \frac{1}{n - g} \sumkg \sum_{i: \yi = k} (\xi - \mukh) (\xi - \mukh)^T \quad \text{(LDA)} \\
\end{aligned}
\end{equation*}

\lz

As $\Sigmah_k, \Sigmah$ are $p \times p$ matrices (for $p$ features), estimating all $\Sigmah_k$ involves $\frac{p(p+1)}{2} \cdot g$ parameters across $g$ classes (vs. just $\frac{p(p+1)}{2}$ for LDA's $\Sigmah$)\\
(in addition to estimating priors and class means)
\end{vbframe}

\begin{vbframe}{QDA parameter estimation example}
\begin{small}
E.g., for a simple two-class, 2-dimensional dataset:\\

Class 1: $\xv_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}, \xv_2 = \begin{pmatrix} 2 \\ 3 \end{pmatrix} $, 
Class 2: $\xv_3 = \begin{pmatrix} 6 \\ 8 \end{pmatrix}, \xv_4 = \begin{pmatrix} 7 \\ 9 \end{pmatrix}, \xv_5 = \begin{pmatrix} 8 \\ 10 \end{pmatrix}$

\lz

Class priors: $\pikh[1] = \frac{n_1}{n} = \frac{2}{5} = 0.4, \quad \pikh[2] = \frac{n_2}{n} = \frac{3}{5} = 0.6$

Class means: $\mukh[1] = \frac{1}{2} \left( \xv_1 + \xv_2 \right) = \begin{pmatrix} 1.5 \\ 2.5 \end{pmatrix}, \quad \mukh[2] = \frac{1}{3} \left( \xv_3 + \xv_4 + \xv_5 \right) = \begin{pmatrix} 7 \\ 9 \end{pmatrix}$

Class covariances: $(\xv_1 - \mukh[1])(\xv_1 - \mukh[1])^\top = \begin{pmatrix} 0.25 & 0.25 \\ 0.25 & 0.25 \end{pmatrix} = (\xv_2 - \mukh[1])(\xv_2 - \mukh[1])^\top$\\ 
$\Rightarrow \Sigmah_1 = \frac{1}{1} \left(\begin{pmatrix} 0.25 & 0.25 \\ 0.25 & 0.25 \end{pmatrix} + \begin{pmatrix} 0.25 & 0.25 \\ 0.25 & 0.25 \end{pmatrix} \right) = \begin{pmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{pmatrix}$

$(\xv_3 - \mukh[2])(\xv_3 - \mukh[2])^\top = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} = (\xv_5 - \mukh[2])(\xv_5 - \mukh[2])^\top$, $(\xv_4 - \mukh[2])(\xv_4 - \mukh[2])^\top = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}$\\
$\Rightarrow \Sigmah_2 = \frac{1}{2} \left( \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} + \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix} + \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \right) = \begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix}$
\end{small}
\end{vbframe}


\begin{vbframe}{Discriminant analysis comparison}
\begin{small}
\begin{itemize}
\item We benchmark on simple toy data set(s)
\item Normally distributed data per class, but unequal cov matrices
\item And then increase dimensionality
\item We might assume that QDA always wins here ...
\end{itemize}
\end{small}

\begin{center}
\includegraphics[width=0.8\textwidth, clip=true, trim={0 0 0 0}]{figure/disc_cod.png}
\end{center}

$\Rightarrow$ LDA might be preferable over QDA in higher dimensions!

\end{vbframe}
\endlecture

\end{document}
