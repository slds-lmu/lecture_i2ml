\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Basic Definitions
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/db_examples
}{% Learning goals, wrapped inside itemize environment
  \item Basic notation
  \item Hard labels vs. probabilities vs. scores
  \item Decision regions and boundaries
  \item Generative vs. discriminant approaches
}


\begin{vbframe}{Notation and Target Encoding}

\begin{itemize}
\item In classification, we aim at predicting a discrete output 
$$
y \in \Yspace = \{C_1, ..., C_g\}
$$
with $2 \le g < \infty$, given data $\D$ 

\item For convenience, we often encode these classes differently

\item \textbf{Binary case, $g = 2$}: Usually use $\Yspace = \setzo$ or $\Yspace = \setmp$

\item \textbf{Multiclass case, $g \ge 3$}: Could use $\Yspace = \gset$, but often use \textbf{one-hot encoding} $o(y)$, i.e., $g$-length vector with $o_k(y) = \I(y = k) \in \{0,1\}$:

\end{itemize}

\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics[width=0.65\linewidth]{figure_man/one-hot_encoding.png} 
\end{center}

\end{vbframe}


\begin{vbframe}{Classification Models} 

\begin{itemize}

\item While for regression the model $f: \Xspace \to \R$ simply maps to the label space $\Yspace=\R$, classification is slightly more complicated.
\item We sometimes like our models to output (hard) classes, 
sometimes probabilities, sometimes class scores. The latter 2 are vectors. 
\item The most basic / common form is the score-based classifier, this is why we defined models already as $f: \Xspace \to \R^g$.
\item To minimize confusion, we distinguish between all 3 in notation: $\hx$ for hard labels, $\pix$ for probabilities and $\fx$ for scores

\item Why all of that and not only hard labels? a) Scores / probabilities are more informative than hard class predictions; b) from an optimization perspective, it is much (!) easier to work with continuous values.

\end{itemize}

\end{vbframe}


\begin{vbframe}{Scoring Classifiers}
\begin{itemize}
% \item Scoring classifiers assume the output variable to be -1/+1-encoded, i. e. $\Yspace = \{-1, 1\}$
\item Construct $g$ \textbf{discriminant} / \textbf{scoring functions} $f_1, ..., f_g: \Xspace \to \R$
\item Predicted class is usually the one with max score 
$$
h(\xv) = \argmax_{k \in \gset} \fkx[k]
$$ 

\item For $g = 2$, a single discriminant function $\fx = f_{1}(\xv) - f_{-1}(\xv)$ is sufficient (here, it's natural to label classes with $\setmp$ and we used slight abuse of notation for the subscripts), \\
class labels are constructed by $\hx = \text{sgn}(\fx)$
\item $|\fx|$ or $|\fkx|$ is loosely called \enquote{confidence}
\end{itemize}

\vspace{-0.3cm}

\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics{figure_man/scores.png} 
\end{center}
 
\end{vbframe}

\begin{vbframe}{Probabilistic Classifiers}
\begin{itemize}
% \item Probabilistic classifiers assume the output variable to be 0/1-encoded, i. e. $\Yspace = \{0, 1\}$
\item Construct $g$ \textbf{probability functions} $\pi_1, ..., \pi_g: \Xspace \to [0, 1],~\sumkg \pikx = 1$ 
\item Predicted class is usually the one with max probability
$$
\hx = \argmax_{k \in \gset} \pikx
$$ 
\item For $g = 2$, single $\pix$ is constructed, which models the predicted probability for the positive class (natural to encode $\Yspace = \setzo$)
\end{itemize}

\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics{figure_man/probabilities.png} 
\end{center}
\end{vbframe}


\begin{frame}{Thresholding}
  
\begin{itemize}
\item For imbalanced cases or class with costs, we might want to deviate from the standard conversion of scores to classes 
\item Introduce basic concept (for binary case) and add details later
\item Convert scores or probabilities to class outputs by thresholding: \\[0.5ex]
$\hx:= [\pix \ge c]$ or $\hx := [\fx \ge c]$ for some threshold $c$
\item Standard thresholds: $c = 0.5$ for probabilities, $c = 0$ for scores
%\item There are also versions of thresholding for the multiclass case

\end{itemize}

\only<1>{
  \begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics[clip=true, trim={0 390 310 350}]{figure_man/threshold_1.png}
\end{center}
}

\only<2>{
  \begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics[clip=true, trim={0 390 310 350}]{figure_man/threshold_2.png}
  \end{center}
  }

\end{frame} 

\begin{vbframe}{Decision regions}

Set of points $\xv$ where class $k$ is predicted:
$$
\Xspace_k = \{\xv \in \Xspace : \hx = k\}
$$

\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics{figure_man/decision_regions.png} 
\end{center}
\end{vbframe} 

\begin{vbframe}{Decision Boundaries}
Points in space where classes with maximal score are tied and the corresponding hypersurfaces are called \textbf{decision boundaries}

%Formally:
\begin{eqnarray*}
\{ \xv \in \Xspace: \exists~i \ne j \text{ s.t. } \fkx[i] = \fkx[j] \land \fkx[i], \fkx[j] \ge \fkx[k] ~ \forall k \ne i, j\}
\end{eqnarray*}  
  
In binary case we can simply use the threshold:
$$
    \{ \xv \in \Xspace : \fx = c \}
$$

$c=0$ for scores and $c=0.5$ for probs is consistent with the above.

\begin{center}
  % SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
  \includegraphics[width=0.7\textwidth]{figure_man/decision_boundaries.png} 
\end{center}

\end{vbframe}


\begin{vbframe}{Decision boundary examples}

{\centering 
\includegraphics[width=0.80\textwidth]{figure/db_examples.png} 
}

\end{vbframe}

%\begin{vbframe}{Classification Approaches}

  %Two fundamental approaches exist to construct classifiers:\\
  %The \textbf{generative approach} and the \textbf{discriminant approach}.

%\lz
%They tackle the classification problem from different angles:

%\begin{itemize}
%\item \textbf{Generative} classification approaches assume a data-generating process in which the distribution of the features $\xv$ is different for the various classes of the output $y$, and try to learn these conditional distributions:\\ \enquote{Which $y$ tends to have $\xv$ like these?}
%\lz
%\item \textbf{Discriminant} approaches use \textbf{empirical risk minimization} based on a suitable loss function:\\ \enquote{What is the best prediction for $y$ given these $\xv$?}
%\end{itemize}
%\end{vbframe}

\begin{vbframe}{Generative approach}

Models class-conditional $\pdfxyk$, and employs Bayes' theorem:
$$\pikx \approx \postk = \frac{\P(\xv | y = k) \P(y = k)}{\P(\xv)} = \frac{\pdfxyk \pik}{\sumjg \pdfxyk[j] \pi_j}$$

Prior probs $\pi_k = \P(y = k)$ can easily be estimated from training data as relative frequencies of each class:

%\vspace{-1.2cm}

\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
\includegraphics[width=0.8\textwidth]{figure_man/prior_probabilities.png} 
\end{center}

\end{vbframe}

\begin{vbframe}{Generative approach}
Decision boundary implicitly defined via the conditional distributions

\begin{center}
\includegraphics[width=0.5\textwidth]{figure/approach_generative.png} 
\end{center}


Examples are Naive Bayes, LDA and QDA. \\
NB: LDA and QDA have 'discriminant' in their name, but are generative!
\end{vbframe}

\begin{vbframe}{Discriminant approach}

Here we optimize the discriminant functions (or better: their parameters) directly, usually via ERM:
$$ \fh = \argmin_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi$$

\begin{center}
% SEE ALSO rsrc/fig-approaches.R FOR INDIVIDUAL PLOTS
% SOURCE: https://docs.google.com/presentation/d/1X2FxetT6fewXhoGLZmgJyEHY_pWbKjQ6AHZiZQHvwzA/edit#slide=id.p
\includegraphics[width=1.1\textwidth]{figure_man/disc_approach.png} 
\end{center}

Examples are neural networks, logistic regression and SVMs

\end{vbframe}

\endlecture


\end{document}
