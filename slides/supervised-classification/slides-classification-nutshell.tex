\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Supervised Classification
  }{% Lecture title  
  In a Nutshell
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/nutshell_classif_logistic_function.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand basic concept of classifiers
  \item Know concepts of probabilistic and scoring classifier
  \item Know distinction between discriminant and generative approach 
  \item Understand ideas of logistic regression and Naive Bayes
}

\sloppy

\begin{vbframe}{Classification Tasks}
\begin{itemize}
\item \small Learn function that assigns categorical class labels to observations
\item \small Each observation belongs to exactly one class
\item \small The task can contain two (binary) or multiple (multi-class) classes
\end{itemize}

%\end{center} \hspace{0.1cm}

\begin{columns}  
\begin{column}{0.1\textwidth} 
\begin{center}
Training
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1bTwO7ARiUTmzx29aELSs8F3pY2GBhPfY/edit#slide=id.p1
  \includegraphics[width = 0.75\textwidth]{figure_man/nutshell-classification-training-task.png}
\end{center}
\end{column}
\end{columns}
%\end{center}
%\begin{center}
\begin{columns}
\begin{column}{0.1\textwidth} 
\begin{center}
Prediction
\end{center}
\end{column}
\begin{column}{0.9\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1J3ArdDpbRYAJHxL17LNLQAXNRx2rONMf/edit#slide=id.p1
  \includegraphics[width = 0.75\textwidth]{figure_man/nutshell-classification-prediction-task.png} 
\end{center}
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Basic Definitions}
\begin{itemize}
\item \small For every observation a model outputs the probability (probabilistic classifier) or score (scoring classifier) of each class
\item \small In the multi-class case, the class label is usually assigned by choosing the class with the maximum score or probability
\item \small In the binary case, a class label is assigned by choosing the class whose probability or score exceeds a threshold value c
\end{itemize}

\vspace{5mm}

\begin{center}
%https://docs.google.com/presentation/d/1zGtHxzMxFuaKChvZBtjxuyhxsR0xjS95/edit#slide=id.p1
  \includegraphics[width = \textwidth]{figure_man/nutshell-classification-label-assignment.png}
\end{center}

Two fundamental approaches exist to construct a classifier:
\begin{itemize}
\item \small \textbf{Discriminant approach} asks ``What is the best prediction for the class given these data?'' (uses loss functions and empirical risk minimization)
\item \small \textbf{Generative approach} asks ``Which class tends to have data like these?'' (models the feature distributions in each class separately)

\end{itemize}

\begin{center}
%https://docs.google.com/presentation/d/1XSmFrEZrCzqCUx3CcVeR5dmURlDZ_mHU/edit#slide=id.p1
  \includegraphics[width = 0.8\textwidth]{figure_man/nutshell_classif_binary_task.png}
\end{center}
\end{vbframe}


%\begin{vbframe}{Linear Classifiers}
%\end{vbframe}

\begin{vbframe}{Logistic Regression}

\begin{itemize}
\item \small Logistic regression is a \textbf{discriminant approach} for binary classification. It turns scores into probabilities with the logistic function.
\item \small We just need to compute the probability for \textbf{one} class (usually class 1).
\item \small If the probability exceeds a threshold value \textbf{c} $\Rightarrow$ class 1 is predicted.
\end{itemize}

\begin{center}
%https://docs.google.com/presentation/d/1Dw_vkWk9oHIZ4jIaOK29Jt5_yPMUgVyY/edit#slide=id.p1
  \includegraphics[width = 1\textwidth]{figure_man/nutshell-classif-logistic-regression.png}
\end{center}
\begin{columns}
\begin{column}{0.35\textwidth} 
\begin{center}
%https://docs.google.com/presentation/d/1jm185-HDse3LYFawYQ_AZOWI9_g8OjKg/edit#slide=id.p1
\includegraphics[width=\textwidth]{figure_man/nutshell-classification-text-box-logisticreg.png}
\end{center}
\end{column}
\begin{column}{0.6\textwidth} 
\begin{center}

  \includegraphics[width=1\textwidth]{figure/nutshell_classif_logistic_function.png}
\end{center}
\end{column}
\end{columns}



\end{vbframe}



%\begin{vbframe}{Discriminant Analysis}
%\end{vbframe}

\begin{vbframe}{Naive Bayes}
\begin{itemize}
\item \small Naive Bayes is a \textbf{generative multi-class approach}. It computes the class probability for each class based on the training data.
\item \small It considers the data distribution on three different levels:
    \begin{itemize}
    \item \small Marginal distributions $\P(X)$ of each feature (in the entire data set)
    \item \small Marginal distribution $\P(Y)$ of classes (in the entire data set)
    \item \small Conditional distributions $\P(X|Y)$ of each feature in each class
    \end{itemize}
\vspace*{1cm}
\end{itemize}
\begin{center}
%https://docs.google.com/presentation/d/1-cARMPe_-y3uviYN1wQ3Qhs-17PgS4R7/edit#slide=id.p1
  \includegraphics[width=1\textwidth]{figure_man/nutshell-classif-distributions_learning.png}
\end{center}

\newpage
\begin{itemize}
\item \small Example: Class probability of ``not too happy'' given health = ``fair'':
\end{itemize}
\begin{center}
%https://docs.google.com/presentation/d/1yi8WzAxaepsvL7YcBJKA_8Q0xSaPr31l/edit#slide=id.p1
  \includegraphics[width=1\textwidth]{figure_man/nutshell-classif-distributions-prediction.png}
\end{center}
\begin{center}
%https://docs.google.com/presentation/d/1D1ulTdSHMNMsKxrvXVrvkmstuOd845Cd/edit#slide=id.p1
  \includegraphics[width=0.7\textwidth]{figure_man/nutshell-classif-naive-bayes-formula.png}
\end{center}

\end{vbframe}

\endlecture
\end{document}
