\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Logistic Regression
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/log_reg-scores
}{% Learning goals, wrapped inside itemize environment
  \item Hypothesis space of LR
  \item Log-Loss derivation 
  \item Intuition for loss
  \item LR as linear classifier
}

\framebreak


\begin{vbframe}{Motivation}

\begin{itemize}

\item Let's build a \textbf{discriminant} approach, for binary classification, as a probabilistic classifier $\pixt$

\item We encode $y \in \setzo$ and use ERM:
  
$$ \argmin_{\thetav \in \Theta} \risket = \argmin_{\thetav \in \Theta} \sumin \Lpixyit$$

\item We want to \enquote{copy} over ideas from linear regression

\item In the above, our model structure should be \enquote{mainly} linear and we need a loss function

\end{itemize}


\end{vbframe}

\begin{vbframe}{Direct linear model for probabilities}

We could directly use an LM to model $\pixt = \thx$.\\
And use L2 loss in ERM.

\lz 

{\centering \includegraphics[width=0.95\textwidth]{figure/preds_with_probs-linear.png}
}

But: This obviously will result in predicted probabilities $\pixt \not\in [0,1]$!

\end{vbframe}

\begin{vbframe}{Hypothesis space of LR}

To avoid this, logistic regression \enquote{squashes} the estimated linear scores $\thx$ to $[0,1]$ through the \textbf{logistic function} $s$:
\[
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right) = s(\fx)
\]

\begin{center}
  \includegraphics[width=0.74\textwidth]{figure/logistic_function.png} 
\end{center}

$\Rightarrow$ \textbf{Hypothesis space} of LR:
\begin{eqnarray*}
  \Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pixt = s(\thx) ~|~ \thetav \in \R^{p+1} \right\}
\end{eqnarray*}

\end{vbframe}

\begin{vbframe}{Logistic Function}

Intercept $\theta_0$ shifts $\pi = s(\theta_0 + f) = \frac{\exp(\theta_0 + f)}{1+\exp(\theta_0 + f)}$ horizontally

{
\centering \includegraphics[width=0.8\textwidth]{figure/logistic_shifted.png}
}

Scaling $f$ like $s(\alpha f) = \frac{\exp(\alpha f)}{1+\exp(\alpha f)}$ controls slope and direction

{
\centering \includegraphics[width=0.8\textwidth]{figure/logistic_scaled.png}
}

\end{vbframe}

\begin{vbframe}{The Logit}

The inverse $s^{-1}(\pi) = \log\left(\frac{\pi}{1 - \pi}\right)$ where $\pi$ is a probability is called \textbf{logit} (also called \textbf{log odds} since it is equal to the logarithm of the odds $\frac{\pi}{1-\pi}$)

\begin{center}
\includegraphics[width=0.6\textwidth]{figure/logit_function.png}
\end{center}

\begin{itemize}

\item Positive logits indicate probabilities > 0.5 and vice versa

\item E.g.: if $p = 0.75$, odds are $3:1$ and logit is $log(3) \approx 1.1$

\item Features $\xv$ act linearly on logits, controlled by coefficients $\thetav$:

$$
s^{-1}(\pix) = \log\left(\frac{\pix}{1 - \pix}\right) = \thetav^T x
$$

\end{itemize}


\end{vbframe}

\begin{vbframe}{Deriving Log-Loss}

We need to find a suitable loss function for \textbf{ERM}. We look at likelihood which multiplies up $\pixit$ for positive examples, and $1-\pixit$ for negative. 


$$
\LLt = \prod_{i \text{ with } \yi = 1} \pixit \prod_{i \text{ with } \yi = 0} (1-\pixit) 
$$

We can now cleverly combine the 2 cases by using exponents \\
(note that only one of the 2 factors is not 1 and \enquote{active}):

$$
\LLt = \prodin \pixit^{\yi} \left(1-\pixit\right)^{1-\yi}
$$

\framebreak

Taking the log to convert products into sums:
\begin{align*}
\loglt &= \log \LLt = 
\sumin \log\left(\pixit^{\yi} \left(1-\pixit\right)^{1-\yi}\right) \\
       &= \sumin \yi \log\left(\pixit\right) + \left(1-\yi\right)\log\left(1-\pixit\right)
\end{align*}

Since we want to minimize the risk, we work with the negative $\loglt$:
$$
- \loglt = \sumin - \yi \log\left(\pixit\right) - \left(1-\yi\right)\log\left(1-\pixit\right)
$$


\end{vbframe}

\begin{vbframe}{Bernoulli / Log Loss}

The resulting loss 
$$\Lypi = -y\log(\pi)-(1-y)\log(1-\pi)$$ 
is called \textbf{Bernoulli, binomial, log} or \textbf{cross-entropy} loss

\lz

{\centering \includegraphics[width=0.95\textwidth]{figure/log_loss.png}
}

\lz

\begin{itemize}
  \item Penalizes confidently wrong predictions heavily
  \item Is used for many other classifiers, e.g., in NNs or boosting 


  
\end{itemize}


\end{vbframe}


\begin{vbframe}{Logistic Regression in 2D}

LR is a linear classifier, as $\pixt = s\left( \thx \right)$ 
and $s$ is isotonic.

\lz\lz

\begin{columns}[T]
\begin{column}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/log_reg-db.png}
\end{column}
\begin{column}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/log_reg-scores.png}
\end{column}
\end{columns}

\end{vbframe}


\begin{vbframe}{Optimization}

\begin{itemize}

\item Log-Loss is convex, under regularity conditions LR has a unique solution (because of its linear structure), but not an analytical one

\item To fit LR we use numerical optimization, e.g., Newton-Raphson

\item If data is linearly separable, the optimization problem is unbounded and we would not find a solution; way out is regularization 

\item Why not use least squares on $\pix = s(\fx)$? \\
Answer: ERM problem is not convex anymore :(

\item We can also write the ERM as  
$$
\argmin_{\thetav \in \Theta} \risket = \argmin_{\thetav \in \Theta} \sumin \Lxyit
$$
With $\fxt = \thetav^T \xv$ and 
$\Lyf = -yf + \log(1 + \exp(f)) $\\
\lz
This combines the sigmoid with the loss and shows a convex loss directly on a linear function
\end{itemize}
\end{vbframe}



\endlecture

\end{document}
