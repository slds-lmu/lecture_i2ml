\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Logistic Regression
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/log_reg-scores
}{% Learning goals, wrapped inside itemize environment
  \item Hypothesis space of LR
  \item Log-Loss derivation 
  \item Intuition for loss
  \item LR as linear classifier
}

\begin{vbframe}{Motivation}

\begin{itemize}

\item Let's build a \textbf{discriminant} approach, for binary classification, as a probabilistic classifier $\pixt$

\item We encode $y \in \setzo$ and use ERM:
  
$$ 
\argmin_{\thetav \in \Theta} \risket = \argmin_{\thetav \in \Theta} \sumin \Lpixyit
$$

\item We want to \enquote{copy} over ideas from linear regression

\item In the above, our model structure should be \enquote{mainly} linear and we need a loss function

\end{itemize}


\end{vbframe}

\begin{vbframe}{Direct linear model for probabilities}

We could directly use an LM to model $\pixt = \thx$.\\
And use L2 loss in ERM.

\lz 

\image[0.95]{figure/preds_with_probs-linear.png}

But: This obviously will result in predicted probabilities $\pixt \not\in [0,1]$!

\textbf{Caveat}: In Econometrics, the so-called linear probability model is still used \citelink{COX1989}.

\end{vbframe}

\begin{vbframe}{Hypothesis space of LR}

To avoid this, logistic regression \enquote{squashes} the estimated linear scores $\thx$ to $[0,1]$ through the \textbf{logistic function} $s$:
$$
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right) = s(\fx)
$$

\image[0.74]{figure/logistic_function.png}

$\Rightarrow$ \textbf{Hypothesis space} of LR:
$$
  \Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pixt = s(\thx) ~|~ \thetav \in \R^{p+1} \right\}
$$

\end{vbframe}

\begin{vbframe}{Logistic Function}

Intercept $\theta_0$ shifts $\pi = s(\theta_0 + f) = \frac{\exp(\theta_0 + f)}{1+\exp(\theta_0 + f)}$ horizontally

\image[0.8]{figure/logistic_shifted.png}

Scaling $f$ like $s(\alpha f) = \frac{\exp(\alpha f)}{1+\exp(\alpha f)}$ controls slope and direction

\image[0.8]{figure/logistic_scaled.png}

\end{vbframe}

\begin{vbframe}{The Logit}

The inverse $s^{-1}(\pi) = \log\left(\frac{\pi}{1 - \pi}\right)$ where $\pi$ is a probability is called \textbf{logit} (also called \textbf{log odds} since it is equal to the logarithm of the odds $\frac{\pi}{1-\pi}$)

\image[0.6]{figure/logit_function.png}

\begin{itemize}

\item Positive logits indicate probabilities > 0.5 and vice versa

\item E.g.: if $p = 0.75$, odds are $3:1$ and logit is $log(3) \approx 1.1$

\item Features $\xv$ act linearly on logits, controlled by coefficients $\thetav$:

$$
s^{-1}(\pix) = \log\left(\frac{\pix}{1 - \pix}\right) = \thetav^T x
$$

\end{itemize}


\end{vbframe}

\begin{vbframe}{Deriving Log-Loss}

We need to find a suitable loss function for \textbf{ERM}. We look at likelihood which multiplies up $\pixit$ for positive examples, and $1-\pixit$ for negative. 


$$
\LLt = \prod_{i: \yi = 1} \pixit \prod_{i: \yi = 0} (1 - \pixit) 
$$

We can now cleverly combine the 2 cases by using exponents \\
(note that only one of the 2 factors is not 1 and \enquote{active}):

$$
\LLt = \prodin \pixit^{\yi} \left(1-\pixit\right)^{1-\yi}
$$

\end{vbframe}

\begin{vbframe}{Deriving Log-Loss continued}

Taking the log to convert products into sums:
\begin{align*}
\loglt &= \log \LLt = 
\sumin \log\left(\pixit^{\yi} \left(1-\pixit\right)^{1-\yi}\right) \\
       &= \sumin \yi \log\left(\pixit\right) + \left(1-\yi\right)\log\left(1-\pixit\right)
\end{align*}

Since we want to minimize the risk, we work with the negative $\loglt$:
$$
- \loglt = \sumin - \yi \log\left(\pixit\right) - \left(1-\yi\right)\log\left(1-\pixit\right)
$$


\end{vbframe}

\begin{vbframe}{Bernoulli / Log Loss}

The resulting loss 
$$\Lpixy = -y\log(\pix)-(1-y)\log(1-\pix)$$ 
is called \textbf{Bernoulli, binomial, log} or \textbf{cross-entropy} loss

\lz

\image[0.95]{figure/log_loss.png}

\lz

\begin{itemize}
  \item Penalizes confidently wrong predictions heavily
  \item Is used for many other classifiers, e.g., in NNs or boosting 


  
\end{itemize}


\end{vbframe}


\begin{vbframe}{Logistic Regression in 2D}

LR is a linear classifier, as $\pixt = s\left( \thx \right)$ 
and $s$ is isotonic.

\lz\lz

\splitVTT{
  \image[1.0]{figure/log_reg-db.png}
}{
  \image[1.0]{figure/log_reg-scores.png}
} 

\end{vbframe}


\begin{vbframe}{Optimization}

\begin{itemize}

\item Log-Loss is convex, under regularity conditions LR has a unique solution (because of its linear structure), but not an analytical one

\item To fit LR we use numerical optimization, e.g., Newton-Raphson

\item If data is linearly separable, the optimization problem is unbounded and we would not find a solution; way out is regularization 

\item Why not use least squares on $\pix = s(\fx)$? \\
Answer: ERM problem is not convex anymore :(

\item We can also write the ERM as  
$$
\argmin_{\thetav \in \Theta} \risket = \argmin_{\thetav \in \Theta} \sumin \Lxyit
$$
With $\fxt = \thetav^T \xv$ and 
$\Lxy = -y \fx + \log(1 + \exp(\fx)) $\\
\lz
This combines the sigmoid with the loss and shows a convex loss directly on a linear function
\end{itemize}
\end{vbframe}



\endlecture

\end{document}
