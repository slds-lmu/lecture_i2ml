\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/reg_class_log_7}
\newcommand{\learninggoals}{
\item Understand the definition of the logit model
\item Understand how a reasonable loss function for binary classification can be derived
\item Know the hypothesis space that belongs to the logit model}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-lm.tex}

%! includes: classification-basicdefs, classification-linear

\lecturechapter{Classification: Logistic Regression}
\lecture{Introduction to Machine Learning}
\framebreak


\begin{vbframe}{Motivation}

A \textbf{discriminant} approach for directly modeling the posterior probabilities $\pixt$ of the labels is \textbf{logistic regression}. 

For now, let's focus on the binary case $y \in \setzo$ and use empirical risk minimization.
  
$$ \argmin_{\thetab \in \Theta} \risket = \argmin_{\thetab \in \Theta} \sumin \Lpixyit.$$

\lz
A naive approach would be to model
\[
\pixt = \thx .
\]

NB: We will often suppress the intercept in notation.

\lz

Obviously this could result in predicted probabilities $\pixt \not\in [0,1]$.

\end{vbframe}

\begin{vbframe}{Logistic Function}

To avoid this, logistic regression \enquote{squashes} the estimated linear scores $\thx$ to $[0,1]$ through the \textbf{logistic function} $s$:
\[
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right)
\]

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_1} 

}



\end{knitrout}

\framebreak
The intercept shifts $s(f)$ horizontally $s(\theta_0 + f) = \frac{\exp(\theta_0 + f)}{1+\exp(\theta_0 + f)}$
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_2}  

}



\end{knitrout}

Scaling $f$ like $s(\alpha f) = \frac{\exp(\alpha f)}{1+\exp(\alpha f)}$ controls the slope and direction.
\lz
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_3} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Bernoulli / Log loss}

We need to define a loss function for the ERM approach:

\begin{itemize}
  \item $\Lpixy = -y\ln(\pix)-(1-y)\ln(1-\pix)$
  \item Penalizes confidently wrong predictions heavily
  \item Called Bernoulli, log or cross-entropy loss 
  \item We can derive it from the negative log-likelihood of Bernoulli / logistic regression model in statistics
  \item Used for many other classifiers, e.g., in NNs or boosting 
\end{itemize}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_4}  

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Logistic Regression in 1D}
With one feature $\xv \in \R$. The figure shows data and $\xv \mapsto \pix$.

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_5} 

}



\end{knitrout}
\end{vbframe}
\begin{vbframe}{Logistic Regression in 2D}

Obviously, logistic regression is a linear classifier, as $\pixt = s\left( \thx \right)$ 
and $s$ is isotonic.

\lz
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_6}  

}



\end{knitrout}

\framebreak

\begin{columns}[T]
\begin{column}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure_man/logreg-2vars-surface.png}
\end{column}
\begin{column}{0.5\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_class_log_7} 

}



\end{knitrout}
\end{column}
\end{columns}

% <<fig.width=2, fig.height=2>>=
% x1 = seq(-1, 8, by = 0.1)
% dfn = expand.grid(x1, x1)
% names(dfn) = c("x1", "x2")
% dfn$prob = predict(m, newdata = dfn)
% dfn$score = predict(mm, newdata = dfn)
% z = matrix(dfn$prob, nrow = length(x1))
% res = persp(x1, x1, z, phi = 30, theta = 30,
%   xlab = "x1", ylab = "x2",
%   main = expression(pi(x)), border = "grey"
% )
% cols = viridisLite::viridis(2, end = .9)
% mypoints = trans3d(df$x1, df$x2, df$prob, pmat = res)
% points(mypoints, pch = 19, col = cols[as.numeric(df$y) + 1])
% @ 


% \framebreak 

% \begin{center}
  % \includegraphics[width=0.4\textwidth]{figure_man/logreg-2vars-surface.png}
% \end{center}


\end{vbframe}


% \begin{vbframe}{Logistic Regression}

% \textbf{Multivariate logistic regression} combines the hypothesis space of linear functions 

% \vspace*{-0.3cm}

% \begin{eqnarray*}
%   \Hspace = \left\{f: \Xspace \to \R ~|~\fx = \thx \right\}
% \end{eqnarray*}

% with the \textbf{logistic loss}

% \vspace*{-0.2cm}

% \begin{eqnarray*}
%   \Lxy = \log \left[1 + \exp \left(-y\fx\right)\right].
% \end{eqnarray*}

% We transform scores into probabilities by 

% $$
% \pix = \P(y = 1 ~|~\xv) = s(\fx) = \frac{1}{1 + \exp(- \fx)},
% $$

% with $s(.)$ being the logistic sigmoid function as introduced in chapter 2.

% \framebreak 

% As already shown before, an equivalent approach that directly outputs probabilities $\pix$ is minimizing the \textbf{Bernoulli loss} 

% \begin{eqnarray*}
% \Lpixy = -y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)
% \end{eqnarray*}

% for $\pix$ in the hypothesis space

% \begin{eqnarray*}
%   \Hspace = \left\{\pi: \Xspace \to [0, 1] ~|~\pix = s(\thx)\right\}
% \end{eqnarray*}

% with $s(.)$ again being the logistic function. 

% \end{vbframe}

% \begin{vbframe}{Logistic Regression: Discriminant functions and decision boundary}

% Logistic regression gives us a \textbf{linear classifier}. 

% In order to minimize the loss (misclassification), we should predict $y=1$ if

% $$
% \pixt = \frac{\exp(\thx)}{1+\exp(\thx)} \ge 0.5,
% $$

% which is equivalent to
% $$
% \thx \ge 0 \implies y = 1.
% $$


% \lz 

% If we apply $g(y) = \log \left(\frac{y}{1 - y}\right)$ (which is a monotone, rank preserving function) on $\pix = \frac{1}{1 + \exp(-\thetab^\top \xv)}$, we get

% \begin{footnotesize}
% \begin{eqnarray*}
%   g(\pix) &=& g\left(\pix\right)\\
%   &=& \log\left(\pix)\right) - \log \left(1 - \pix\right) \\ &=& - \log (1 + \exp(- \fx)) - \log \left(1 - \frac{1}{1 + \exp(-\fx)}\right)\\
%   &=& - \log (1 + \exp(- \fx)) - \log \left(\frac{\exp(-\fx)}{1 + \exp(-\fx)}\right) \\
%   &=& - \log (1 + \exp(- \fx)) - \log \left(\exp(-\fx)\right) + \log\left(1 + \exp(-\fx)\right)\\
%    &=& \fx = \thx.
% \end{eqnarray*}
% \end{footnotesize}

% $\fx = \thx$ is the discriminant function of logistic regression and $\fx = \thx = 0$ represents the decision boundary, which is a \textbf{hyperplane} in the $p$ dimensional space. 

% \lz  

% The discriminant function can be interpreted as \textbf{log-odds ratio}: 

  % $$ \log \dfrac{\pix}{1-\pix} = \log \dfrac{\P(y = 1 ~|~ \xv)} {\P(y = 0 ~|~ \xv)} = \thx
  % = \fx.
  % $$
 

% \framebreak 

% <<logreg-plot, echo=FALSE, eval = FALSE, include = FALSE>>=
% n = 30
% set.seed(1234L)
% tar = factor(c(rep(1L, times = n), rep(2L, times = n)))
% feat1 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
% feat2 = c(rnorm(n, sd = 1.5), rnorm(n, mean = 2, sd = 1.5))
% bin.data = data.frame(feat1, feat2, tar)
% bin.tsk = makeClassifTask(data = bin.data, target = "tar")
% plotLearnerPrediction("classif.logreg", bin.tsk) + 
%   scale_f_d()
% @

% \end{vbframe}


\begin{frame}{Summary}
% \lz

\textbf{Hypothesis Space:} 
\begin{eqnarray*}
  \Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pix = s(\thx) \right\}
\end{eqnarray*}

\lz

\textbf{Risk:} Logistic/Bernoulli loss function.

\begin{eqnarray*}
  \Lpixy = -y\ln(\pix)-(1-y)\ln(1-\pix)
  % \Lxy = \log \left[1 + \exp \left(-y\fx\right)\right].
\end{eqnarray*}

\lz


\textbf{Optimization:} Numerical optimization, typically gradient-based methods.

% We transform scores into probabilities by 

% $$
% \pix = \P(y = 1 ~|~\xv) = s(\fx) = \frac{1}{1 + \exp(- \fx)},
% $$

% with $s(.)$ being the logistic sigmoid function as introduced in chapter 2.

\end{frame}

\endlecture

\end{document}
