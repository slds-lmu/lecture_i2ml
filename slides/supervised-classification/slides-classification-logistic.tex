\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Classification
  }{% Lecture title  
  Logistic Regression
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/log_reg-scores
}{% Learning goals, wrapped inside itemize environment
  \item Understand the definition of the logit model
  \item Understand how a reasonable loss function for binary classification can be derived
  \item Know the hypothesis space that belongs to the logit model
}

\framebreak


\begin{vbframe}{Motivation}

A \textbf{discriminant} approach for directly modeling the posterior probabilities $\pixt$ of the labels is \textbf{logistic regression}. 

\lz

For now, let's focus on the binary case $y \in \setzo$ and use empirical risk minimization.
  
$$ \argmin_{\thetav \in \Theta} \risket = \argmin_{\thetav \in \Theta} \sumin \Lpixyit.$$

\lz
A naive approach would be to model $\pixt$ as a simple linear model $\thx$ 
(NB: We will often suppress the intercept in notation).

\end{vbframe}

\begin{vbframe}{linear models of probabilities}
E.g., fitting a simple linear model $\pixt = \thx$ on some example data with one feature $\xv \in \R$ and L2 loss:

\lz 

{\centering \includegraphics[width=0.95\textwidth]{figure/preds_with_probs-linear.png}
}

However, this could result in predicted probabilities $\pixt \not\in [0,1]$!

\end{vbframe}

\begin{vbframe}{Restricting the hypothesis space}

To avoid this, logistic regression \enquote{squashes} the estimated linear scores $\thx$ to $[0,1]$ through the \textbf{logistic function} $s$:
\[
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right)
\]

\begin{center}
  \includegraphics[width=0.74\textwidth]{figure/logistic_function.png} 
\end{center}

$\Rightarrow$ The \textbf{hypothesis space} of logistic regression is defined as:
\begin{eqnarray*}
  \Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pixt = s(\thx) ~|~ \thetav \in \R^{p+1} \right\}
\end{eqnarray*}

\end{vbframe}

\begin{vbframe}{Logistic Function}

The intercept $\theta_0$ shifts $\pi = s(\theta_0 + f) = \frac{\exp(\theta_0 + f)}{1+\exp(\theta_0 + f)}$ horizontally.

{
\centering \includegraphics[width=0.8\textwidth]{figure/logistic_shifted.png}
}

Scaling $f$ like $s(\alpha f) = \frac{\exp(\alpha f)}{1+\exp(\alpha f)}$ controls the slope and direction.

{
\centering \includegraphics[width=0.8\textwidth]{figure/logistic_scaled.png}
}

\end{vbframe}

\begin{vbframe}{The Logit}

The inverse $s^{-1}(\pi) = \log\left(\frac{\pi}{1 - \pi}\right)$ where $\pi$ is a probability is called \textbf{logit} (also called \textbf{log odds} since it is equal to the logarithm of the odds $\frac{\pi}{1-\pi}$).

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/logit_function.png}
\end{center}

The regression coefficients $\thetav$ represent the effect of $\xv$ on the logits.

\lz

\small{E.g., if $p = 0.75$, the \textit{odds} of success are $3:1$, and the \textit{logit} is $log(3) \approx 1.1$.}

\vspace{1em}

$\Rightarrow$ Positive logits indicate probabilities greater than 0.5 and vice versa.
\end{vbframe}

\begin{vbframe}{Deriving a loss function}

We need to find a suitable loss function to use \textbf{ERM}. Starting from the likelihood function $\LL$ for the binary case:
\begin{small}
\begin{align*}
\LLt &= \prod_{i \text{ with } \yi = 1} \pixit \prod_{i \text{ with } \yi = 0} (1-\pixit) \\
     &= \pixit^{\yi} (1-\pixit)^{1-\yi}
\end{align*}
\end{small}
Taking the log to convert products into sums:
\begin{small}
\begin{align*}
\loglt &= \log \LLt = \log(\pixit^{\yi} (1-\pixit)^{1-\yi}) \\
       &= \yi \log(\pixit) + (1-\yi)\log(1-\pixit)
\end{align*}
\end{small}

Since we want to minimize our risk, we work with the negative $\loglt$:
\begin{small}
\begin{align*}
- \loglt = - \yi \log(\pixit) - (1-\yi)\log(1-\pixit)
\end{align*}
\end{small}

The resulting loss $\Lpixy = -y\log(\pix)-(1-y)\log(1-\pix)$ is called the \textbf{Bernoulli, log} or \textbf{cross-entropy} loss.

\end{vbframe}

\begin{vbframe}{Bernoulli / Log Loss}

Graphically:

{\centering \includegraphics[width=0.95\textwidth]{figure/log_loss.png}

}

The \textbf{log loss}
\begin{itemize}
  \item penalizes confidently wrong predictions heavily
  \item is used for many other classifiers, e.g., in NNs or boosting 
  \item has no analytical solution for \textbf{optimization} (non-linear, non-convex)! Thus, we use \textbf{numerical optimization}, typically gradient-based methods, to fit a logistic regression model.
\end{itemize}


\end{vbframe}

\begin{vbframe}{Logistic Regression in 1D}
Using logistic regression on our example data with one feature $\xv \in \R$, we again see $\xv \mapsto \pix$, with $\pix \in [0,1]$.

\lz

{\centering \includegraphics[width=0.95\textwidth]{figure/preds_with_probs-logistic.png}
}

\end{vbframe}

\begin{vbframe}{Logistic Regression in 2D}

Obviously, logistic regression is a linear classifier, as $\pixt = s\left( \thx \right)$ 
and $s$ is isotonic.

\lz

\begin{columns}[T]
\begin{column}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/log_reg-db.png}
\end{column}
\begin{column}{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/log_reg-scores.png}
\end{column}
\end{columns}

\end{vbframe}

\endlecture

\end{document}
