%<<setup-child, include = FALSE>>=
%library(knitr)
%library(qrmix)
%library(mlr)
%library(quantreg)
%library(reshape2)
%set_parent("../style/preamble.Rnw")
%@

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-eval}
\input{../../latex-math/ml-trees} % For the comparison of Brier and Gini index

\newcommand{\titlefigure}{figure/plot_bernoulli_prob}
\newcommand{\learninggoals}{
  \item Know the Bernoulli loss and related losses (log-loss, logistic loss, Binomial loss)
  \item Derive the risk minimizer
  \item Derive the optimal constant model 
  \item Understand the connection between log-loss and entropy splitting 
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}



\begin{document}

\lecturechapter{Bernoulli Loss}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Bernoulli Loss}

\vspace*{-0.5cm}
\begin{eqnarray*}
  L(y, \fx) &=& \ln(1+\exp(-y \cdot \fx)) \quad \text{for } y \in \setmp\\
  L(y, \fx) &=& - y \cdot \fx + \log(1 + \exp(\fx)) \quad \text{for } y \in \setzo 
\end{eqnarray*}

\begin{itemize}
  % \item Two equivalent formulations: labels $y \in \setmp$ or $y \in \setzo$
  \item Two equivalent formulations for different label encodings
  \item Negative log-likelihood of Bernoulli model, e.g., logistic regression
  \item Convex, differentiable
  \item Pseudo-residuals (0/1 case): $\tilde{r} = y - \frac{1}{1+\exp(-\fx)}$\\   
    Interpretation: $L1$ distance between 0/1-labels and posterior prob!
\end{itemize}

\vspace{0.2cm}
\begin{center}
% \includegraphics[width = 9cm ]{figure_man/bernoulli.png} \\
\includegraphics[width = 8cm ]{figure/plot_bernoulli_plusmin_encoding.png}
\end{center}

\end{vbframe}




\begin{vbframe}{Bernoulli loss on probabilities}

If scores are transformed into probabilities by the logistic function  $\pix = \left(1 + \exp(- \fx)\right)^{-1}$ (or equivalently if $f(x) = \log\left(\frac{\pix}{1 - \pix}\right)$ are the log-odds of $\pix$), we arrive at another equivalent formulation of the loss, where $y$ is again encoded as $\setzo$:

  $$
    L(y, \pix) = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right). 
  $$
  
\begin{center}
% \includegraphics[width = 10cm ]{figure_man/bernoulli-loss.png} \\
\includegraphics[width = 10cm ]{figure/plot_bernoulli_prob.png}
\end{center}

\end{vbframe}



\begin{vbframe}{Bernoulli loss: Risk Minimizer}

The risk minimizer for the Bernoulli loss defined for probabilistic classifiers $\pix$ and on $y \in \{0, 1\}$ is

$$
  \pixbayes = \eta(\xv) = \P(y = 1 ~|~ \xv = \xv). 
$$

\vspace*{0.2cm}

\begin{footnotesize}
\textbf{Proof:} We can write the risk for binary $y$ as follows: 

% Again, we make use of the law of total expectation

% \begin{eqnarray*}
%   \riskf  & = & \Exy\left[\Lxy\right] = \E_x \left[ \E_{y|x} [ L(y, \fx) ] \right] \\
%           & = & \E_x \left[\sum_{k \in \Yspace} L(k, \fx) \P(y = k~|~ \xv = \xv)\right]\,. 
%           % & = & E_x \sum_{k \in \Yspace} L(k, \fx) \pikx,
% \end{eqnarray*}

% In the binary case this becomes 

% \vspace*{-0.3cm}

\vspace*{-0.3cm}

\begin{eqnarray*}
  \riskf &=& \E_x \left[L(1, \pix) \cdot \eta(\xv) + L(0, \pix) \cdot (1 - \eta(\xv)) \right],
\end{eqnarray*}

with $\eta(\xv) = \P(y = 1 ~|~ \xv = \xv)$ (see chapter on the 0-1-loss for more details). 

For a fixed $\xv$ we compute the point-wise optimal value $c$ by setting the derivative to $0$: 

\vspace*{-0.3cm}

\begin{eqnarray*}
  \frac{\partial }{\partial c} \left(- \log c  \cdot \eta(\xv)- \log (1 - c) \cdot (1 - \eta(\xv))\right) &=& 0 \\
  - \frac{\eta(\xv)}{c} + \frac{1 - \eta(\xv)}{1 - c} &=& 0 \\
  % - \frac{\eta(\xv) (1 - c)}{c (1 - c)} + \frac{c(1 - \eta(\xv))}{c(1 - c)} &=& 0 \\
  \frac{- \eta(\xv) + \eta(\xv) c + c - \eta(\xv) c}{c (1 - c)} &=& 0 \\
  c &=& \eta(\xv). 
\end{eqnarray*}

\end{footnotesize}

\framebreak 

The risk minimizer for the Bernoulli loss defined on $y \in \{-1, 1\}$ and scores $\fx$ is the point-wise log-odds:

\begin{eqnarray*}
\fxbayes &=&  \ln \biggl(\frac{\P(y~|~\xv = \xv)}{1-\P(y~|~\xv = \xv)}\biggr).
\end{eqnarray*}

The function is undefined when $P(y~|~\xv = \xv) = 1$ or $P(y~|~\xv = \xv) = 0$, but predicts a smooth curve which grows when $P(y~|~\xv = \xv)$ increases and equals $0$ when $P(y~|~\xv = \xv) = 0.5$.

\lz 

\textbf{Proof: } As before we minimize 

\begin{eqnarray*}
  \riskf &=& \E_x \left[L(1, \fx) \cdot \eta(\xv) + L(-1, \fx) \cdot (1 - \eta(\xv)) \right] \\
  &=&  \ln(1 + \exp(- \fx)) \eta(\xv)+ \ln(1 + \exp(\fx)) (1 - \eta(\xv)). 
\end{eqnarray*}

\framebreak 

For a fixed $\xv$ we compute the point-wise optimal value $c$ by setting the derivative to $0$: 


\begin{footnotesize}
  \begin{eqnarray*}
  \frac{\partial }{\partial c} \ln(1 + \exp(-c)) \eta(\xv)+ \ln(1 + \exp(c)) (1 - \eta(\xv)) &=& 0 \\
  - \frac{\exp(-c)}{1 + \exp(-c)} \eta(\xv) + \frac{\exp(c)}{1 + \exp(c)} (1 - \eta(\xv)) &=& 0 \\ 
   - \frac{\exp(-c)}{1 + \exp(-c)} \eta(\xv) + \frac{1}{1 + \exp(- c)} (1 - \eta(\xv)) &=& 0\\ 
  % &=& -  \frac{\exp(-c)}{1 + \exp(-c)} p + \frac{1}{1 + \exp(-c)} - \frac{1}{1 + \exp(-c)} p \\
  - \eta(\xv) + \frac{1}{1 + \exp(-c)} &=& 0\\
  \eta(\xv) &=& \frac{1}{1 + \exp(-c)} \\
   c &=& \ln\left(\frac{\eta(\xv)}{1 - \eta(\xv)}\right)
  \end{eqnarray*}
\end{footnotesize}

\end{vbframe}



\begin{vbframe}{Bernoulli: Optimal constant Model}

The optimal constant probability model $\pix = \theta$ w.r.t. the Bernoulli loss for labels from $\Yspace = \setzo$ is:

\begin{eqnarray*}
  \thetah = \argmin_{\theta} \risket = \frac{1}{n} \sumin \yi
\end{eqnarray*}

Again, this is the fraction of class-1 observations in the observed data.
We can simply prove this again by setting the derivative of the risk to 0 and solving for $\theta$.

\framebreak

The optimal constant score model $\fx = \theta$ w.r.t. the Bernoulli loss labels from $\Yspace = \setmp$ or $\Yspace = \setzo$ is:

\begin{eqnarray*}
  \thetah = \argmin_{\theta} \risket = \ln \frac{\np}{\nn} = \ln \frac{\np / n}{\nn /n} 
\end{eqnarray*}

where $\nn$ and $\np$ are the numbers of negative and positive observations, respectively.

\lz

This again shows a tight (and unsurprising) connection of this loss to log-odds.

\lz

Proving this is also a (quite simple) exercise.

\end{vbframe}

\begin{vbframe}{Bernoulli-Loss: Naming Convention}

We have seen three loss functions that are closely related. In the literature, there are different names for the losses: 

\begin{eqnarray*}
  L(y, \fx) &=& \ln(1+\exp(-y\fx)) \quad \text{for } y \in \setmp \\
  L(y, \fx) &=& - y \cdot \fx + \log(1 + \exp(\fx)) \quad \text{for } y \in \setzo 
\end{eqnarray*}

are referred to as Bernoulli, Binomial or logistic loss. 

  $$
    L(y, \pix) = - y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right) \quad \text{for } y \in \setzo
  $$

is referred to as cross-entropy or log-loss. 

\lz 

We usually refer to all of them as \textbf{Bernoulli loss}, and rather make clear whether they are defined on labels $y \in \setzo$ or $y \in \setmp$ and on scores $\fx$ or probabilities $\pix$. 

\end{vbframe}



\begin{vbframe}{Bernoulli loss min = Entropy splitting}

When fitting a tree we minimize the risk within each node $\Np$ by risk minimization and predict the optimal constant. Another approach that is common in literature is to minimize the average node impurity $\text{Imp}(\Np)$. 

\vspace*{0.2cm}

\textbf{Claim:} Entropy splitting $\text{Imp}(\Np) = \sum_{k = 1}^g \pikN \log \pikN$ is equivalent to minimize risk measured by the Bernoulli loss. 

\begin{footnotesize}
Note that $\pikN := \frac{1}{n_{\Np}} \sum\limits_{(\xv,y) \in \Np} [y = k]$. 
\end{footnotesize}

\vspace*{0.2cm}

\textbf{Proof: } To prove this we show that the risk related to a subset of observations $\Np \subseteq \D$ fulfills 

\vspace*{- 0.2cm}


$$
  \risk(\Np) = n_\Np \text{Imp}(\Np),
$$
  
  where $I$ is the entropy criterion $\text{Imp}(\Np)$ and $\risk(\Np)$ is calculated w.r.t. the (multiclass) Bernoulli loss  

$$
  L(y, \pikx) = \sum_{k = 1}^g [y = k] \log \left(\pi_k(\xv)\right).
$$

\framebreak 

\begin{eqnarray*}
\risk(\Np) &=& \sum_{\xy \in \Np}  \sum_{k = 1}^g [y = k] \log \pi_k(\xv) \overset{(*)}{=} \sum_{k = 1}^g \sum_{\xy \in \Np} [y = k]\log \pikN \\
&=& \sum_{k = 1}^g \log \pikN \underbrace{\sum_{\xy \in \Np} [y = k]}_{n_{\Np}\cdot \pikN } \\
 &=& n_{\Np} \sum_{k = 1}^g \pikN \log \pikN = n_\Np \text{Imp}(\Np), 
\end{eqnarray*} 

where in $^{(*)}$ the optimal constant per node $\pikN = \frac{1}{n_{\Np}} \sum\limits_{(\xv,y) \in \Np} [y = k]$ was plugged in. 
% \framebreak

% \textbf{Conclusion}: 
% \begin{itemize}
%   \item Stumps/trees with entropy splitting use the same loss function as logistic regression (binary) / softmax regression (multiclass).
%   \item    While logistic regression is based on the hypothesis space of \textbf{linear functions}, stumps/trees use \textbf{step functions} as hypothesis spaces. 

% \end{itemize}  

\end{vbframe}







% \vspace*{-0.2cm}

% \begin{eqnarray*}
%   \Lxy = \log \left[1 + \exp \left(-y\fx\right)\right].
% \end{eqnarray*}

% We transform scores into probabilities by

% $$
% \pix = \P(y = 1 ~|~\xv) = s(\fx) = \frac{1}{1 + \exp(- \fx)},
% $$

% with $s(.)$ being the logistic sigmoid function as introduced in chapter 2.

% \framebreak

% As already shown before, an equivalent approach that directly outputs probabilities $\pix$ is minimizing the \textbf{Bernoulli loss}

% \begin{eqnarray*}
% \Lxy = -y \log \left(\pix\right) - (1 - y) \log \left(1 - \pix\right)
% \end{eqnarray*}

% for $\pix$ in the hypothesis space

% \begin{eqnarray*}
%   \Hspace = \left\{\pi: \Xspace \to [0, 1] ~|~\pix = s(\thetab^\top \xv)\right\}
% \end{eqnarray*}

% with $s(.)$ again being the logistic function.

% \framebreak


% Logistic Regression with one feature $\xv \in \R$. The figure shows how $\xv \mapsto \pix$.

% <<fig.height=4>>=
% set.seed(1234)
% n = 20
% x = runif(n, min = 0, max = 7)
% y = x + rnorm(n) > 3.5
% df = data.frame(x = x, y = y)

% model = glm(y ~ x,family = binomial(link = 'logit'), data = df)
% df$score = predict(model)
% df$prob = predict(model, type = "response")
% x = seq(0, 7, by = 0.01)
% dfn = data.frame(x = x)
% dfn$prob = predict(model, newdata = dfn, type = "response")
% dfn$score = predict(model, newdata = dfn)

% p2 = ggplot() + geom_line(data = dfn, aes(x = x, y = prob))
% p2 = p2 + geom_point(data = df, aes(x = x, y = prob, colour = y), size = 2)
% p2 = p2 + xlab("x") + ylab(expression(pi(x)))
% p2 = p2 + theme(legend.position = "none")
% p2
% @

% \framebreak

% Logistic regression with two features:

\endlecture

\end{document}