
\begin{algorithm}[H]
  \begin{footnotesize}
  \begin{center}
  \caption{Componentwise Gradient Boosting.}
    \begin{algorithmic}[1]
      \State Initialize $f^{[0]}(\xv) = \argmin_{\theta} \sum  \limits_{i=1}^n L(\yi, \theta)$
      \For{$m = 1 \to M$}
        \State For all $i$: $\rmi = -\left[\fp{L(\yi, f(\xi))}{f(\xi)}\right]_{f=\fmd}$
        \For {$j= 1\to J$}
          \State Fit regression base learner $\hat{b}_j \in \mathcal{B}_j$ to the pseudo-residuals $\rmi$:
          \State $\thetamh_j = \argmin_{\pmb\theta_j} \sum  \limits_{i=1}^n 
          (\rmi - \bmmh_j(\xi, \pmb\theta_j))^2$
        \EndFor
        \State $\hat{j} = \argmin_{j} \sum  \limits_{i=1}^n (\rmi - \bmmh_j(\xi, \thetamh_j))^2$
        \State Update $\fm(\xv) = \fmd(\xv) + \nu b_{\hat{j}}(\xv, \thetamh_{\hat{j}})$
      \EndFor
      \State Output $\fh(\xv) = f^{[M]}(\xv)$
    \end{algorithmic}
    \end{center}
    \end{footnotesize}
\end{algorithm}
