\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/gbm_anim_51.png}
\newcommand{\learninggoals}{
  \item Examples for GB with trees
  \item Understand relationship between model structure and interaction depth
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting with Trees 1}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Gradient boosting with trees}

Trees are most popular BLs in GB.

\begin{blocki}{Reminder: advantages of trees}
\item No problems with categorical features.
\item No problems with outliers in feature values.
\item No problems with missing values.
\item No problems with monotone transformations of features.
\item Trees (and stumps!) can be fitted quickly, even for large $n$.
\item Trees have a simple, built-in type of variable selection.
%\item Interpretation of Trees is rather easy.
\end{blocki}
GB with trees inherits these, and strongly improves predictive power.
% Furthermore, it is possible to adapt gradient boosting to tree learners in a targeted manner.

%\framebreak
\end{vbframe}
% ------------------------------------------------------------------------------
\begin{vbframe}{Example 1}
\begin{footnotesize}
\textbf{Simulation setting:}
\begin{itemize}
\item Given: one feature $x$ and one numeric target variable $y$ of 50 observations.
\item $x$ is uniformly distributed between 0 and 10.
\item $y$ depends on $x$ as follows: $y^{(i)} = \sin{(x^{(i)})} + \epsilon^{(i)}$ with $\epsilon^{(i)} \sim \mathcal{N}(0, 0.01)$, $\forall i \in \{1, \dots, 50\}$.
\end{itemize}

\vfill

\begin{minipage}[c]{0.55\textwidth}
  \vspace{0pt}%
  \includegraphics[width = 0.9\textwidth]{figure/gbm_anim_data.png}
\end{minipage}%
\begin{minipage}[c]{0.02\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.4\textwidth}
  \vspace{0pt}%
  \raggedright
  \textbf{Aim:} we want to fit a gradient boosting model to the data by using 
  stumps as base learners.
  
  \lz
  Since we are facing a regression problem, we use $L2$ loss.
\end{minipage}%

\framebreak

% ------------------------------------------------------------------------------

% !!! ADJUST VALUES IF YOU MODIFY SIMULATION (rsrc file contains kable commands)

\textbf{Iteration 0:} initialization by optimal constant (mean) prediction 
$\hat f^{[0](i)}(x) = \bar{y} \approx 0.2$.

\vfill

\begin{minipage}[c]{0.35\textwidth}
  \vspace{0pt}%
  \scriptsize
  \centering
  \begin{tabular}{r|r|r|r}
    $i$ & $x^{(i)}$ & $\yi$ & $\hat{f}^{[0]}$ \\
    \hline
    1 & 0.03 & 0.16 & 0.20 \\
    2 & 0.03 & -0.06 & 0.20 \\
    3 & 0.07 & 0.09 & 0.20 \\
    \vdots & \vdots & \vdots & \vdots \\
    50 & 9.69 & -0.08 & 0.20 \\
  \end{tabular}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.6\textwidth}
  \vspace{0pt}%
  \includegraphics{figure/gbm_anim_init.png}
\end{minipage}%

\end{footnotesize}

\framebreak

% ------------------------------------------------------------------------------

% !!! ADJUST VALUES IF YOU MODIFY SIMULATION (rsrc file contains kable commands)

\begin{footnotesize}

\textbf{Iteration 1:} (1) Calculate pseudo-residuals $\tilde{r}^{[m](i)}$ and (2) 
fit a regression stump $b^{[m]}.$ %which is multiplied by the learning rate $\beta$% (here: $\hat \beta^{[1]} = 1$).

\vfill

\begin{minipage}[c]{0.5\textwidth}
  \vspace{0pt}%
  \centering
  \scriptsize
  \begin{tabular}{r|r|r|r|r|r}
    $i$ & $x^{(i)}$ & $\yi$ & $\hat{f}^{[0]}$ & $\tilde{r}^{[1](i)}$ & 
    $\hat{b}^{[1](i)}$\\ 
    \hline
    1 & 0.03 & 0.16 & 0.20 & -0.04 & -0.17 \\
    2 & 0.03 & -0.06 & 0.20 & -0.25 & -0.17 \\
    3 & 0.07 & 0.09 & 0.20 & -0.11 & -0.17 \\
    \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
    50 & 9.69 & -0.08 & 0.20 & -0.27 & 0.33 \\
  \end{tabular}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.45\textwidth}
  \vspace{0pt}%
  \includegraphics{figure/gbm_anim_init.png}
\end{minipage}%

\vfill

(3) Update model by $\hat{f}^{[1]}(x) = \hat{f}^{[0]}(x) + \hat{b}^{[1]}.$
\end{footnotesize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Example 1}

\footnotesize
Repeat step (1) to (3):

\begin{center}
  \only<1>{ \includegraphics[width=\textwidth]{figure/gbm_anim_02.png} }
  \only<2>{ \includegraphics[width=\textwidth]{figure/gbm_anim_03.png} }
  %\only<3>{ \includegraphics[width=\textwidth]{figure/gbm_anim_04.png} }
  %\only<4>{ \includegraphics[width=\textwidth]{figure/gbm_anim_05.png} }
  %\only<5>{ \includegraphics[width=\textwidth]{figure/gbm_anim_06.png} }
  \only<3>{ \includegraphics[width=\textwidth]{figure/gbm_anim_51.png} }
\end{center}

\end{frame}
% ------------------------------------------------------------------------------

\begin{vbframe}{Example 2}

This \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{\textcolor{blue}{website}} shows on various 3D examples how tree depth and number of iterations influence the model fit of a GBM with trees. 
% \begin{itemize}
%   \item Given: 2-dimensional regression problem
%   \item Aim: reconstruct $ y = \fx = f (x_1, x_2) $
%   %\item Link: \href{http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html}{Open in browser}
% \end{itemize}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/gbm_anim/old/gbm5.jpg}
\end{center}



\end{vbframe}

% \begin{frame}{Visualization 3}
% Tree depth: 2 \\
% Number of trees: 
% \only<1>{1}
% \only<2>{3}
% \only<3>{5}
% \only<4>{7}
% \only<5>{10}
% 
% \begin{columns}
%   \begin{column}{0.49\textwidth}
%     \begin{center}
%     Prediction of previous trees 
%         \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_1.png}}
%         \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_3.png}}
%         \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_5.png}}
%         \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_7.png}}
%         \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_target_10.png}}
%         
%     \end{center}  
%   \end{column}
%   \begin{column}{0.49\textwidth}
%     \begin{center}
%     Residuals 
%         \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_1.png}}
%         \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_3.png}}
%         \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_5.png}}
%         \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_7.png}}
%         \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_2_res_10.png}}
%     \end{center}
%   \end{column}
% \end{columns}
% \end{frame}
% 
% 
% \begin{frame}{Visualization 3}
% Tree depth: 1 \\
% Number of trees: 
% \only<1>{1}
% \only<2>{3}
% \only<3>{5}
% \only<4>{7}
% \only<5>{10}
% 
% \begin{columns}
%   \begin{column}{0.49\textwidth}
%     \begin{center}
%     Prediction of previous trees 
%         \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_1.png}}
%         \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_3.png}}
%         \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_5.png}}
%         \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_7.png}}
%         \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_target_10.png}}
%         
%     \end{center}  
%   \end{column}
%   \begin{column}{0.49\textwidth}
%     \begin{center}
%     Residuals 
%         \only<1>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_1.png}}
%         \only<2>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_3.png}}
%         \only<3>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_5.png}}
%         \only<4>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_7.png}}
%         \only<5>{\includegraphics[width=\textwidth]{figure_man/gbm_anim/gbm_tree_anim_depth_1_res_10.png}}
%     \end{center}
%   \end{column}
% \end{columns}
% \end{frame}

%--------------------------------------------------------------------------------------------------------
% \begin{vbframe}{Visualization 1}
% 
% As before, we want to minimize the empirical risk defined by
% $$
% \riskef = \sum_{i=1}^n L\left(\yi,\fxi \right) =
% \sum_{i=1}^n L\left(\yi, \sum_{m=1}^M \bl(\xv^{(i)})\right)
% $$
% 
% with trees as base learners $ b(\xv) = \sum_{t=1}^{T} c_t \mathds{1}_{\{\xv \in R_t\}} $,
% where $R_t$ are the terminal regions and $c_t$ the corresponding constant parameters.\\
% \lz
% The additive structure of the boosting model can be written as follows:
% $$
% \fm(\xv) = \fmd(\xv) +  \bl(\xv) = \fmd(\xv) +  \sum_{t=1}^{\Tm} \ctm \mathds{1}_{\{\xv \in \Rtm\}}. 
% $$
% 
% \framebreak
% 
% While the parameters we need to learn e.g. for an LM are the respective coefficients, for trees we need to learn the tree structure which is usually harder. It follows that we want to add at each iteration step $m$ the tree $\bl(\xv)$ which minimizes the chosen loss function.
% 
% $$
% \hat{\mathbf{c}}^{[m]} = \argmin_{(c_1,\dots,c_{T^{[m]}})}\sum_{i = 1}^n L(\yi, \fmd(\xi) + \bl(\xi, c_1,\dots,c_{T^{[m]}})).
% $$
% 
% \begin{center}
% 
% \includegraphics[width=0.38\textwidth]{figure_man/gbm_leaf_adjustment.pdf}
% 
% \end{center}
% 
% \framebreak
% 
% The direction of steepest descent for the update is then
% 
% $$
% -\frac{\partial L(y, \fmd(\xv) + \bl(\xv))}{\partial \bl(\xv)}
% $$
% 
% Using MSE as loss function, a direct solution is possible. However, for other loss functions a Taylor approximation is necessary which will be discussed in more detailed in the xgboost chapter.
% 
% 
% 
% \framebreak
% 
% \input{algorithms/gradient_tree_boosting_algorithm2.tex}
% 
% \framebreak

\begin{vbframe}{Model structure and Interaction Depth}

\begin{footnotesize}
Model structure directly influenced by depth of $\bl(\xv)$.
$$
f(\xv) =  \sum_{m=1}^M \alpha^{[m]} \bl(\xv)
$$
Remember how we can write trees as additive model over paths to leafs.

\lz
\begin{columns}[T]
\begin{column}{0.6\textwidth}
With stumps (depth = 1), $\fx$ is additive model (GAM) without interactions:\\
$$
f(\xv) = f_0 + \sum_{j=1}^p f_j(x_j)
$$
With trees of depth 2, we have two-way interactions:\\
$$
f(\xv) = f_0 + \sum_{j=1}^p f_j(x_j) + \sum_{j \neq k} f_{j,k}(x_j, x_k)
$$

with $f_0$ being a constant intercept.
\end{column}
\begin{column}{0.4\textwidth}
\includegraphics[width=0.8\textwidth]{figure_man/boosting_interact_expl1.PNG}
\includegraphics[width=\textwidth]{figure_man/boosting_interact_expl2.PNG}
\end{column}
\end{columns}
\end{footnotesize}

% Figures are created in google slides: https://docs.google.com/presentation/d/1iZq25FpXUxNhy8NEuzsentcKGWD4lbnb2jNlbd2IcMU/edit?usp=sharing
\framebreak


\textbf{Simulation setting:}
\begin{itemize}
\item Features $x_1$ and $x_2$ and numeric $y$; with $n=500$
\item $x_1$ and $x_2$ are uniformly distributed between -1 and 1
\item $y^{(i)} = x_1^{(i)} -  x_2^{(i)} + 5\cos(5 x_2^{(i)}) \cdot x_1^{(i)} + \epsilon^{(i)}$ with $\epsilon^{(i)} \sim \mathcal{N}(0, 1)$
\item We fit 2 GB models, with tree depth 1 and 2, respectively. 
\end{itemize} 

\vfill

\begin{center}
\includegraphics[width=0.6\textwidth]{ figure_man/boosting_interaction_targetfunction3D.png}
\end{center}

\framebreak

% Figures are created in google slides: https://docs.google.com/presentation/d/1iZq25FpXUxNhy8NEuzsentcKGWD4lbnb2jNlbd2IcMU/edit?usp=sharing
% \textbf{GBM with interaction depth of 1 (GAM)}\\
% No interactions are modelled: Marginal effects of $x_1$ and $x_2$ add up to joint effect (plus the constant intercept $\hat{f_0} = -0.07$)
% 
% \begin{center}
% \includegraphics[width=0.3\textwidth]{figure_man/boosting_interaction_example_gam.png}
% \includegraphics[width=0.25\textwidth]{figure_man/boosting_interaction_depth1fit3D_x1.png}
% \includegraphics[width=0.43\textwidth]{figure/interaction_td1_d3.png}
% \end{center}
% \begin{align*}
% \hat{f}(-0.999,-0.998) &= \hat{f_0} + \hat{f_1}(-0.999) + \hat{f_2}(-0.998)\\ 
% &= -0.07 + 0.3 + 0.96 = 1.19
% \end{align*}

\framebreak

\textbf{GBM with interaction depth of 1 (GAM)}\\
No interactions are modelled: Marginal effects of $x_1$ and $x_2$ add up to joint effect (plus the constant intercept $\hat{f_0} = -0.07$).

\vspace{0.5cm}

\begin{minipage}[c]{0.4\textwidth}
  \centering
  \includegraphics[width=0.75\textwidth]{
  figure/boosting_interaction_example_gam.png}
\end{minipage}%
\begin{minipage}[c]{0.3\textwidth}
  \centering
  \includegraphics[width=0.75\textwidth, trim = 0 60 0 50, clip]{
  figure_man/boosting_interaction_depth1fit3D_x1.png}
\end{minipage}%
\begin{minipage}[c]{0.3\textwidth}
  \includegraphics[width=0.75\textwidth, trim = 0 70 0 30, clip]{
  figure_man/boosting_interaction_depth1fit3D_x2.png}
\end{minipage}%

\vfill

\begin{minipage}[c]{0.4\textwidth}
  \footnotesize
  $\hat{f}(-0.999,-0.998)$\\
  $= \hat{f_0} + \hat{f_1}(-0.999) + \hat{f_2}(-0.998)$ \\
  $= -0.07 + 0.3 + 0.96 = 1.19$
\end{minipage}%
\begin{minipage}[c]{0.6\textwidth}  
  \centering
  \includegraphics[width=0.7\textwidth]{figure/interaction_td1_d3.png}
\end{minipage}%

% \begin{center}
% \includegraphics[width=0.3\textwidth]{figure_man/boosting_interaction_example_gam.png}
% \includegraphics[width=0.25\textwidth]{figure_man/boosting_interaction_depth1fit3D_x2.png}
% \includegraphics[width=0.43\textwidth]{figure/interaction_td1_d3.png}
% \end{center}

% \footnotesize
% 
% \begin{align*}
% \hat{f}(-0.999,-0.998) &= \hat{f_0} + \hat{f_1}(-0.999) + \hat{f_2}(-0.998)\\ 
% &= -0.07 + 0.3 + 0.96 = 1.19
% \end{align*}

\framebreak

\textbf{GBM with interaction depth of 2}\\
Interactions between $x_1$ and $x_2$ are modelled: Marginal effects of $x_1$ and $x_2$ do NOT add up to joint effect due to interaction effects.

\begin{center}
\includegraphics[width=0.4\textwidth]{figure/boosting_interaction_example_ID2.png}
\includegraphics[width=0.48\textwidth]{figure/interaction_td2_d3.png}
\end{center}

% \framebreak
% Comparison of ID = 1 (left), ID =2 (right) and target function (bottom).
% \begin{center}
% \includegraphics[width=0.47\textwidth]{figure_man/boosting_interaction_depth1fit3D.PNG}
% \includegraphics[width=0.47\textwidth]{figure_man/boosting_interaction_depth2fit3D.PNG}
% \includegraphics[width=0.47\textwidth]{figure_man/boosting_interaction_targetfunction3D.PNG}
% \end{center}

\end{vbframe}

\endlecture
\end{document}
