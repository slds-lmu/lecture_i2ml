\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure_man/adaboost_example_adjusted.PNG}
\newcommand{\learninggoals}{
  \item General idea of boosting
  \item Introduction of AdaBoost algorithm 
  \item Bagging vs. Boosting
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Introduction to Boosting / AdaBoost}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Introduction to boosting}
  \begin{itemize}
    \item
      Boosting is considered to be one of the most powerful learning ideas within the last twenty years.
    \item
      Originally designed for classification, (especially gradient) boosting handles regression (and many other supervised tasks) naturally nowadays.
    \item
      Homogeneous ensemble method (like bagging), but fundamentally different approach.
    \item
      {\bf Idea:} Take a weak classifier and sequentially apply it to modified versions of the training data.
    \item
      We will begin by describing an older, simpler boosting algorithm designed for binary classification, the popular \enquote{AdaBoost}.
  \end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Boosting vs. Bagging}

% The general concept of boosting is a sequential fitting of weak learner on the error:
\begin{center}
\includegraphics[width=0.65\textwidth]{figure_man/bagging_vs_boosting.png}
\end{center}

% In bagging, the models are fitted parallel and not sequential.

\end{vbframe}

\begin{vbframe}{The boosting question}

The first boosting algorithm ever was in fact no algorithm for practical purposes, but the solution for a theoretical problem:

\lz

\enquote{Does the existence of a weak learner for a certain problem imply
the existence of a strong learner?} (Kearns, 1988)

\lz

\begin{itemize}
\item \textbf{Weak learners} are defined as a prediction rule with a correct classification rate that is at least slightly better than random guessing (> 50\% accuracy on a balanced binary problem).
\item We call a learner a \textbf{strong learner} \enquote{if there exists a polynomial-time algorithm that achieves low error with high confidence for all concepts in the class} (Schapire, 1990).

\end{itemize}

In practice it is typically easy to construct weak learners, but difficult to build a strong one.

\end{vbframe}

% ------------------------------------------------------------------------------

% \section{AdaBoost}

\begin{vbframe}{The boosting answer - AdaBoost}

Any weak (base) learner can be iteratively boosted to become
a strong learner (Schapire and Freund, 1990).
The proof of this ground-breaking idea generated the first boosting algorithm.

\begin{itemize}
  \item The \textbf{AdaBoost} (Adaptive Boosting) algorithm is a \textbf{boosting} method
    for binary classification by Freund and Schapire (1997).
  \item The base learner is sequentially applied to weighted training observations.
  \item After each base learner fit, currently misclassified observations receive a higher weight for
    the next iteration, so we focus more on instances that are harder to classify.
\end{itemize}

Leo Breiman (referring to the success of AdaBoost):

\enquote{Boosting is the best off-the-shelf classifier in the world.}

\framebreak

\begin{itemize}
  \item Assume a target variable $y$ encoded as $\setmp$,
    and weak base learners (e.g., tree stumps) from a hypothesis space $\mathcal{B}$.
  \item Base learner models $\bmm$ are binary classifiers that map to $\Yspace = \setmp$.
    We might sometimes write $\bmmxth$ instead.
  \item Predictions from all base models $\bmm$ over $M$ iterations are combined in an additive manner by the formula:
    $$
    \fx = \sum_{m=1}^{M} \betam \bmm(\xv).
    $$
  \item Weights $\betam$ are computed by the boosting algorithm.
    Their purpose is to give higher weights to base learners with higher predictive accuracy.
  \item The number of iterations $M$ is the main tuning parameter.
  \item The discrete prediction function is $h(\xv) = \text{sign}(\fx) \in \setmp$.
\end{itemize}

% \framebreak
% 
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]
%     \State Initialize observation weights: $w^{[1](i)} = \frac{1}{n} \quad \forall i \in \nset$
%     \For {$m = 1 \to M$}
%       \State Fit classifier to training data with weights $\wm$ and get $\bmmh$
%       \State Calculate weighted in-sample misclassification rate
%       $$
%         \errm = \frac{\sumin \wmi \cdot \mathds{1}_{\{\yi \,\neq\, \bmmh(\xi)\}}}{\sumin \wmi}
%       $$
%       \State Compute: $ \betamh = \frac{1}{2} \log \left( \frac{1 - \errm}{\errm}\right)$
%       \State Set: $w^{[m+1](i)} = \wmi \cdot \exp\left(\betamh \cdot
%         \mathds{1}_{\{\yi \,\neq\, \bmmh(\xi)\}} \right)$
%     \EndFor
%     \State Output: $\fxh = \sum_{m=1}^{M} \betamh \bmmh(\xv)$
%   \end{algorithmic}
%   \caption{AdaBoost}
% \end{algorithm}

%\end{vbframe}

\framebreak

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \State Initialize observation weights: $w^{[1](i)} = \frac{1}{n} \quad \forall i \in \nset$
    \For {$m = 1 \to M$}
      \State Fit classifier to training data with weights $\wm$ and get $\bmmh$
      \State Calculate weighted in-sample misclassification rate
      $$
        \errm = \sumin \wmi \cdot \mathds{1}_{\{\yi \,\neq\, \bmmh(\xi)\}}
      $$
      \State Compute: $ \betamh = \frac{1}{2} \log \left( \frac{1 - \errm}{\errm}\right)$
      \State Set: $w^{[m+1](i)} = \wmi \cdot \exp\left(- \betamh \cdot
        \yi \cdot \hat{h}(\xi)\right) $
      \State Normalize $w^{[m+1](i)}$ such that $\sumin w^{[m+1](i)} = 1$
    \EndFor
    \State Output: $\fxh = \sum_{m=1}^{M} \betamh \bmmh(\xv)$
  \end{algorithmic}
  \caption{AdaBoost}
\end{algorithm}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Adaboost illustration}
\begin{footnotesize}

\textbf{Example description}

\begin{itemize}
  \item $n = 10$ observations and two features $x_1$ and $x_2$ 
  \item Tree stumps as base learners $\bmm(\xv)$
  \item Balanced classification task with $y$ encoded as $\setmp$
  \item $M = 3$ iterations $\Rightarrow$ initial weights 
  $w^{[1](i)} = \frac{1}{10} \quad \forall i \in 1,\dots ,10$. 
  % \item The label of every observation is represented by triangle or circle.
  % \item Dark grey area: Base model in iteration $m$ predicts triangle.
  % \item Light grey area: Base model in iteration $m$ predicts circle.
\end{itemize}

\vfill

\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width=0.9\textwidth]{figure/adaboost_viz_mlr3_1.png}\\
  \textbf{Iteration} $m$ = 1:
  \begin{itemize}
    \item $\text{err}^{[1]} = 0.3$
    % \item The ratio $(1 - \errm) / \errm$ used to calculate the weights 
    \item $\hat{\beta}^{[1]} = \frac{1}{2} \log \left( \frac{1 - 0.3}{0.3} 
    \right) \approx 0.42$
  \end{itemize}
\end{minipage}%
\begin{minipage}[b]{0.55\textwidth}
  New observation weights:
  \begin{itemize}
    %\item $\wmi \cdot \exp \left(\betamh \cdot \mathds{1}_{\{\yi \neq \bmmh(\xi)\}} \right)$
    \item Prediction correct: \\
      $w^{[2](i)} = w^{[1](i)} \cdot \exp \left(-\hat \beta^{[1]} \cdot 1 
      \right)$\\ $\approx 0.065.$
    \item For 3 misclassified observations: \\
      $w^{[2](i)} = w^{[1](i)} \cdot \exp \left(-\hat \beta^{[1]} \cdot (-1) 
      \right)$\\ $\approx 0.15.$
    \item After normalization: 
    \begin{itemize}
      \begin{footnotesize}
      \item correctly classified: $w^{[2](i)} \approx 0.07$
      \item misclassified: $w^{[2](i)} \approx 0.17$
      \end{footnotesize}
    \end{itemize}
\end{itemize}
\end{minipage}

\end{footnotesize}

% ------------------------------------------------------------------------------

\framebreak

\begin{minipage}[c]{0.4\textwidth}
  \includegraphics[width = \textwidth]{figure/adaboost_viz_mlr3_2.png}
\end{minipage}%
\begin{minipage}[c]{0.05\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[c]{0.6\textwidth}
  \begin{footnotesize}
  \textbf{Iteration} $m = 2$:
  \begin{itemize}
    \item $\text{err}^{[2]} = 3 \cdot 0.07 = 0.21$ 
    \item $\hat{\beta}^{[2]} \approx 0.65$
    % \item The ratio $(1 - \errm) / \errm$ used to calculate the weights 
  \end{itemize}
  New observation weights:
  \begin{itemize}
    \item E.g., for 3 misclassified observations:
      $w^{[3](i)} = w^{[2](i)} \cdot \exp \left(-\hat \beta^{[1]} \cdot (-1) 
      \right) \approx 0.14.$
    \item After normalization: $w^{[3](i)} \approx 0.17$ (misclassified)
  \end{itemize}
  \textbf{Iteration} $m = 3$:
  \begin{itemize}
    \item $\text{err}^{[3]} = 3 \cdot 0.045 \approx 0.14$ 
    \item $\hat{\beta}^{[3]} \approx 0.92$
  % \item The ratio $(1 - \errm) / \errm$ used to calculate the weights 
  \end{itemize}
  \end{footnotesize}
\end{minipage}

\vfill

\begin{footnotesize}
\textbf{Note:} the smaller the error rate of a base learner, the larger the 
weight, e.g., $\text{err}^{[3]} \approx 0.14 < \text{err}^{[1]} \approx 0.3$ 
and $\hat \beta^{[3]} \approx 0.92 > \hat \beta^{[1]} \approx 0.42.$
\end{footnotesize}

\framebreak

% ------------------------------------------------------------------------------

With $\fxh = \sum_{m=1}^{M} \betamh \bmmh(\xv)$ and $h(\xv) = \text{sign}(\fx) 
\in \setmp$, \\we get:

\begin{center}
  \includegraphics[trim = 0 20 0 10, clip, width = 0.8\textwidth]{
  figure_man/adaboost_example_adjusted.PNG}
\end{center}

Hence, when all three base classifiers are combined, all samples are classified 
correctly.

\end{vbframe}

%\begin{vbframe}{Adaboost illustration}

%\begin{columns}
%\column{7cm}

%%\includegraphics[width=7cm]{figure_man/adaboost_example2.png}
%\includegraphics[width=7cm]{figure_man/adaboost_example_adjusted.PNG}
%%

%{\footnotesize Schapire, Boosting, 2012.}

%\column{3cm}

%The three base models are combined into one classifier.

%All observations are correctly classified.

%\end{columns}

%\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bagging vs Boosting}

\begin{minipage}[t]{0.47\textwidth}
  \textbf{Random forest}
  \begin{itemize}
    \item Base learners are typically deeper decision trees (not only stumps!)
    \item Equal weights for base learners
    \item Base learners independent of each other
    \item Aim: variance reduction
    \item Tends \textbf{not} to overfit
  \end{itemize}
\end{minipage}%
\begin{minipage}[t]{0.06\textwidth}
  \phantom{foo}
\end{minipage}%
\begin{minipage}[t]{0.47\textwidth}
  \textbf{AdaBoost}
  \begin{itemize}
    \item Base learners are weak learners, e.g., only stumps
    \item Base learners have different weights depending on their predictive 
    accuracy
    \item Sequential algorithm, hence order matters
    \item Aim: bias and variance reduction
    \item Tends to overfit
  \end{itemize}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bagging vs Boosting Stumps}

Random forest versus AdaBoost (both with stumps) on \texttt{Spirals} data from 
\texttt{mlbench} ($n=200$, $sd=0$), with $5 \times 5$ repeated CV.

% \begin{minipage}[c]{0.6\textwidth}
%   \includegraphics[width = \textwidth]{figure_man/stump_plot_ntree.png}
% \end{minipage}%
% \begin{minipage}[c]{0.4\textwidth}
%   \begin{minipage}[c]{\textwidth}
%     \includegraphics[width = \textwidth]{figure_man/stump_plot_rf.png}
%   \end{minipage}
%   \begin{minipage}[c]{\textwidth}
%     \includegraphics[width = \textwidth]{figure_man/stump_plot_boost.png}
%   \end{minipage}
% \end{minipage}

\vfill

\includegraphics[width=\textwidth]{figure/stump_plots.png}

\vfill

Weak learners do not work well with bagging as only variance, but no bias reduction happens.

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Overfitting behavior}

Historically, the overfitting behavior of AdaBoost was often discussed.
% Random Forest versus AdaBoost on \texttt{Spirals} data from \texttt{mlbench} ($n=200$, $sd=0.3$).
% With $5 \times 5$ repeated CV
Increasing standard deviation to $sd = 0.3$ and allowing for more flexibility in 
the base learners, AdaBoost overfits with increasing number of trees while the 
RF only saturates.
The overfitting of AdaBoost here is quite typical as data is very noisy.

\vfill

\includegraphics[width=\textwidth]{figure/stump_plots_noisy.png}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Overfitting behavior}

% A long-lasting discussion in the context of AdaBoost is its overfitting behavior.

% % \lz

% % \emph{When a prediction rule concentrates too much on peculiarities of the specific sample of training observations, it will often perform poorly on a new data set.}

% % \lz

% The main instrument to avoid overfitting is the stopping iteration $M$:
% \begin{itemize}
% \item High values of $M$ lead to complex solutions. Overfitting?
% \item Small values of $M$ lead to simple solutions. Underfitting?
% \end{itemize}
% Although it will overfit eventually, AdaBoost in general shows a rather slow overfitting behavior.
% \framebreak

% As an example we have a look on random forest vs. AdaBoost on the \textit{Spiral} data
% from \texttt{mlbench} (n = 200, sd = 0.3). Performance (mmce) is measured
% with 5-fold CV.

% <<comparing-rf-ada1, echo=FALSE, fig.width=8, fig.height=4>>=
% load("rsrc/overfitting_ada_rf.Rdata")
% result$M = as.numeric(as.character(result$M))
% p = ggplot(result, aes(x = M, y = mmce, group = learner)) + 
%   geom_line()  +
%   xlab("number trees")
% p
% @

% AdaBoost overfits with increasing number of trees while
% the mmce of the random forest fluctuates on a constant level for the test data.
% \framebreak

% This becomes even more apparent when we take a look on the prediction surface. 
  
% <<comparing-rf-ada2, echo=FALSE, fig.width=13, fig.height=4.5 >>=
% load("rsrc/overfitting_plots.RData")
% learnerPredPlot1 = learnerPredPlot1  + ggplot2::ggtitle("rf") 
% learnerPredPlot2 = learnerPredPlot2 + ggplot2::ggtitle("adaboost")
% gridExtra::grid.arrange(learnerPredPlot1, learnerPredPlot2, ncol = 2)
% @

% \end{vbframe}

% % \begin{vbframe}{Software in R}
% %   \begin{itemize}
% %     \item Bagging: Package \pkg{mlr} is able to bag any learner with \code{makeBaggingWrapper()}.
% %   \item Random Forests: Package \pkg{randomForest} with function
% %     \code{randomForest()} based on CART.
% %   \item AdaBoost: included in the packages \code{ada} and \code{boosting}.
% % \end{itemize}
% % \end{vbframe}



\endlecture
\end{document}

