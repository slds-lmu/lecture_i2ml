\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-bagging.tex}
\input{../../latex-math/ml-boosting.tex}
\input{../../latex-math/ml-trees.tex}

%\newcommand{\titlefigure}{figure_man/split-finding02.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Deep Dive: XGBoost Optimization}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Risk minimization}

    \pkg{XGBoost} uses a risk function with 3 regularization terms:

    \begin{multline*}
      \riskr^{[m]} = \sum_{i=1}^{n} L\left(\yi, \fmd(\xi) + \bmm(\xi)\right)\\
       + \lambda_1 J_1(\bmm) + \lambda_2 J_2(\bmm) + \lambda_3 J_3(\bmm),
    \end{multline*}

    \lz

    with $J_1(\bmm) = T^{[m]}$ the number of leaves in the tree to penalize tree depth.

    \lz

    $J_2(\bmm) = \left\|\mathbf{c}^{[m]}\right\|^2_2$ and $J_3(\bmm) = \left\|\mathbf{c}^{[m]}\right\|_1$ are $L2$ and $L1$ penalties of the terminal region values $c_t^{[m]}, t=1,\dots,T^{[m]}$.

    \lz

    We define $J(\bmm) := \lambda_1 J_1(\bmm) + \lambda_2 J_2(\bmm) + \lambda_3 J_3(\bmm)$.

    \framebreak

    %Recall the way a tree base learner $\bmm(\xv)$ can be fitted loss-optimally in gradient boosting:

    %$$
    %\tilde{\mathbf{c}}^{[m]} = \argmin_{(c_1,\dots,c_{T^{[m]}})}\sum_{i = 1}^n L(\yi, \fmd(\xi) + \bmm(\xi, c_1,\dots,c_{T^{[m]}})).
    %$$

    %The direction of steepest descent for the update is then

    %$$
    %-\frac{\partial L(y, \fmd(\xv) + \bmm(\xv))}{\partial \bmm(\xv)}
    %$$

    %for each $\xi, i = 1,\dots, n$.

    %\lz

    %\textbf{Note:} $J(\bmm)$ is omitted for now but will be re-introduced later.

    %\framebreak

    To approximate the loss in iteration $m$, a second-order Taylor expansion around $\fmd(\xv)$ is computed:
    \vskip -1em
    \begin{align*}
      &L(y, \fmd(\xv) + \bmm(\xv)) \approx \\
      &\qquad L(y, \fmd(\xv)) + g^{[m]}(\xv)\bmm(\xv) + \frac12 h^{[m]}(\xv)\bmm(\xv)^2,
    \end{align*}

    with gradient

    $$
    g^{[m]}(\xv) = \frac{\partial L(y, \fmd(\xv))}{\partial \fmd(\xv)}
    $$

    and Hessian

    $$
    h^{[m]}(\xv) = \frac{\partial^2 L(y, \fmd(\xv))}{\partial {\fmd(\xv)}^2}.
    $$

    \textbf{Note:} $g^{[m]}(\xv)$ are the negative pseudo-residuals $-\rmm$ we use in standard gradient boosting to determine the direction of the update.

    \framebreak

    Since $L(y, \fmd(\xv))$ is constant, the optimization simplifies to

    \begin{align*}
    \riskr^{[m]} = &\sum_{i = 1}^n g^{[m]}(\xi)\bmm(\xi) + \frac12 h^{[m]}(\xi)\bmm(\xi)^2 + J(\bmm) + const\\
    \propto&\sum_{t=1}^{T^{[m]}}\sum_{\xi\in R_t^{[m]}} g^{[m]}(\xi)c^{[m]}_t + \frac12 h^{[m]}(\xi)(c^{[m]}_t)^2 + J(\bmm) \\
    =& \sum_{t=1}^{T^{[m]}}G^{[m]}_t c^{[m]}_t+\frac12 H^{[m]}_t (c^{[m]}_t)^2 + J(\bmm).
    \end{align*}

    Where $G^{[m]}_t$ and $H^{[m]}_t$ are the accumulated gradient and Hessian values in terminal node $t$.



    \framebreak

    Expanding $J(\bmm)$:
    \begin{align*}
    \riskr^{[m]} = &\sum_{t=1}^{T^{[m]}}\left(G^{[m]}_t c^{[m]}_t+\frac12 H^{[m]}_t (c^{[m]}_t)^2 + \frac12\lambda_2(c^{[m]}_t)^2 + \lambda_3 |c^{[m]}_t|\right) + \lambda_1 T^{[m]}\\
    =&\sum_{t=1}^{T^{[m]}}\left(G^{[m]}_t c^{[m]}_t+\frac12 (H^{[m]}_t + \lambda_2) (c^{[m]}_t)^2 + \lambda_3 |c_t^{[m]}|\right) + \lambda_1 T^{[m]}.
    \end{align*}

    \lz

    \textbf{Note:} The factor $\frac12$ is added to the $L2$ regularization to simplify the notation as shown in the second step.
    This does not impact estimation since we can just define $\lambda_2 = 2\tilde\lambda_2$.

    \framebreak

    Computing the derivative for a terminal node constant value $c^{[m]}_t$ yields

    $$
    \frac{\partial \riskr^{[m]}}{\partial c^{[m]}_t} = (G^{[m]}_t + \sign{(c^{m}_t)}\lambda_3) + (H^{[m]}_t + \lambda_2) c^{m}_t.
    $$

    The optimal constants $\hat{c}^{[m]}_1,\dots, \hat{c}^{[m]}_{T^{[m]}}$ can then be calculated as

    \lz
    $$
    \hat{c}^{[m]}_t = - \frac{t_{\lambda_3}\left(G^{[m]}_t\right)}{H^{[m]}_t + \lambda_2}, t=1,\dots T^{[m]},
    $$
    with $$t_{\lambda_3}(x) = \begin{cases}
      x + \lambda_3 &\text{ for } x < - \lambda_3 \\
      0  &\text{ for } |x| \leq \lambda_3 \\
      x - \lambda_3 &\text{ for } x > \lambda_3.
    \end{cases}$$

    \end{vbframe}



\begin{vbframe}{Loss minimization - split finding}

    To evaluate the performance of a candidate split that divides the instances in region $R_t^{[m]}$ into a left and right node we use the \textbf{risk reduction} achieved by that split:
    $$
    \tilde S_{LR} =
     \frac12 \left[
     \frac{t_{\lambda_3} \left( G^{[m]}_{tL} \right)^2}{H^{[m]}_{tL} + \lambda_2} + \frac{t_{\lambda_3}\left(G^{[m]}_{tR}\right)^2}{H^{[m]}_{tR} + \lambda_2} - \frac{t_{\lambda_3}\left(G^{[m]}_{t}\right)^2}{H^{[m]}_{t} + \lambda_2}
     \right] - \lambda_1,
    $$
    where the subscripts $L$ and $R$ denote the left and right leaves after the split.

    % derivation for this: write out change in loss function, case distinction acording to sign of t_a(G)/(H + lambda_2)

    \lz

    \framebreak

    \begin{algorithm}[H]

    \begin{footnotesize}
    \begin{center}

      \begin{algorithmic}[1]
        \State \textbf{Input} $I$: \emph{instance set of current node}
        \State \textbf{Input} $p$: \emph{dimension of feature space}
        \State $gain \gets 0$
        \State $G \gets \sum_{i \in I} g(\xi), {H} \gets \sum_{i \in I} h(\xi)$
        \For{$j = 1 \to p$}
          \State $G_L \gets 0, {H}_L \gets 0$
          \For{$i$ in sorted($I$, by $x_{j}$)}
            \State ${G}_L \gets {G}_L + g(\xi), {H}_L \gets {H}_L + h(\xi)$
            \State ${G}_R \gets G - {G}_L, {H}_R \gets {H} - {H}_L$
            \State compute $\tilde S_{LR}$
          \EndFor
        \EndFor
        \State \textbf{Output} Split with maximal $\tilde S_{LR}$
      \end{algorithmic}
    \end{center}
    \end{footnotesize}
    \caption{(Exact) Algorithm for split finding}
    \end{algorithm}

    \end{vbframe}

    \endlecture
\end{document}
