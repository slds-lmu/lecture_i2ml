\input{../../style/preamble} 
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/illustration_title.png}
\newcommand{\learninggoals}{
  \item See simple visualizations of boosting in regression
  \item Understand impact of different losses and base learners
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Gradient Boosting - Illustration}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Gradient boosting illustration - GAM}

% We now compare different loss functions and base learners.
GAM / Splines as BL and compare $L2$ vs. $L1$ loss.
% the $L1$ loss.\\
\vspace*{0.2cm}

% \textbf{Reminder:} Pseudo-residuals
\begin{itemize}
% \begin{enumerate}
  \item L2: Init = optimal constant = mean(y);
      for L1 it's median(y)
  % target variable in the case of $L2$ loss and median in the case of $L1$ loss.
  % \item Then calculate pointwise pseudo-resids on training data
  % \item Fit GAM on training features and pseuso-resids 
  % \begin{enumerate}
    \item BLs are cubic $B$-splines with 40 knots.
    % i.e., $\bmmxth = \sum_{k = 1}^{K + \ell} B_k^{(3)}(\xv)$, with $\ell = 3$ 
    % and $K = 40$ knots.
        % and added to the previous
    % model.
    % \item This procedure is repeated multiple times.
  % \end{enumerate}
% \end{enumerate}
\item PRs $L2$: $\tilde{r}(f) = r(f) = y - f(\xv)$
\item PRs $L1$: $\tilde{r}(f) = sign(y - f(\xv))$
    \item Constant learning rate $0.2$ 
\end{itemize}

\begin{columns}[c]
\begin{column}{0.5\textwidth}
\footnotesize
Univariate toy data: 
\vspace{-0.2cm}
\begin{align*}
y^{(i)} &=  -1 + 0.2 \cdot x^{(i)} + 0.1 \cdot sin(x^{(i)}) + \epsilon^{(i)} \\
& n = 50 \text{ ; } \epsilon^{(i)} \sim \mathcal{N}(0, 0.1) \quad 
% \forall i \in \nset
\end{align*}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
  \includegraphics[width = 0.99\textwidth]{figure/illustration_data_normal.png}
\end{figure}
\end{column}
\end{columns}


% \framebreak

% % ------------------------------------------------------------------------------

% \begin{enumerate}
%   \item L2: Init = optimal constant = mean(y);
%       for L1 it's median(y).
%   % target variable in the case of $L2$ loss and median in the case of $L1$ loss.
%   \item Then calculate pointwise pseudo-resids on training data
%   \item Fit GAM on training features and pseuso-resids 
%   % \begin{enumerate}
%     \item BLs are cubic $B$-splines with 40 knots. 
%     % i.e., $\bmmxth = \sum_{k = 1}^{K + \ell} B_k^{(3)}(\xv)$, with $\ell = 3$ 
%     % and $K = 40$ knots.
%     \item Constant learning rate $0.2$ 
%         % and added to the previous
%     % model.
%     % \item This procedure is repeated multiple times.
%   % \end{enumerate}
% \end{enumerate}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{GAM with $L2$ VS $L1$ loss}

Top: $L2$ loss, bottom: $L1$ loss

\vfill

% \begin{center}
\only<1>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_1.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_1.png}

\vfill

Iteration 1

}

\only<2>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_2.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_2.png}

\vfill

Iteration 2
}
\only<3>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_3.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_3.png}

\vfill

Iteration 3
}
\only<4>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_10.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_10.png}

\vfill

Iteration 10
}

\only<5>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_100.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_100.png}

\vfill

Iteration 100
}

\vfill

\footnotesize

Shape of PRs affects gradual model fit: $L1$ only 
sees resids' sign, BLs are not affected size of values
as in $L2$ and hence lead to more moderate changes.%'s approximation of ever smaller residuals.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{GAM with Huber loss}
% Closer to $L2$ for large $\delta$ values and closer to $L1$ for 
% smaller $\delta$ values. 
Top: $\delta$ = 2, bottom: $\delta$ = 0.2.

\vfill

\includegraphics[width=\textwidth]{figure/illustration_gaussian_huber_2_10.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_huber_02_10.png}

\vfill

Iteration 10

%\vfill

\footnotesize
\begin{columns}[c]
\begin{column}{0.75\textwidth}
For small $\delta$, PRs are often
bounded, resulting in $L1$-like behavior, while the upper plot more closely 
resembles $L2$ loss.
\end{column}
\begin{column}{0.24\textwidth}
\includegraphics[width=1.2\textwidth]{figure/fig-loss-huber-delta.png}
\end{column}
\end{columns}
% The lower right plot shows that pseudo-residuals often take values of the upper and lower
% bounds (depending on $\delta$) and thus the behaviour is closer to the $L1$ loss while the upper plots show a
% similar behavior to the $L2$ loss. 

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{GAM with outliers}
Instead of Gaussian noise, let's use $t$-distrib, that leads 
to outliers in $y$.
Top: $L2$, bottom: $L1$.

\vfill

\only<1>{ 
\includegraphics[width=\textwidth]{figure/illustration_tdist_L2_10.png}
\includegraphics[width=\textwidth]{figure/illustration_tdist_L1_10.png}

\vfill
Iteration 10
}
\only<2>{ 
\includegraphics[width=\textwidth]{figure/illustration_tdist_L2_100.png}
\includegraphics[width=\textwidth]{figure/illustration_tdist_L1_100.png}

\vfill
Iteration 100
}

\vfill

\footnotesize

$L2$ loss is affected by outliers rather strongly, whereas $L1$ solely considers 
residuals' sign and not their magnitude, resulting in a more robust model.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{LM with $L2$ vs $L1$ loss}
% Instead of using a GAM as base learner we now use a simple linear model.
% It follows that this is equivalent to applying gradient descent on linear 
% regression.
Top: $L2$, bottom: $L1$.

\vfill

\only<1>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_1.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_1.png}

\vfill
Iteration 1
}

% \only<2>{ 
% \includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_2.png}
% \includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_2.png}
% 
% \vfill
% Iteration 2
% }
% \only<3>{
% \includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_3.png}
% \includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_3.png}
% 
% \vfill
% Iteration 3
% }

\only<2>{
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_10.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_10.png}

\vfill
Iteration 10
}

\only<3>{ 
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L2_lin_100.png}
\includegraphics[width=\textwidth]{figure/illustration_gaussian_L1_lin_100.png}

\vfill
Iteration 100
}

\vfill

\footnotesize
$L2$: as $\tilde r(f) = r(f)$, BL of 1st iter already optimal; 
but learn rate slows us down.
% $L1$: Less se learner LMs fit pseudo-residuals that differ from 
% model residuals, leading to a less monotonic optimization path.

\end{frame}
% 
% \textbf{Iteration 1:} Step 1 to 3
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration2.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 2:} Repeat Step 2 and 3
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration3.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 3:} Repeat Step 2 and 3
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration4.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 5:} Repeat Step 2 and 3
% \vspace{0.2cm}
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration5.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 10:} Repeat Step 2 and 3
% \vspace{0.2cm}
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration6.png}
% \end{figure}
% 
% 
% \framebreak
% \textbf{Iteration 100:} Repeat Step 2 and 3
% \vspace{0.2cm}
% \begin{figure}
%   \includegraphics[width=0.7\textwidth]{figure_man/illustration7.png}
% \end{figure}
% 
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{LM: Grad Boost vs Grad Desc Deep Dive}
% 
% \footnotesize
% 
% \begin{itemize}
%   \item GB with LMs and $L2$ is basically equivalent to fit LM via GD. 
%   % the LR $\alpha$.
%   % \item Equivalent to fitting LM via GD. 
%   \item Here, a grad step in param space equals a grad step in function space.
%   \item GD update for LMs:
%   $\thetab^{[m+1]} \leftarrow \thetam - \alpha \cdot \nabla_{\thetam} 
%   \riske(\thetam) =  
%   \thetam + \alpha (-\Xmat^T \yv + \Xmat^T\Xmat \thetam)$ \\
%   \item Now let's fit LM against PRs in GB\\
%   NB: adding a linear BL to an LM simply sums params
%   \begin{eqnarray*}
%     \footnotesize
%     \frac{\partial}{\partial \thetab^{[m+1]}} 
%     \left \| (\yv - \Xmat \thetam) - \Xmat \thetab^{[m+1]} \right \|^2_2 
%     &=& 0 \\
%     -2\Xmat^T (\yv - \Xmat \thetam) + 2\Xmat^T\Xmat 
%     \thetab^{[m+1]} &=& 0 \\
%     \thetab^{[m+1]} &=& (\Xmat^T\Xmat)^{-1} \Xmat^T 
%     (\yv - \Xmat \thetam) \\
%     \thetab^{[m+1]} &=& (\Xmat^T\Xmat)^{-1} \Xmat^T \yv
%     - (\Xmat^T\Xmat)^{-1} \Xmat^T \Xmat \thetam \\
%     \thetab^{[m+1]} &=& (\Xmat^T\Xmat)^{-1} \Xmat^T \yv
%     - \thetam % \quad \quad \textcolor{gray}{\rvert \cdot (-\Xmat^T\Xmat)} 
%     \\
%     \thetab^{[m+1]} &=& - \Xmat^T \yv + \Xmat^T\Xmat 
%     \thetam
%   \end{eqnarray*}
%   $\Rightarrow \fh^{[m+1]} = \Xmat \tilde \thetab^{[m+1]} = 
%   \Xmat \left( \thetam + \alpha (-\Xmat^T \yv + 
%   \Xmat^T\Xmat \thetam) \right)$
% \end{itemize}
% \end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}

