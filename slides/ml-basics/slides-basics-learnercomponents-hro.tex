\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: basics-supervised, basics-datatask, basics-learners, basics-riskminimization

\lecturechapter{Introduction: Components of a Learner}
\lecture{Introduction to Machine Learning}

\begin{frame}{Components of a Learner}

Nearly all supervised learning algorithms can be described
in terms of three components:

\lz
\begin{center}
\textbf{Learning = Hypothesis Space + Risk + Optimization}
\end{center}
\lz

\begin{itemize}
\item
  \textbf{Hypothesis Space:} Defines (and restricts!) what kind of model $f$ can be learned from the data.\\
\item
  \textbf{Risk:} A metric that quantifies how well a specific model performs on a
  given data set. This defines how to compare observed values to predictions and allows us to rank candidate models in order to choose the best one.\\
\item
  \textbf{Optimization:} Defines how to search for the best model in the \textbf{hypothesis space}, typically guided by the metric used for the \textbf{risk}.\\
\end{itemize}

\end{frame}

\begin{frame}{Hypothesis Space + Risk + Optimization}
\lz
By decomposing learners into these components
\begin{itemize}
\item we have a framework to understand how they work \\
\item we can more easily evaluate in which settings they may be more or less suitable\\
\item we can tailor learners to specific problems by clever choice of each of the three components\\
\end{itemize}
\end{frame}

\begin{frame}{Supervised Learning, Formalized:}

A \textbf{learner} 
\begin{itemize}
\item receives a \textbf{training set} $\D \in \Xspace \times \Yspace$
\item and uses an \textbf{optimization} procedure to find
$$
\fh = \argmin_{f \in \Hspace} \riskef.
$$
\item for a given \textbf{hypothesis class} $\Hspace$ of \textbf{models} $f:\Xspace \rightarrow \R^g$
\item based on a \textbf{risk} function $\riskef$ that quantifies the performance of $f \in \Hspace$ on $\D$.
\end{itemize}
\lz\lz\lz
{\footnotesize (This does not cover all special cases, but it's a useful framework for most supervised ML problems.)}
\end{frame}


\begin{frame}[squeeze]{Components of a Learner}
\vskip -.5em
\begin{footnotesize}

$\textbf{Hypothesis Space} :\begin{cases} 
\text{Step functions} \\
\text{Linear functions}\\
\text{Sets of rules}\\
\text{Neural networks}\\
\text{Voronoi tesselations}\\
\text{...}
\end{cases}$

$\phantom{Hypothesis Space RISK RISK} \textbf{Risk} :\begin{cases}
\text{Mean squared error}\\
\text{Misclassification rate}\\
\text{Negative log-likelihood}\\
\text{Information gain}\\
\text{...}
\end{cases}$

$\phantom{hypothesis space risk RISK RISK} \textbf{Optimization} :\begin{cases}
\text{Analytic solution}\\
\text{Gradient descent}\\
\text{Combinatorial optimization}\\
\text{Genetic algorithms}\\
\text{...}
\end{cases}$

\end{footnotesize}
\end{frame}
\endlecture
\end{document}
