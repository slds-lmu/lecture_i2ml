\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  ML-Basics
  }{% Lecture title  
  In a Nutshell
}{% Relative path to title page image: Can be empty but must not start with slides/
figure/nutshell-ml-basics-hypothesisspace-classif.png % figure_man/nutshell-titlefigure.png
%Stock free image from pexels.com
}{% Learning goals, wrapped inside itemize environment
  \item Understand fundamental goal of supervised machine learning
  \item Know concepts of task, model, parameter, learner, loss function, and empirical risk minimization
}

\sloppy


\begin{vbframe}{What is ML?}
%\begin{itemize}
% \item \small Mathematically well-defined and solves reasonably narrow tasks.
 %   \item \small ML algorithms usually construct predictive/decision models from data, instead of explicitly programming them.
    %\item
    ``A computer program is said to learn from experience E with respect to
  some task T and some performance measure P, if its performance on T, as
  measured by P, improves with experience E.''\\
  \begin{footnotesize}
  \emph{Tom Mitchell, Carnegie Mellon University, 1998}\\
  \end{footnotesize}
%\end{itemize}

\vspace{1ex}

%\begin{center}
$\Rightarrow$ 99 $\%$ of this lecture is about \textbf{supervised learning}:
%\end{center} \hspace{0.1cm}

\begin{columns}
\begin{column}{0.1\textwidth}
\begin{center}
Training
\end{center}
\end{column}
\begin{column}{0.9\textwidth}
\begin{center}
%https://docs.google.com/presentation/d/1r-MzVTtq-s1gZW33fZebP0Yj86NXVK3_/edit?usp=drive_web&ouid=111297583342413087383&rtpof=true
  \includegraphics[width = 0.8\textwidth]{figure_man/nutshell-ml-basics-catdog-learning.png}
\end{center}
\end{column}
\end{columns}
%\end{center}
%\begin{center}
\begin{columns}
\begin{column}{0.1\textwidth}
\begin{center}
Prediction
\end{center}
\end{column}
\begin{column}{0.9\textwidth}
\begin{center}
%https://docs.google.com/presentation/d/1XMLqhHX27kNNGDY37looSx7nnXxSRyMX/edit#slide=id.p1
%  \includegraphics[width = 0.8\textwidth]{figure_man/nutshell-ml-basics-catdog-prediction.png} 
  \includegraphics[width = 0.8\textwidth]{figure_man/nutshell-ml-basics-catdog-prediction.png}
\end{center}
\end{column}
\end{columns}
% Stock free images taken from pexels.com
%https://www.pexels.com/de-de/lizenz/

\end{vbframe}

% \begin{vbframe}{Data}
% \begin{itemize}
% \item Two important assumptions regarding the data set:
%     \begin{itemize}

%       \item We assume the observed data $\D$ to be generated by a random process. The true distribution is \textbf{unknown} to us. Learning (part of) its structure is what ML is all about.

%     \item We assume data to be drawn \emph{i.i.d.} (\textbf{i}ndependent and \textbf{i}dentically
%     \textbf{d}istributed) from the joint probability density function (pdf) / probability mass function (pmf) $\pdfxy$.
%    \end{itemize}

% \item We distinguish two basic forms our data may come in:

%   \begin{itemize}

%       \item For \textbf{labeled} data we have already observed the target (the output variable).

%     \item For \textbf{unlabeled} data the target labels are unknown.
%    \end{itemize}

% \item Sometimes you need to encode the data before running an algorithm:

%   \begin{itemize}
%     \item  \small Most learning algorithms can only deal with numerical features,
%       although there are some exceptions (e.g., decision trees can use integers and categoricals without problems).
%       For other feature types, we usually have to pick or create an
%       appropriate \textbf{encoding}, for example: One-hot encoding.
%   % \item \textbf{Encoding} means transforming categorical features to numeric
%   % quantities.
%      \item \small \textbf{One-hot Encoding for categorical features}: For example consider the categorical feature "Fruit" with $k$ different categories (for $k$ = 3 the categories might be "Banana", "Apple" and "Lemon"). For encoding we represent the feature "Fruit" as a vector with $k = 3$ entries. Each entry corresponds to one category. For encoding "Banana" only the corresponding entry will be set to $1$, all other entries will be set to $0$. E.g. "Banana" = $(1,0,0)$.

%   %one to $\tilde k$ columns in the
%   %design matrix, each element being a \textbf{binary dummy variable} assuming
%   %values from \setzo.
%   %\item $x$ is thus represented by

%     \item \small \textbf{Dummy Encoding for categorical features}: Consider above example. For Dummy encoding one category is selected as reference category which is not explicitly encoded. For encoding we represent the feature "Fruit"  as a vector with $k - 1 = 2$ entries. E.g. for encoding "Banana" (in the case of "Banana" being the reference category) we have $(0,0)$ (otherwise $(0,1)$).

%   %-- note that equidistant values signify a linear ordering here.)
% \end{itemize}

% \end{itemize}

% \end{vbframe}

\begin{vbframe}{Tasks}
\begin{itemize}
    \item Supervised tasks are labeled data situations where the goal is to learn the functional
        relationship between inputs (features) and output (target)

    \item We distinguish between \textbf{regression} and \textbf{classification} tasks, depending on whether the
        target is \textbf{numerical} or \textbf{categorical}
\end{itemize}

\lz

\begin{columns}
\begin{column}{0.4\textwidth}
\small \textbf{Regression}: Target is \textbf{numerical}, e.g., predict days a patient has to stay in hospital

  \begin{center}
    \includegraphics[width=\textwidth]{figure/nutshell-ml-basics-supervised-regression-task.png}
  \end{center}
\end{column}

\begin{column}{0.4\textwidth}
\small \textbf{Classification}: Target is \textbf{categorical}, e.g., predict one of two risk categories for a life insurance customer

  \begin{center}
    \includegraphics[width=\textwidth]{figure/nutshell-ml-basics-supervised-classification-task.png}
  \end{center}
\end{column}
\end{columns}

\end{vbframe}


\begin{vbframe}{Models and Parameters}
\small
\begin{itemize}
    \item A model is a function that maps features to predicted targets
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/the_model_web.png}
\end{center}

\small
\begin{itemize}
\item  For finding the model that describes the relation between features and target best, one needs to restrict the set of all possible functions
\item This restricted set of functions is called \textbf{hypothesis space}. E.g., one could consider only simple linear functions as hypothesis space
\item Functions are fully determined by parameters. E.g., in the case of linear functions, $y = \theta_0 + \theta_1 x $, the parameters $\theta_0$ (intercept) and $\theta_1$ (slope) determine the relationship between $y$ and $x$
\item Finding the optimal model means finding the optimal set of parameters
\end{itemize}

\end{vbframe}

\begin{vbframe}{Learner}

\begin{itemize}

\item Learns automatically the relation between features and target -- given a set of training data
\item Learner picks the best element of the \textbf{hypothesis space}, i.e., the function that fits the training data best
\end{itemize} \hspace{0.4cm}

\begin{columns}
\begin{column}{0.4\textwidth}
\textbf{Regression}:

  \begin{center}
    \includegraphics[width=\textwidth]{figure/nutshell-ml-basics-hypothesisspace-regr.png}
  \end{center}
\end{column}

\begin{column}{0.4\textwidth}
\textbf{Classification}:

  \begin{center}
    \includegraphics[width=\textwidth]{figure/nutshell-ml-basics-hypothesisspace-classif.png}
  \end{center}
\end{column}
\end{columns}

\newpage

\begin{itemize}
\item Learner uses labeled training data to learn a model $f$. This model is applied to new data for predicting the target variable
\end{itemize}

 \begin{center}
    \includegraphics[width = 0.75\textwidth]{figure_man/the_inducer_web.png}
  \end{center}

\end{vbframe}

\begin{vbframe}{Loss and Risk Minimization}

\begin{itemize}
\item Loss: Measured pointwise for each observation, e.g., $L_2$-loss
\begin{center}
$\Lxy = (y - \fx)^2$
\end{center}
\item Risk: Measured for entire model. Sums up pointwise losses.
\begin{center}
$ \riskef = \sumin \Lxyi $
\end{center}
\hspace{1cm}
\end{itemize}
\begin{columns}
\begin{column}{0.5\textwidth}
\small Squared \textbf{loss} of one \textbf{observation}.
  \begin{center}
    \includegraphics[width=0.9\textwidth]{figure/nutshell-ml-basics-loss-regression.png}
  \end{center}
\end{column}
\begin{column}{0.5\textwidth}
\small Empirical \textbf{risk} of entire \textbf{model}

  \begin{center}
    \includegraphics[width=0.9\textwidth]{figure/nutshell-ml-basics-empirical-risk-regression.png}
  \end{center}
\end{column}
\end{columns}
\end{vbframe}
\begin{vbframe}{Empirical Risk Minimization}

\begin{itemize}
\item \small The risk surface visualizes the empirical risk for all possible parameter values of the parameter vector $\thetav$
\item \small Minimizing the empirical risk is usually done by numerical optimization
\end{itemize}

\begin{center}
$
\thetavh = \argmin_{\thetav \in \Theta} \risket.
$
\end{center}


\begin{center}
 \includegraphics[width = 0.9\textwidth]{figure_man/nutshell-ml-basics-risk-surface.png}
\end{center}


%\begin{itemize}
%item \small In general $\thetah$ does not necessarily correspond with the global minimum of the risk surface.
%\end{itemize}

%\small
%\begin{itemize}
%\item It depends on the model and parameter structure which numerical technique should be applied for emprical risk minimization.
%\small
%\item One can imagine the empirical risk as an error surface similar to a mountain landscape, where one starts nearby the top and wants to find a way to the valley (minimum of risk).
%\small
%\item One of the simplest algorithms for reaching the valley is \textbf{Gradient Descent (GD)}:
%    \begin{itemize}
%    \small
%    \item In the case of GD one starts at a point on the surface (can be randomly selected or e.g. by domain knowledge). The algorithm then finds the direction of the surface with the steepest descent and moves towards this direction with a certain step size (learning rate).
%    \small
%    \item Moving with the \textbf{right} step size is crucial for the success of GD. If it`s too big the valley might not be reached because of overstepping, if it`s too small the algorithm might be too slow.
%    \end{itemize}

%\end{itemize}

\end{vbframe}

%\begin{vbframe}{Components of a Learner}

%\begin{itemize}
%\item A learner is basically designed by three components:
%\begin{center}
%  \textbf{Learning = Hypothesis Space + Risk + Optimization}
%\end{center}
%\item It can be extended by a \textbf{regularization} term which controls the complexity of a model.
%\item Note that exceptions exist, where the ML method is not defined via \textbf{risk minimization} and defining the \textbf{hypothesis space} can be very hard or impossible.
%\end{itemize}


%\end{vbframe}


% ------------------------------------------------------------------------------
\endlecture
\end{document}
