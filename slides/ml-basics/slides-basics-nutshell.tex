\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
ML-Basics
}{
In a Nutshell
}{
figure_man/nutshell-ml-basics-risk-surface.png
}{
\item Understand fundamental goal of supervised machine learning
\item Know concepts of data, task, model, parameter, learner, loss function, and empirical risk minimization
}


\begin{framev}{What is ML?}
``A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.''\\
\begin{footnotesize}
\emph{Tom Mitchell, Carnegie Mellon University, 1998}\\
\end{footnotesize}
\vfill
$\Rightarrow$ 99 $\%$ of this lecture is about \textbf{supervised learning}:
\vfill
\splitVCC[0.1]{
Training
}{
\imageC[0.9]{figure_man/nutshell-ml-basics-catdog-learning.png} %https://docs.google.com/presentation/d/1r-MzVTtq-s1gZW33fZebP0Yj86NXVK3_/edit?usp=drive_web&ouid=111297583342413087383&rtpof=true
}
\splitVCC[0.1]{
Prediction
}{
\imageC[0.9]{figure_man/nutshell-ml-basics-catdog-prediction.png} %https://docs.google.com/presentation/d/1XMLqhHX27kNNGDY37looSx7nnXxSRyMX/edit#slide=id.p1
}
\end{framev}


\begin{framev}{Data in SL}
Measurements on different aspects of $n$ objects:
\spacer
\begin{itemize}
  \item \textbf{Target}: output variable / goal of prediction
  \item \textbf{Features}: properties that describe an object 
\end{itemize}
\vfill
\splitV[0.4]{
\image[1]{figure_man/feat_targ_rel.jpg}%https://docs.google.com/presentation/d/1hoUHBJmXzCyVQXT9uP4sI84N6leQ42YhigFvQiCY0oo/edit?usp=sharing
}{
\image[1]{figure_man/ml-basic-data-example-iris.png} %https://docs.google.com/presentation/d/1hnRPmCke8EqBhTrymC_pnuPe17Dk5iMkdBz6AQuIa3A/edit?usp=sharing page 1
}
\vfill
$$
\D = \Dset \in \defAllDatasetsn
$$
\end{framev}


\begin{framei}{Tasks}
\item Supervised tasks are labeled data situations with the goal of learning the functional relationship between features and target
\item We distinguish between \textbf{regression} and \textbf{classification} tasks, depending on whether the target is \textbf{numerical} or \textbf{categorical}
\vfill
\splitVThreeCustom[0.47]{0.06}{0.47}{
\footnotesize \textbf{Regression}: Target is \textbf{numerical}, e.g., predict days a patient has to stay in hospital
\imageC[0.9]{figure/nutshell-ml-basics-supervised-regression-task.png}
}{
$\;$
}{
\footnotesize \textbf{Classification}: Target is \textbf{categorical}, e.g., predict one of two risk categories for a life insurance customer
\imageC[0.9]{figure/nutshell-ml-basics-supervised-classification-task.png}
}
\end{framei}


\begin{framei}[fs=small]{Models and Parameters}
\item Maps features to predicted targets
$$
f_{\thetav} : \Xspace \rightarrow \R^g
$$ 
\imageC[0.7]{figure_man/the_model_web.png}
\item  We consider a restricted set of parametrized functions of a certain form, the \textbf{hypothesis space}, e.g., simple linear functions
\item Models are fully determined by parameters. E.g., in the case of linear functions, $y = \theta_0 + \theta_1 x $, the parameters $\theta_0$ (intercept) and $\theta_1$ (slope) determine the relationship between $y$ and $x$
\item Finding the optimal model means finding the optimal set of parameters
\end{framei}


\begin{framei}{Learner}
\item Learns automatically the relation between features and target -- given a set of training data
\item Learner picks the best element of the \textbf{hypothesis space}, i.e., the function that fits the training data best:
$$
\ind: \preimageInducerShort \rightarrow \Hspace
$$
\vfill
\splitV[0.5]{
\center\textbf{Regression}
\imageL[0.9]{figure/nutshell-ml-basics-hypothesisspace-regr.png}
}{
\center\textbf{Classification}
\imageR[0.9]{figure/nutshell-ml-basics-hypothesisspace-classif.png}
}
\end{framei}


\begin{framei}{Learner}
\item Learner uses labeled training data to learn a model $f$. This model is applied to new unlabeled data for predicting the target variable
\imageC[0.75]{figure_man/the_inducer_web.png}
\end{framei}


\begin{framei}{Loss and Risk Minimization}
\item Loss: Measured pointwise for each observation, e.g., $L_2$-loss
$$
\Lxy = (y - \fx)^2
$$
\item Risk: Measured for entire model. Sums up pointwise losses
$$
\riskef = \sumin \Lxyi
$$
\vfill
\splitV[0.5]{
\center \small Squared \textbf{loss} of one \textbf{observation}
\imageC[0.9]{figure/nutshell-ml-basics-loss-regression.png}
}{
\center \small Empirical \textbf{risk} of entire \textbf{model}
\imageC[0.9]{figure/nutshell-ml-basics-empirical-risk-regression.png}
}
\end{framei}


\begin{framei}[fs=small]{Empirical Risk Minimization}
\item The risk surface visualizes the empirical risk for all possible parameter values of the parameter vector $\thetav$
\item Minimizing the empirical risk is often done by numerical optimization
$$
\thetavh = \argmin_{\thetav \in \Theta} \risket
$$
\image[1]{figure_man/nutshell-ml-basics-risk-surface.png}
\end{framei}

\endlecture
\end{document}
