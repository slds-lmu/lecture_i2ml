\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
ML-Basics
}{
Data
}{
figure/sample-dgp-2d.pdf
}{
\item Understand structure of tabular data in ML 
\item Understand difference between target and features 
\item Understand difference between labeled and unlabeled data 
\item Know concept of data-generating process
}


\begin{framei}{Iris data set}
\item Introduced by statistician Fisher, frequently used toy example
\item Classify iris subspecies based on flower measurements
\item 150 iris flowers: 50 versicolor, 50 virginica, 50 setosa
\item Sepal length / width and petal length / width in [cm]
\vfill
\imageC[0.9][https://rpubs.com/vidhividhi/irisdataeda]{figure_man/iris_species.png}
\item Word of warning: ``iris'' is a small, clean, low-dim data set, very easy to classify; this is not necessarily true in the wild 
\end{framei}


\begin{framei}{Data in SL}
\item Measurements on different aspects of $n$ objects:
\begin{itemizeL}
\item \textbf{Target}: output variable / goal of prediction
\item \textbf{Features}: properties that describe the object
\end{itemizeL}
\item We assume some relationship between features and target, and want to discover that predictive rule.
\vfill
\splitV[0.4]
{
\image[1]{figure_man/feat_targ_rel.jpg} %SOURCE: https://docs.google.com/presentation/d/1hoUHBJmXzCyVQXT9uP4sI84N6leQ42YhigFvQiCY0oo/edit?usp=sharing
}{
\imageL[1]{figure_man/ml-basic-data-example-iris.png} %SOURCE: https://docs.google.com/presentation/d/1hnRPmCke8EqBhTrymC_pnuPe17Dk5iMkdBz6AQuIa3A/edit?usp=sharing
}
\end{framei}


\begin{framei}{Data types}
\item Features and target may be of different data types 
\begin{itemize}
\item \textbf{Numerical} variables: $\in \R$
\item \textbf{Integer} variables:  $\in \Z$
\item \textbf{Categorical} variables: $\in \{C_1,...,C_g\}$
\item \textbf{Binary} variables:   $\in \{0, 1\}$
\end{itemize}
\item For the \textbf{target} variable, this results in different tasks of supervised learning: \textit{regression} and \textit{classification}
\item Most learning algorithms can only deal with numerical features, although there are some exceptions (e.g., trees can use categoricals without problems).
For other types we usually have to pick an appropriate \textbf{encoding} to cast them to numerical values.
\item If not stated otherwise we assume numerical features
\end{framei}


\begin{framei}[fs=small]{Encoding for categoricals}
\item We expand the representation of a variable $x$ with $k$ mutually exclusive categories from a scalar to a length-$\tilde k$ vector with at most one element being 1, and 0 otherwise: $\bm{o}(x)_j =\I(x = j) \in \{0,1\}$
\item Each entry of $\bm{o}(x)\in \{0,1\}^{\tilde k}$ is treated as a separate feature
\item Two popular ways to do this are
\begin{itemize}\footnotesize
\item \textbf{One-hot encoding}: $\tilde k = k$ dummies, so \textit{exactly one} element is 1 (\enquote{hot}).
E.g., $x \in \{ a, b, c\} \mapsto \bm{o}(x) = (x_a, x_b, x_c)$, with $x_a = x_b = 0, x_c = 1$ and $\bm{o}(x) = (0, 0, 1)$ for $x = c$.
\item \textbf{Dummy encoding}: $\tilde k = k - 1$ dummies, so \textit{at most one} element is 1, removing the redundancy of one-hot encoding (necessary for learners that require non-singular input matrices, such as in linear regression). \\
E.g., $x \in \{ a, b, c\} \mapsto \bm{o}(x) = (x_a, x_b)$ for reference category $c$, with $x_a = x_b = 0$ and $\bm{o}(x) = (0, 0)$ for $x = c$.
\end{itemize}
\item We also often use this encoding for categorical targets
\item For features with a natural \textbf{order} in their categories we resort to encodings that reflect this ordinality, e.g., a sequence of integer values
\end{framei}


\begin{framei}{Observation Labels}
\item We call the entries of the target column \textbf{labels}
\item We distinguish two types of data:
\begin{itemize}
 \item \textbf{Labeled} data: target is observed
\item \textbf{Unlabeled} data:  target is unknown
\end{itemize}
\vfill
\imageL[0.9]{figure_man/ml-basic-data-example-iris-with-new.png} %SOURCE: https://docs.google.com/presentation/d/1hnRPmCke8EqBhTrymC_pnuPe17Dk5iMkdBz6AQuIa3A/edit?usp=sharing page 2
\end{framei}


\begin{framev}{Notation for data}
$$
\D = \Dset \in \defAllDatasetsn
$$
Where
\begin{itemize}
\item $\Xspace$: input space, usually $\Xspace \subset \R^p$
\item $\Yspace$: output / target space
\item \(\xyi\) $\in \Xspace\times \Yspace$:  \(i\)-th observation
\item $\xj = \xjvec$: j-th feature vector
\end{itemize}
We also use
\begin{itemize}
\item  $\defAllDatasetsn$ is the set of all data sets of size $n$, as $\allDatasetsn$
\item $\defAllDatasets$ is the set of all finite data sets, as $\allDatasets$
\end{itemize}
\vfill
$\Rightarrow$ We observe $n$ objects, described by $p$ features
\end{framev}


\begin{framei}[sep=L]{Data-Generating Process}
\item We assume the observed data $\D$ to be generated by a process that can be characterized by some probability distribution $$\Pxy,$$ defined on $\Xspace \times \Yspace$
\item We denote the random variables following this distribution by lowercase $\xv$ and $y$
\item We usually assume the true distribution to be \textbf{unknown}.
Learning (part of) its structure to predict $y$ is what ML is all about
\end{framei}


\begin{framei}{Data-Generating Process}
\item We assume data to be drawn \emph{i.i.d.} (\textbf{i}ndependent and \textbf{i}dentically \textbf{d}istributed) from $\Pxy$
\item This means: We assume that all samples are drawn from the same distribution and are mutually independent -- the $i$-th realization does not depend on the other $n-1$ ones
\item This is a strong assumption, not always realistic.
More complex scenarios (e.g., time series) exist, but we focus on the i.i.d. scenario here
\vfill
\splitV[0.5]{
\imageC[0.6]{figure/sample-dgp-3d.png}
}{
\imageL[0.9]{figure/sample-dgp-2d.pdf}
}
\end{framei}


\begin{framev}{Data-Generating Process}
\textbf{Remarks:}
\begin{itemize}
\item With a slight abuse of notation we write random variables, e.g., $\xv$ and $y$, in lowercase, as normal variables or function arguments. 
The context will make clear what is meant.
\item Often, distributions are characterized by a parameter vector $\thetav \in \Theta$. We then write $\pdfxyt$.
\item This lecture mostly takes a frequentist perspective. Distribution parameters $\thetav$ appear behind the | for improved legibility, not to imply that we condition on them in a probabilistic Bayesian sense.
So, strictly speaking, $p(\xv | \thetav)$ should usually be understood to mean $p_{\thetav}(\xv)$ or $p(\xv, \thetav)$ or $p(\xv; \thetav)$.
On the other hand, this notation makes it very easy to switch to a Bayesian view.
\end{itemize}
\end{framev}

\endlecture
\end{document}
