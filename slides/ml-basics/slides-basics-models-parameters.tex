\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}
\titlemeta{
ML-Basics
}{
Models \& Parameters
}{
figure_man/network.PNG
}{
\item Understand that an ML model is simply a parametrized function
\item Understand that the hypothesis space lists all admissible models
\item Understand relationship between hypothesis and parameter space
}


\begin{framei}{What is a Model?}
\item A \textbf{model} (or \textbf{hypothesis}) 
$$
f : \Xspace \rightarrow \R^g
$$ 
is a function that maps feature vectors to predicted target values
\item In regression: $g = 1$; in classification, $g$ is the number of classes, and output vectors are scores or class probabilities
\imageL[0.8]{figure_man/the_model_web} %https://docs.google.com/presentation/d/1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo  /edit?usp=sharing Page 1
\end{framei}


\begin{framei}{What is a Model?}
\item $f$ is meant to capture intrinsic patterns of the data, the underlying assumption being that these hold true for \emph{all} data drawn from $\Pxy$
\item Models can range from super simple (e.g., linear, tree stumps) to very complex (e.g., DL) with lots of choices
\vfill
\splitV[0.4]{
\vfill
\image[1]{figure_man/monotone_trafo1.png}
}{
\image[1]{figure_man/network.PNG} %https://docs.google.com/presentation/d/13X6v1M7u3xkMUDCNNgtiFje2nAz8v5bW_bQhrQrx5To/edit#slide=id.p
}
\item ML requires \textbf{constraining} $f$ to a certain type of functions
\end{framei}


\begin{framei}[sep=L]{Hypothesis Spaces}
\item Without restrictions on the functional family, the task of finding a \enquote{good} model among all available models is impossible to solve
\item We have to determine the class of our model \emph{a priori}, thereby narrowing down the search space. We call this a \textbf{structural prior}.
\item The set of functions defining a specific model class is called a 
\textbf{hypothesis space} $\Hspace$:
$$\Hspace = \{f: f \text{ belongs to a certain functional family}\}$$
\end{framei}


\begin{framei}{Parametrization}  
\item All models within a hypothesis space share a common functional structure typically constructed as \textbf{parametrized family of functions}
\item We collect all parameters in a \textbf{parameter vector} $\thetav = (\theta_1, \theta_2, \ldots, \theta_d)$ from \textbf{parameter space} $\Theta$
\item They are our means of fixing a specific function from the family: once set our model is fully determined
\item Therefore, we can re-write $\Hspace$ as:
\begin{equation*}
\begin{split}
\Hspace = \{f_{\thetav}: \quad & f_{\thetav} \text{ belongs to a certain functional family } \\
& \text{parameterized by } \thetav\}
\end{split}
\end{equation*}
\end{framei}


\begin{framei}{Parametrization}
\item Finding optimal model $=$ finding optimal parameters
\item This allows us to operationalize our search for the best model as a search for the optimal value on a $d$-dimensional parameter surface
\imageC[0.4]{figure_man/bijection_f_theta.PNG} % https://docs.google.com/presentation/d/1WoFzKsfmvVdkHJO0aEIN6U-DW-mglanIcPoW3d_Ecu0/edit#slide=id.p
\item $\thetav$ might be scalar or very high-dimensional with thousands of parameters depending on the complexity of our model
\end{framei}


\begin{framei}[sep=L]{Parametrization}
\item Some parameter vectors, for some model classes, encode the same function: the parameter-to-model mapping could be non-injective
\item We call this a non-identifiable model
\item This shall not concern us here
\imageC[0.6]{figure_man/bijection_f_theta_2.PNG} % https://docs.google.com/presentation/d/1KYNhUr5SLH8mmLcgARAE4VRPFwYRytclpaSTLuKMuW0/edit#slide=id.p
\end{framei}


\begin{framev}{Example: Univariate linear functions}
$$
\Hspace = \{f: \fx = \textcolor{orange}{\theta_0} + \textcolor{blue}{\theta_1} x, \thetav \in \R^2\}
$$
\vfill
\image[1]{figure/hs-lin-functions.pdf}
\end{framev}


\begin{framev}{Example: Bivariate quadratic functions}
$$
\Hspace = \{f: \fx = \textcolor{orange}{\theta_0} + \textcolor{blue}{\theta_1} x_1 + \textcolor{blue}{\theta_2} x_2 + \textcolor{red}{\theta_3} x_1^2 + \textcolor{red}{\theta_4} x_2^2 + \textcolor{red}{\theta_5} x_1 x_2, \thetav \in \R^6\}
$$
\vfill
\splitVThree{
\tiny
\begin{center}
\begin{equation*}
\begin{split}
f(x) &= \textcolor{orange}{3} + \textcolor{blue}{2} x_1 + \textcolor{blue}{4} x_2 \\
\textcolor{white}{x} 
\end{split}
\end{equation*}
\vspace*{-1.5\baselineskip}
\includegraphics[trim = 100 10 100 50, clip, width = \textwidth]{figure/hs-quadric-1.pdf}
\end{center}
}{
\tiny
\begin{center}
\begin{equation*}
\begin{split}
f(x) &= \textcolor{orange}{3} + \textcolor{blue}{2} x_1 + \textcolor{blue}{4} x_2 + \\
&+ \textcolor{red}{1} x_1^2 + \textcolor{red}{1} x_2^2
\end{split}
\end{equation*}
\vspace*{-1.5\baselineskip}
\includegraphics[trim = 100 10 100 50, clip, width = \textwidth]{figure/hs-quadric-2.pdf}
\end{center}
}{
\tiny
\begin{center}
\begin{equation*}
\begin{split}
f(x) &= \textcolor{orange}{3} + \textcolor{blue}{2} x_1 + \textcolor{blue}{4} x_2 + \\
&+ \textcolor{red}{1} x_1^2 + \textcolor{red}{1} x_2^2 + \textcolor{red}{4} x_1 x_2
\end{split}
\end{equation*}
\vspace*{-1.5\baselineskip}
\includegraphics[trim = 100 10 100 50, clip, width = \textwidth]{figure/hs-quadric-3.pdf}
\end{center}
}
\end{framev}


\begin{framev}{Example: RBF Network}
Radial basis function networks with Gaussian basis functions
$$
\Hspace = \bigg\{ f: \fx =  \sumik \textcolor{blue}{a_i} \rho ( \| \xv - \textcolor{orange}{\textbf{c}_i}  \|) \bigg\}
$$ 
\splitV[0.55]{
\begin{small}
where 
\begin{itemize}
\item $\textcolor{blue}{a_i}$ is the weight of the $i$-th neuron 
\item $\textcolor{orange}{\textbf{c}_i}$ its center vector and
\item $\rho( \| \xv - \textcolor{orange}{\textbf{c}_i} \|) = \exp(- \beta \| \xv - \textcolor{orange}{\textbf{c}_i} \|^2)$ is the $i$-th radial basis function with bandwidth $\beta \in \R$
\end{itemize}
Usually number of centers $k$ and bandwidth $\beta$ need to be set in advance (so-called \emph{hyperparameters})
\end{small}
}{
\image[1][https://www.researchgate.net/figure/Structure-of-Stacked-Autoencoders_fig2_325025951]{figure_man/rbf_network.png}
}
\end{framev}


\begin{framev}{Example: RBF Network}
\image[1]{figure/hs-rbf-network-2d.pdf}
\vspace{-1em}
\tiny
\splitVThree{
\begin{center}
$a_1 = 0.4, a_2 = 0.2, a_3 = 0.4$
$c_1 = -3, c_2 = -2, c_3 = 1$
\includegraphics[trim = 50 30 50 50, clip, width = \textwidth]{figure/hs-rbf-network-3d-1.pdf}
$a_1 = 0.4, a_2 = 0.2, a_3 = 0.4$ \\
$c_1 = (2,-2), c_2 = (0,0)$, \\
$c_3 = (-3,2)$
\end{center}
}{
\begin{center}
$a_1 = 0.4, a_2 = 0.2, a_3 = 0.4$
$c_1 = -3, c_2 = -2, \textcolor{orange}{c_3 = 4}$
\includegraphics[trim = 50 30 50 50, clip, width = \textwidth]{figure/hs-rbf-network-3d-2.pdf}
$a_1 = 0.4, a_2 = 0.2, a_3 = 0.4$ \\
$c_1 = (2,-2), \textcolor{orange}{c_2 = (3,3)},$ \\
$c_3 = (-3,2)$
\end{center}
}{
\begin{center}
$\textcolor{blue}{a_1 = 0.6}, a_2 = 0.2, \textcolor{blue}{a_3 = 0.2}$
$c_1 = -3, c_2 = -2, c_3 = 1$
\includegraphics[trim = 50 30 50 50, clip, width = \textwidth]{figure/hs-rbf-network-3d-3.pdf}
$\textcolor{blue}{a_1 = 0.2, a_2 = 0.45, a_3 = 0.35}$
$c_1 = (2,-2), c_2 = (0,0),$ \\
$c_3 = (-3,2)$
\end{center}
}
\end{framev}

\endlecture
\end{document}
