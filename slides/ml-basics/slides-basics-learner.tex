\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
ML-Basics
}{
Learner
}{
figure_man/the_inducer_web.png
}{
\item Know formal definition of learner
\item Understand that a learner receives training data and outputs the best model from $\Hspace$
}


\begin{framei}{Supervised Learning Example}
\item Imagine we want to investigate how working conditions affect productivity of employees
\item It is a \textbf{regression} task since the target \emph{productivity} is continuous
\item We collect data about worked minutes per week (\emph{productivity}), how many people work in the same office as the employee in question, and the employee's salary
\vfill
\imageL[0.8]{figure_man/data_table} % FIGURE SOURCE: https://docs.google.com/presentation/d/1qIWHJq-iZqfUsLLJD81Z9LhobGTIN3sDHTevnm5dxZ0/edit?usp=sharing
\end{framei}


\begin{framev}{Supervised Learning Example}
How could we construct a model from these data?\\
\spacer
We could investigate the data manually and come up with a simple, hand-crafted rule such as:
\spacer
\begin{itemize}
\item The baseline productivity of an employee with salary 3000 and 7 people in the office is 1850 minutes
\item A decrease of 1 person in the office increases productivity by 30
\item An increase of the salary by 100 increases productivity by 10
\end{itemize}
\vfill
$\Rightarrow$ Obviously, this is neither feasible nor leads to a good model.
\end{framev}


\begin{framei}[sep=L]{Idea of Supervised Learning}
\item \textbf{Goal:} Identify the functional relationship that maps features to target
\item \textbf{Supervised} learning means we use \emph{labeled} data to learn model $f$
\item Later, we use model $f$ to predict $y$ for new \emph{unlabeled} data
\imageC[0.8]{figure_man/what_is_a_model_web} % FIGURE SOURCE: https://docs.google.com/presentation/d/1WLPubv9vxLL-JIlHAtsvTBBG5pbF4xgRGW_prkOAEnE/edit?usp=sharing Page 4 
\end{framei}


\begin{framei}{Learner Definition}
\item Algorithm for finding  $f$ is called \textbf{learner} / \textbf{learning algorithm} /  \textbf{inducer}
\item The learner is our means of picking the best element from the hypothesis space $\Hspace$ for given training data
\item Formally it maps training data $\D \in \allDatasets$ (plus a vector of \textbf{hyperparameter} control settings $\lamv \in \Lam$) to a model:
\splitV[0.5]{
\imageC[0.8]{figure_man/the_inducer_web.png} %FIGURE SOURCE: https://docs.google.com/presentation/d/1bc6EQSsHEuVnyqFGX9E8oNfwOjAwVglRaIllxnOjLBo/edit#slide=id.p
}
{
\vfill
$\ind: \preimageInducerShort \rightarrow \Hspace$
}
\item Practically, we often construct a mapping 
$\ind: \preimageInducerShort \rightarrow \Theta$
\end{framei}


\begin{framev}{Learner Definition}
In pseudo-code:
\begin{itemize}
\item Learner gets a hypothesis space of parametrized functions $\Hspace$
\item User passes data set $\Dtrain$ and control settings $\lamv$
\item Learner sets parameters such that model fits data best
\item Optimal parameters $\thetavh$ or function $\fh$ is returned for later usage
\end{itemize}
\imageC[0.6]{figure_man/the_inducer_web.png}
\end{framev}

\endlecture
\end{document}
