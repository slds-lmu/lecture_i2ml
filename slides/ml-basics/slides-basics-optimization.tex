\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
ML-Basics
}{
Optimization
}{
figure/err_surf
}{
\item Understand how the risk function is optimized to learn the optimal parameters of a model
\item Understand the idea of gradient descent as a basic risk optimizer
}


\begin{framei}{Learning as Parameter Optimization}
\item Operationalize search for model $f$ that matches training data best by looking for parametrization $\thetav \in \Theta$ with lowest risk $\risket$
\item Traverse error surface downwards; often local search from some start point to minimum (hopefully)
\vfill
\begin{center}
\includegraphics[trim=2.4cm 2.4cm 2.4cm 2.4cm, width=0.4\textwidth]{figure/err_surf}
\end{center}
\end{framei}


\begin{framev}{Learning as Parameter Optimization}
ERM optimization problem:
$$
\thetavh = \argmin_{\thetav \in \Theta} \risket 
$$
For \textbf{(global) minimum} $\thetavh$:
$$
\forall \thetav \in \Theta :\quad \riske(\thetavh) \leq \risket 
$$
Does not imply that $\thetavh$ is unique \\
\vfill
\begin{itemizeM}
\item Best numerical optimizer depends on problem structure
\item Continuous params? Uni-modal $\risket$? 
\item Numerical optimization not our focus here, now
\end{itemizeM}
\end{framev}


\begin{framei}{Local Minima}
\item Definition of \textbf{local minimum} $\thetavh$:
$$
\exists \epsilon > 0\; \forall \thetav \text{ with } \left\Vert\thetavh - \thetav\right\Vert < \epsilon: \quad \riske(\thetavh) \leq \risket 
$$
\item Clearly every global minimum is also a local minimum
\item Finding local minimum is easier than global one
\vfill
\imageC[0.7]{figure/g_l_min}
\end{framei}


\begin{framev}[small]{Local Minima and stationary points}
If $\riske$ continuously differentiable, \textbf{sufficient condition} for local minimum: \\
$\thetavh$ is \textbf{stationary}, so 0 gradient, so no local improvement possible:
$$
\frac{d \riske}{d\thetav}(\thetavh) = 0
$$
\textbf{and} Hessian at $\thetavh$ is positive definite. \\
Neg. gradient points into direction of fastest local decrease;\\
Hessian measures local curvature.
\vfill
\begin{figure}[!htb]
\splitVThree{
\image[0.9]{figure/grad}
\caption*{\footnotesize$\frac{d\riske}{d\thetav}(\thetav)$}
}{
\includegraphics[trim=2.2cm 2.2cm 2.2cm 2.2cm, width=0.9\linewidth]{figure/hess1}
\caption*{\footnotesize const. pos. def. Hessian}
}{
\includegraphics[trim=2.2cm 2.2cm 2.2cm 2.2cm, width=0.9\linewidth]{figure/hess2}
\caption*{\footnotesize const. neg. def. Hessian}
}
\end{figure}
\end{framev}


\begin{framei}{Gradient Descent}
\item Iteratively improve  current candidate $\thetav^{[t]}$ 
\item Move in direction of  neg. gradient, so direction of steepest descent
\item Use step size / learning rate $\alpha$ 
$$
\thetav^{[t+1]} = \thetav^{[t]} - \alpha \frac{d \riske}{d\thetav}{(\thetav^{[t]})}
$$
\splitV[0.45]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc1}
}{
Random start $\thetav^{[0]}$ with\\
$\riske(\thetav^{[0]}) = 76.25.$
}
\end{framei}


\begin{framev}{Gradient Descent - Example}
\splitVCC[0.4]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc2}  
}{
Direction of the neg. gradient at $\thetav^{[0]}$
}
\vfill
\splitVCC[0.4]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc3}
}{
Arrive at $\thetav^{[1]}$ with $\riske(\thetav^{[1]}) \approx 42.73$\\
We improved:
$\riske(\thetav^{[1]}) < \riske(\thetav^{[0]})$
}
\end{framev}


\begin{framev}{Gradient Descent - Example}
\splitVCC[0.4]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc4}
}{
$\quad$Now iterate, do the same at $\thetav^{[1]}$
}
\vfill
\splitVCC[0.4]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc5}
}{
$\quad$Now $\thetav^{[2]}$ has risk $\riske(\thetav^{[2]}) \approx 25.08$
}
\end{framev}


\begin{framev}{Gradient Descent - Example}
\splitVCC[0.4]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc6}
}{
We iterate this until some form of convergence or termination
}
\vfill
\splitVCC[0.4]{
\includegraphics[trim=2cm 2cm 2cm 2cm, width=0.9\linewidth]{figure/grad_desc7}
}{
We arrive close to a stationary $\thetavh$ which is hopefully at least a local minimum
}
\end{framev}


\begin{framei}[fs=footnotesize]{Gradient Descent - Learning Rate}
\item Neg. gradient is direction that looks locally promising to reduce $\riske$
\item Hence weights components higher in which $\riske$ decreases more
\item However, the length of $-\frac{d}{d\thetav}{\riske}$ measures only the local decrease rate, i.e., there are no guarantees that we will not go ``too far''
\item We use a learning rate $\alpha$ to scale the step length in each iteration. Too much can lead to overstepping and no convergence, too low leads to slow convergence.
\item Usually, a simple constant rate or rate-decrease mechanisms to enforce local convergence are used
\begin{figure}[!htb]
\splitVThree{
\image[1]{figure/grad_desc_alpha1}
\caption*{\tiny good convergence for $\alpha_1$}
}{
\image[1]{figure/grad_desc_alpha2}
\caption*{\tiny slow convergence for $\alpha_2 (< \alpha_1$)}
}{
\image[1]{figure/grad_desc_alpha3}
\caption*{\tiny no convergence for $\alpha_3 (> \alpha_1$)}
}
\end{figure}
\end{framei}

\begin{framei}{Further topics}
\item Few models, e.g. linear regression, can be optimized analytically. 
\item GD is a so-called first-order method. Second-order methods (like Newton-Raphson) use the Hessian to refine the search direction for faster convergence.
\item There exist many improvements of GD (momentum, ADAM), e.g., to smartly control the learn rate, to escape saddle points, to mimic second order behavior without computing the Hessian.
\item If the gradient is not computed on the complete data, but instead on small, random batches, this is \textbf{stochastic gradient descent} (SGD). For large-scale problems, this is usually more efficient.
\end{framei}

\endlecture

\end{document}
