\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  ML-Basics
  }{% Lecture title  
  Optimization
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/err_surf
}{% Learning goals, wrapped inside itemize environment
\item Understand how the risk function is optimized to
  learn the optimal parameters of a model
\item Understand the idea of gradient descent as a basic risk optimizer
}


\begin{vbframe}{Learning as Parameter Optimization}
\begin{itemize}
\item We have seen, we can operationalize the search for a model $f$ that matches
    training data best, by looking for its parametrization $\thetav \in \Theta$
    with lowest empirical risk $\risket$.
\item Therefore, we usually traverse the error surface downwards; often by local search from a starting point to its minimum.
\end{itemize}
\begin{center}
\begin{figure}[!b]
\includegraphics[trim=2.4cm 2.4cm 2.4cm 2.4cm, width=0.4\textwidth]{figure/err_surf}
\end{figure}
\end{center}

\framebreak

The ERM optimization problem is:
% this means that we find the best model $\hat f$ parametrized by parameters $\thetavh \in \Theta$ regarding an empirical risk $\riske$ by \textbf{minimizing} $\risket$ with respect to $\thetav$, i.e., 

\[
\thetavh  = \argmin_{\thetav \in \Theta} \risket .
\]

For a \textbf{(global) minimum} $\thetavh$ it obviously holds that 
\[
\forall \thetav \in \Theta :\quad \riske(\thetavh) \leq \risket .
\]

This does not imply that $\thetavh$ is unique. \\
\lz
Which kind of numerical technique is reasonable for this problem strongly depends 
on model and parameter structure (continuous params? uni-modal $\risket$?). 
Here, we will only discuss very simple scenarios.

\end{vbframe}
\begin{vbframe}{Local Minima}

If $\riske$ is continuous in $\thetav$ we can define a \textbf{local minimum} $\thetavh$:
\[
\exists \epsilon > 0\; \forall \thetav \text{ with } \left\Vert\thetavh - \thetav\right\Vert < \epsilon: \quad \riske(\thetavh) \leq \risket .
\]
Clearly every global minimum is also a local minimum.
Finding a local minimum is easier than finding a global minimum.

\begin{center}
\begin{figure}[!b]
\includegraphics[width=0.7\textwidth]{figure/g_l_min}
\end{figure}
\end{center}

\end{vbframe}
\begin{vbframe}{Local Minima and stationary points}
\footnotesize
If $\riske$ is continuously differentiable in $\thetav$ then a \textbf{sufficient condition} for a local minimum is that $\thetavh$ is \textbf{stationary} with 0 gradient, so no local improvement is possible:
\[
\frac{\partial}{\partial\thetav}\riske(\thetavh) = 0
\]
and the Hessian $\frac{\partial^2}{\partial\thetav^2}\riske(\thetavh)$ is positive definite. While the neg. gradient points into the direction of fastest local decrease, the Hessian measures local curvature of $\riske$.

\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figure/grad}
  \caption*{\footnotesize$\frac{\partial}{\partial\thetav}\riske(\thetav)$}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[trim=2.2cm 2.2cm 2.2cm 2.2cm, width=\linewidth]{figure/hess1}
  \caption*{\footnotesize const. pos. def. Hessian}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[trim=2.2cm 2.2cm 2.2cm 2.2cm, width=\linewidth]{figure/hess2}
  \caption*{\footnotesize const. neg. def. Hessian}
\endminipage
\end{figure}

\end{vbframe}
\begin{vbframe}{Least Squares Estimator}
Now, for given features $\Xmat \in \mathbb{R}^{n\times p}$ and target $\yv \in \mathbb{R}^n$, we want to find the best linear model regarding the squared error loss, i.e.,
\[\risket = \left\Vert \Xmat\thetav - \yv\right\Vert_2^2 = \sumin (\thetav^\top\xv^{(i)} - \yi)^2\;.\]

With the sufficient condition for continously differentiable functions it can be shown that the \textbf{least squares estimator}
\[\thetavh = (\Xmat^\top\Xmat)^{-1}\Xmat^\top\yv.\]
is a local minimum of $\riske$. If $\Xmat$ is full-rank, $\riske$ is strictly convex and there is only one local minimum - which is also global. \\

\lz \textbf{Note:} Often such analytical solutions in ML are not possible, and we rather have to use iterative numerical optimization.

\end{vbframe}
\begin{vbframe}{Gradient Descent}
    The simple idea of GD is to iteratively go from the current candidate $\thetav^{[t]}$ in the direction of the negative gradient, i.e., the direction of the steepest descent, with learning rate $\alpha$ to the next $\thetav^{[t+1]}$:
\[
    \thetav^{[t+1]} = \thetav^{[t]} - \alpha \frac{\partial}{\partial\thetav}{\riske(\thetav^{[t]})}.
\]

\begin{tabular}{l}

\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc1}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
We choose a random start $\thetav^{[0]}$ with risk $\riske(\thetav^{[0]}) = 76.25.$
\endminipage\hfill
\end{tabular}

\end{vbframe}

\begin{vbframe}{Gradient Descent - Example}
\begin{tabular}{l}
\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc2}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
Now we follow in the direction of the negative gradient at $\thetav^{[0]}$.
\endminipage\hfill
\\
\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc3}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
We arrive at $\thetav^{[1]}$ with risk\\$\riske(\thetav^{[1]}) \approx 42.73.$ \\ We improved:\\$\riske(\thetav^{[1]}) < \riske(\thetav^{[0]}).$
\endminipage\hfill
\end{tabular}
\end{vbframe}
\begin{vbframe}{Gradient Descent - Example}
\begin{tabular}{l}
\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc4}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
Again we follow in the direction of the negative gradient, but now at $\thetav^{[1]}.$
\endminipage\hfill
\\
\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc5}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
Now $\thetav^{[2]}$ has risk $\riske(\thetav^{[2]}) \approx 25.08.$
% Thus clearly, we have improved our estimation again.
\endminipage\hfill
\end{tabular}
\end{vbframe}
\begin{vbframe}{Gradient Descent - Example}
\begin{tabular}{l}
\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc6}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
We iterate this until some form of convergence or termination.
% \footnote[frame]{This is not the only stopping criterium; e.g., we could limit the number of iterations.}. 
\endminipage\hfill
\\
\minipage{0.32\textwidth}
  \includegraphics[trim=2cm 2cm 2cm 2cm, width=\linewidth]{figure/grad_desc7}  
\endminipage\hfill
\minipage{0.1\textwidth}
$\;$
\endminipage\hfill
\minipage{0.54\textwidth}
\vspace{0pt}% 
We arrive close to a stationary $\thetavh$ which is hopefully at least
a local minimum.
\endminipage\hfill
\end{tabular}
\end{vbframe}

\begin{vbframe}{Gradient Descent - Learning Rate}
\footnotesize
\begin{itemize}
\item The negative gradient is a direction that looks locally promising to reduce $\riske$. \\
\item Hence it weights components higher
    % \footnote[frame]{in an absolute sense} 
    in which $\riske$ decreases more. \\
\item However, the length of $-\frac{\partial}{\partial\thetav}{\riske}$ measures only the local decrease rate, i.e., there are no guarantees that we will not go "too far". \\
\item We use a learning rate $\alpha$ to scale the step length in each iteration. Too much can lead to overstepping and no converge, too low leads to slow convergence.
\item Usually, a simple constant rate or rate-decrease mechanisms to enforce local convergence are used
    % control the rate. 
\end{itemize}
\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figure/grad_desc_alpha1}
  \caption*{\tiny good convergence for $\alpha_1$}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figure/grad_desc_alpha2}
  \caption*{\tiny poor convergence for $\alpha_2 (< \alpha_1$)}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{figure/grad_desc_alpha3}
  \caption*{\tiny no convergence for $\alpha_3 (> \alpha_1$)}
\endminipage
\end{figure}

\end{vbframe}

\begin{vbframe}{Further topics}
\begin{itemize}
\item GD is a so-called first-order method. Second-order methods use the Hessian to refine the search direction for faster convergence.
\item There exist many improvements of GD, e.g., to smartly control the learn rate, to escape saddle points, to mimic second order behavior without computing the expensive Hessian.
    % ould also optimize the learning rate $\alpha$.
% \item If some parameters are dependent of each other, we can model such dependicies either by \textbf{equality} or \textbf{inequality constraints}, i.e., 
% \[g(\thetav) = 0 \quad \text{or} \quad h(\thetav) \geq 0.\]
% There exist numerical methods to solve constrained optimization problems, such as sequential quadratic programming.
\item If the gradient of GD is not derived from the empirical risk of the whole data set, but instead from a randomly selected subset, we call this \textbf{stochastic gradient descent} (SGD). For large-scale problems this can lead to higher computational efficiency.
% \item Often it is desirable to not allow arbitrarily large $\Vert \thetavh \Vert$, since this could result, among other things, in numerical instability of the method. This procedure is called \textbf{regularization}.
\end{itemize}
\end{vbframe}


% \begin{vbframe}{Further topics}
    % learning vs optimuzation,.,.,.
% \end{vbframe}


\endlecture

\end{document}
