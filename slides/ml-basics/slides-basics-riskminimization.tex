\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
  ML-Basics
  }{
  Losses \& Risk Minimization
}{
  figure/ml-basic-riskmin-error-surface.png
}{
\item Know concept of loss function
\item Understand concept of theoretical and empirical risk 
\item Understand relationship between risk minimization and finding best model
}


\begin{vbframe}{How to Evaluate Models}

\begin{itemize}
\item Training a learner = optimize over hypothesis space
\item Find function that matches training data best
\item We compare point-wise predicted outputs to 
observed labels
\end{itemize}

\lz

% % FIGURE SOURCE: https://docs.google.com/presentation/d/1dTc5act2POjELGuD8wFIUbPxG0QRZlg1PaYdHGU-FJM/edit?usp=sharing
% \begin{center}\includegraphics[width=0.8\textwidth]{figure_man/eval_inducer1_web} \end{center}

%https://docs.google.com/presentation/d/1l3VlmPYZs_ycbAIVet05lBcCGEbQeUCl3CAG56BllvM/edit?usp=sharing page 1


\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/ml-basics-riskmin-eval.png} \end{center}

  
\end{vbframe}


\begin{vbframe}{Loss}

\textbf{Loss function} $\Lxy$ quantifies point-wise how we measure errors in predictions for a single $\xv$:
    $$
    L: \Yspace \times \R^g \to \R.
    $$
Regression: Could use absolute L1 loss  $\Lxy = |y-\fx|$


% \vfill


%ml-basic-riskmin-1-loss.R first plot
% \begin{center}
% \begin{figure}[!b]

\begin{center}
\includegraphics[width=0.5\textwidth]{figure/ml-basic_riskmin-1-loss_abs.png}
\end{center}
or L2-loss $\Lxy = (y - \fx)^2$
\begin{center}
\includegraphics[width=0.5\textwidth]{figure/ml-basic_riskmin-1-loss_sqrd.png}
\end{center}
% \end{figure}
% \end{center}


%ml-basic-riskmin-1-loss.R first plot
% \begin{center}
% \begin{figure}[!b]
% \includegraphics[width=0.5\textwidth]{figure/ml-basic_riskmin-1-loss_sqrd.png}
% \end{figure}
% \end{center}



\end{vbframe}


\begin{vbframe}{Risk of a Model}

\begin{itemize}
  \item Theoretical \textbf{risk} of a candidate model $f$ is the \textbf{expected loss}
  $$ \riskf := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy $$
  \item Average error we incur when we use $f$ on data from $\Pxy$
  \item Goal in ML: Find a hypothesis $f \in \Hspace$ that \textbf{minimizes} this
\end{itemize}

\vfill

Problems: 
%Minimizing the theoretical risk $\riskf$ directly is not feasible:

\begin{itemize}
\item $\Pxy$ is unknown
\item Could estimate $\Pxy$ non-parametrically, e.g., by kernel density estimation,  doesn't scale to higher dimensions
\item Could efficiently estimate $\Pxy$, if we place assumptions on its form, e.g. cf. discriminant analysis
\end{itemize}

\lz


\end{vbframe}




\begin{vbframe}{Empirical Risk}

\begin{itemize}
\item Have $n$ i.i.d. data from $\Pxy$, approximate expected risk empirically % by computing it on $\D$.

\item Just sum up all losses over training data

\end{itemize}

$$ \riskef = \sumin \Lxyi $$

\begin{itemize}

\item Associates one quality score with each $f \in \Hspace$
\item Encodes: How well does $f$ fits training data
\item Now we get very close to solve this by optimization 

\end{itemize}

$$ \riske:  \Hspace \to \R $$  
\begin{center}
\begin{figure}[!b]
\includegraphics[width=0.8\textwidth]{figure/ml-basic_riskmin-2-risk.png}
\end{figure}
\end{center}

\framebreak 
   
\begin{itemize}
\item Can also define as average loss
  $$
    \riskeb(f) = \frac{1}{n}\sumin \Lxyi
  $$
  \item Constant factor $\frac{1}{n}$ doesn't make a difference in optimization
  \item We usually use $\riske(f)$ 
\item Since $f$ is usually defined by \textbf{parameters} $\thetav$, this becomes:
$$\riske : \R^d \to \R$$
\begin{eqnarray*}
\risket & = & \sumin \Lxyit \cr
% \thetavh & = & \argmin_{\thetav \in \Theta} \risket
\end{eqnarray*}
 

\end{itemize}
   
   
\end{vbframe}




\begin{vbframe}{Empirical risk minimization}

\begin{itemize}
    \item Best model = smallest risk
    \item For finite $\Hspace$: we could tabulate exhaustively
\end{itemize}

\lz

% calculated in ml-basic-riskmin-error-surface.R (theta_risk_df)
\begin{center}
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
\end{center}


\end{vbframe}

\begin{vbframe}{Empirical Risk Minimization}

\begin{itemize}
    \item But usually $\Hspace$ is infinitely large
    \item Instead: Simply consider risk surface w.r.t. the parameters $\thetav$ 
    %\item So: just look at $\risket$ as a function of $\thetav$ 
\end{itemize}

\splitV[0.45]{
\begin{table}
\vspace{2em}
$$\riske : \R^d \to \R$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
}
\end{table}
}
{
\includegraphics[width=\textwidth]{figure/ml-basic-riskmin-error-surface.png}
}

\framebreak

Minimizing this surface is called \textbf{empirical risk minimization} (ERM)
$$
\thetavh = \argmin_{\thetav \in \Theta} \risket
$$
Usually we do this by numerical optimization

\splitV[0.5]{
\begin{table}
$$\riske : \R^d \to \R$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\ 
\rowcolor{lightgray}
\(\displaystyle f_5 \)   & 1.25 & 0.90 & 23.40 \\ 
\end{tabular}
}
\end{table}
}
{
\vspace{2em}
\includegraphics[trim=0 100 0 100,clip,width=\textwidth]{figure/ml-basic-riskmin-error-surface-best.png}
}

\vfill

Kind of: 
Reduced ``learning'' to 
\textbf{numerical parameter optimization}\\
(Later we will learn that this is only part of the complete picture!)

\end{vbframe}

\endlecture

\end{document}
