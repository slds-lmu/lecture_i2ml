\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  ML-Basics
  }{% Lecture title  
  Losses \& Risk Minimization
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/ml-basic-riskmin-error-surface.png
}{% Learning goals, wrapped inside itemize environment
\item Know the concept of loss 
\item Understand the relationship between loss and risk 
\item Understand the relationship between risk minimization and finding the best model
}


\begin{vbframe}{How to Evaluate Models}

\begin{itemize}
\item When training a learner, we optimize over our hypothesis space,
    to find the function which matches our training data best.
\item This means, we are looking for a function, where the 
    predicted output per training point is as close as possible to
    the observed label.
\end{itemize}


% % FIGURE SOURCE: https://docs.google.com/presentation/d/1dTc5act2POjELGuD8wFIUbPxG0QRZlg1PaYdHGU-FJM/edit?usp=sharing
% \begin{center}\includegraphics[width=0.8\textwidth]{figure_man/eval_inducer1_web} \end{center}

%https://docs.google.com/presentation/d/1l3VlmPYZs_ycbAIVet05lBcCGEbQeUCl3CAG56BllvM/edit?usp=sharing page 1


\begin{center}\includegraphics[width=0.8\textwidth]{figure_man/ml-basics-riskmin-eval.png} \end{center}
% \vspace{-0.5cm}
\begin{itemize}
    \item To make this precise, we need to define now how we measure the difference
        between a prediction and a ground truth label pointwise.
%     \begin{itemize}
%       \item Absolute error $|2588 - 2220| = 368$
%       \item Squared error: $(2588 - 2220)^2 = 135,424$\\
%     \end{itemize}
%     \item The choice of this metric has a major influence on the final model, as it determines what a \emph{good} model is. 
%     \item It will determine the ranking of the different models $f \in \Hspace$.
%     \item The metric we use is called the \textbf{loss function}. 
  \end{itemize}
  
\end{vbframe}


\begin{vbframe}{Loss}

The \textbf{loss function} $\Lxy$ quantifies the "quality" of the prediction $\fx$ of a single observation $\xv$:
    $$
    L: \Yspace \times \R^g \to \R.
    $$
In regression, we could use the absolute loss $\Lxy = |\fx - y|$; 


% \vfill


%ml-basic-riskmin-1-loss.R first plot
% \begin{center}
% \begin{figure}[!b]

\begin{center}
\includegraphics[width=0.5\textwidth]{figure/ml-basic_riskmin-1-loss_abs.png}
\end{center}
or the L2-loss $\Lxy = (y - \fx)^2$:
\begin{center}
\includegraphics[width=0.5\textwidth]{figure/ml-basic_riskmin-1-loss_sqrd.png}
\end{center}
% \end{figure}
% \end{center}


%ml-basic-riskmin-1-loss.R first plot
% \begin{center}
% \begin{figure}[!b]
% \includegraphics[width=0.5\textwidth]{figure/ml-basic_riskmin-1-loss_sqrd.png}
% \end{figure}
% \end{center}



\end{vbframe}


\begin{vbframe}{Risk of a Model}

\begin{itemize}
  \item The (theoretical) \textbf{risk} associated with a certain hypothesis $\fx$ measured by a loss function $\Lxy$ is the \textbf{expected loss}
  $$ \riskf := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy. $$
  \item This is the average error we incur when we use $f$ on data from $\Pxy$.
  \item Goal in ML: Find a hypothesis $\fx \in \Hspace$ that \textbf{minimizes} risk.
\end{itemize}

\framebreak


\textbf{Problem}: Minimizing $\riskf$ over $f$ is not feasible:

\begin{itemize}
\item $\Pxy$ is unknown (otherwise we could use it to construct optimal predictions).
\item We could estimate $\Pxy$ in non-parametric fashion from the data $\D$, e.g., by kernel density estimation, but this really does not scale to higher dimensions (see \enquote{curse of dimensionality}).
\item We can efficiently estimate $\Pxy$, if we place rigorous assumptions on its distributional form, and methods like discriminant analysis work exactly this way. 
\end{itemize}

\lz

But as we have $n$ i.i.d. data points from $\Pxy$ available we can simply
approximate the expected risk by computing it on $\D$.

\end{vbframe}




\begin{vbframe}{Empirical Risk}

To evaluate, how well a given function $f$ matches our training data,
we now simply sum-up all $f$'s pointwise losses.
  
$$ \riskef = \sumin \Lxyi $$

This gives rise to the \textbf{empirical risk function} which allows us
to associate one quality score with each of our models,
which encodes how well our model fits our training data.
$$ \riske:  \Hspace \to \R $$  
% The ability of a model $f$ to reproduce the association between $\xv$ and $y$ that is present in the data $\D$ can be measured by the \textbf{summed loss}, also called \textbf{"empirical risk"}: \\
\begin{center}
\begin{figure}[!b]
\includegraphics[width=0.9\textwidth]{figure/ml-basic_riskmin-2-risk.png}
\end{figure}
\end{center}

\framebreak 
   
\begin{itemize}
\item The risk can also be defined as an average loss
  $$
    \riskeb(f) = \frac{1}{n}\sumin \Lxyi. 
  $$
  The factor $\frac{1}{n}$ does not make a difference in optimization, so we will consider $\riske(f)$ most of the time.  
\item Since $f$ is usually defined by \textbf{parameters} $\thetav$, this becomes:
$$\risk : \R^d \to \R$$
\begin{eqnarray*}
\risket & = & \sumin \Lxyit \cr
% \thetavh & = & \argmin_{\thetav \in \Theta} \risket
\end{eqnarray*}
 

\end{itemize}
   
   
\end{vbframe}




\begin{vbframe}{Empirical risk minimization}

The best model is the model with the smallest risk. 

\lz

If we have a finite number of models $f$, we could simply tabulate them
and select the best.


% calculated in ml-basic-riskmin-error-surface.R (theta_risk_df)
\begin{center}
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
\end{center}


\end{vbframe}

\begin{vbframe}{Empirical Risk Minimization}

But usually $\Hspace$ is infinitely large. 

\lz

Instead we can consider the risk surface w.r.t. the parameters $\thetav$.\\
(By this I simply mean the visualization of $\risket$)

\begin{table}
\begin{minipage}{0.4\linewidth}
$$\risket : \R^d \to \R.$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
}
\end{minipage}\hfill
	\begin{minipage}{0.55\linewidth}
\includegraphics[width=\textwidth]{figure/ml-basic-riskmin-error-surface.png}
\end{minipage}
\end{table}


\framebreak

Minimizing this surface is called \textbf{empirical risk minimization} (ERM).
$$
\thetah = \argmin_{\thetav \in \Theta} \risket.
$$
Usually we do this by numerical optimization.

\begin{table}
\begin{minipage}{0.4\linewidth}
$$\risk : \R^d \to \R.$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
 Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
 \hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\ 
\rowcolor{lightgray}
\(\displaystyle f_5 \)   & 1.25 & 0.90 & 23.40 \\ 
\end{tabular}
}
\end{minipage}\hfill
	\begin{minipage}{0.55\linewidth}
\includegraphics[trim=0 100 0 100,clip,width=\textwidth]{figure/ml-basic-riskmin-error-surface-best.png}
\end{minipage}
\end{table}

In a certain sense, we have now reduced the problem of learning to 
\textbf{numerical parameter optimization}.

\end{vbframe}


% \begin{vbframe}{Further remarks}
% \begin{itemize}
% \item For regression tasks, the loss often only depends on the residual $\Lxy = L(y - \fx) = L(\eps)$.
% \item The choice of loss implies which kinds of errors are important or not -- requires \emph{domain knowledge}!
% \item For learners that correspond to probabilistic models, the loss determines / is equivalent to distributional assumptions.
% \item Since learning can be re-phrased as minimizing the loss, the choice of loss strongly affects the computational difficulty of learning:
% \begin{itemize}
%     \item How smooth is $\risket$ in $\thetav$?
%     \item Is $\risket$ differentiable so that we can use gradient-based methods?
%     \item Does $\risket$ have multiple local minima or saddlepoints over $\Theta$?\\
% \end{itemize}

% \end{itemize}
% \end{vbframe}



\endlecture

\end{document}
