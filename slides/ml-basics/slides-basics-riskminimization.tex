\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
ML-Basics
}{
Losses \& Risk Minimization
}{
figure/ml-basic-riskmin-error-surface.png
}{
\item Know concept of loss function
\item Understand concept of theoretical and empirical risk 
\item Understand relationship between risk minimization and finding best model
}


\begin{framei}{How to Evaluate Models}
\item Training a learner = optimize over hypothesis space
\item Find function that matches training data best
\item We compare point-wise predicted outputs to 
observed labels
\vfill
\image[1]{figure_man/ml-basics-riskmin-eval.png} %https://docs.google.com/presentation/d/1l3VlmPYZs_ycbAIVet05lBcCGEbQeUCl3CAG56BllvM/edit?usp=sharing page 1
\end{framei}


\begin{frame2}{Loss}
\textbf{Loss function} $\Lxy$ quantifies point-wise how we measure errors in predictions for a single $\xv$:
$$
L: \Yspace \times \R^g \to \R.
$$
Regression: Could use absolute L1 loss  $\Lxy = |y-\fx|$
\imageC[0.5]{figure/ml-basic_riskmin-1-loss_abs.png}
or L2-loss $\Lxy = (y - \fx)^2$
\imageC[0.5]{figure/ml-basic_riskmin-1-loss_sqrd.png}
\end{frame2}


\begin{framei}{Risk of a Model}
\item Theoretical \textbf{risk} of a candidate model $f$ is the \textbf{expected loss}
$$ \riskf := \E_{xy} [\Lxy] = \int \Lxy \text{d}\Pxy $$
\item Average error we incur when we use $f$ on data from $\Pxy$
\item Goal in ML: Find a hypothesis $f \in \Hspace$ that \textbf{minimizes} this
\vfill
Problems:
\item $\Pxy$ is unknown
\item Could estimate $\Pxy$ non-parametrically, e.g., by kernel density estimation,  doesn't scale to higher dimensions
\item Could efficiently estimate $\Pxy$, if we place assumptions on its form, e.g. cf. discriminant analysis
\end{framei}


\begin{framei}{Empirical Risk}
\item Have $n$ i.i.d. data from $\Pxy$, approximate expected risk empirically
\item Just sum up all losses over training data
$$
\riskef = \sumin \Lxyi
$$
\item Associates one quality score with each $f \in \Hspace$
\item Encodes: How well does $f$ fits training data
\item Now we get very close to solve this by optimization 
$$
\riske: \Hspace \to \R
$$
\imageL[0.8]{figure/ml-basic_riskmin-2-risk.png}
\end{framei}


\begin{framei}{Empirical Risk}
\item Can also define as average loss
$$
\riskeb(f) = \frac{1}{n}\sumin \Lxyi
$$
\item Constant factor $\frac{1}{n}$ doesn't make a difference in optimization
\item We usually use $\riske(f)$ 
\item Since $f$ is usually defined by \textbf{parameters} $\thetav$, this becomes:
$$
\riske : \R^d \to \R
$$
$$
\risket = \sumin \Lxyit
$$
\end{framei}


\begin{framei}{Empirical risk minimization}
\item Best model = smallest risk
\item For finite $\Hspace$: we could tabulate exhaustively
\vfill
\begin{center}
\begin{tabular}{ c | c | c || c } % calculated in ml-basic-riskmin-error-surface.R (theta_risk_df)
Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
\hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
\end{center}
\end{framei}


\begin{framei}{Empirical risk minimization}
\item But usually $\Hspace$ is infinitely large
\item Instead: Simply consider risk surface w.r.t. the parameters $\thetav$ 
\splitV[0.45]{
\begin{table}
$$\riske : \R^d \to \R$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
\hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\rowcolor{lightgray}
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\  
\end{tabular}
}
\end{table}
}{
\imageL[0.9]{figure/ml-basic-riskmin-error-surface.png}
}
\end{framei}


\begin{frame2}{Emprical risk minimization}
Minimizing this surface is called \textbf{empirical risk minimization} (ERM)
$$
\thetavh = \argmin_{\thetav \in \Theta} \risket
$$
Usually we do this by numerical optimization
\splitV[0.5]{
\begin{table}
$$\riske : \R^d \to \R$$
\scalebox{0.8}{
\begin{tabular}{ c | c | c || c }
Model & \(\displaystyle \thetav_{intercept} \) & \(\displaystyle \thetav_{slope} \) & \(\displaystyle \risket \) \\ 
\hline
\(\displaystyle f_1 \)   & 2 & 3 & 194.62 \\
\(\displaystyle f_2 \)   & 3 & 2 & 127.12 \\  
\(\displaystyle f_3 \)   & 6 & -1 & 95.81 \\
\(\displaystyle f_4 \)   & 1 & 1.5 & 57.96 \\ 
\rowcolor{lightgray}
\(\displaystyle f_5 \)   & 1.25 & 0.90 & 23.40 \\ 
\end{tabular}
}
\end{table}
}{
\vfill
\includegraphics[trim=0 100 0 100,clip,width=\textwidth]{figure/ml-basic-riskmin-error-surface-best.png}
}
\vfill
Kind of: Reduced ``learning'' to \textbf{numerical parameter optimization}\\
(Later we will learn that this is only part of the complete picture!)
\end{frame2}

\endlecture

\end{document}
