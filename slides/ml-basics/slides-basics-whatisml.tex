\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{
ML-Basics
}{
What is Machine Learning?
}{figure_man/whatisml.png
}{
\item Understand basic terminology of and connections between ML, AI, DL and statistics
\item Know the main directions of ML: Supervised, Unsupervised and Reinforcement Learning
}


\begin{framei}{ML is changing our world}
\item Search engines learn your search preferences
\item Recommender systems learn your taste in books, music, movies,...
\item Algorithms do automatic stock trading
\item Tools can accurately translate between many different languages  
\item DeepMind beats humans at Go
\item Physicians are supported by personalized medicine
\item LLMs revolutionize many fields (currently especially coding)
\item Data-driven discoveries are made in physics, biology, genetics, astronomy, chemistry, neurology,...
\item ...
\imageC[0.33]{figure_man/ai_world_end} %SOURCE MISSING
\end{framei}


\begin{framev}{AI, ML and DL}
\imageC[0.8]{figure_man/learning}
\spacer
Many people are confused what these terms actually mean\\
\spacer
And what does all this have to do with statistics?
\end{framev}


\begin{framei}[fs=footnotesize]{Artificial Intelligence}
\splitV[0.7]{
\item General term for very large and rapidly developing field
\item No strict definition, but often used when machines perform tasks that could only be solved by humans or are very difficult and assumed to require ``intelligence''.
\item Started in the 1940s -- when the computer was invented. Turing and von Neumann immediately asked: If we can formalize computation, can we use that to formalize ``thinking''?
\item Includes ML, NLP, computer vision, robotics, planning, search, intelligent agents, ...
\item Sometimes misused as a ``hype'' term for ML or ... basic data analysis
\item Or people refer to the fascinating developments in the area of foundation models
}{
\image[1][https://pixabay.com/vectors/robot-structure-metal-metallic-5550026/]{figure_man/artificial-intelligence.png}
}
\end{framei}


\begin{framei}[fs=footnotesize]{Machine Learning}
\splitV[0.5]{
\image[0.95][https://www.oreilly.com/library/view/java-deep-learning/9781788997454/a8fce962-51dd-4e29-a7f9-9bf4fd245b1d.xhtml]{figure_man/whatisml.png}
}{
\item Mathematically well-defined and solves reasonably narrow tasks
\item Usually construct predictive models from data, instead of explicitly programming them
\item ``A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.'' \\
\emph{Tom Mitchell, Carnegie Mellon University, 1998}
}
\end{framei}


\begin{framev}{Deep Learning}
\splitV[0.7]{
\begin{itemizeS}[small]
\item Subfield of ML which studies neural networks
\item Artificial neural networks are roughly inspired by the human brain, but we treat them as useful mathematical models
\item Studied for decades (start in the 1940/50s). Uses more layers, might use specific neurons, e.g., for images, many computational improvements to train on large data.
\item Can be used on tabular data but typical applications are images, texts or signals
\item Last 15-20 years have produced remarkable results and imitations of human ability where the result looked intelligent 
\end{itemizeS}
``Any sufficiently advanced technology is indistinguishable from magic.''
\begin{footnotesize}
\emph{Arthur C. Clarke's 3rd law}
\end{footnotesize}
}{
\begin{center}
\vfill
\image[0.85][https://pixabay.com/vectors/machine-learning-ai-6913525/]{figure_man/deep-learning.png}
\end{center}
}
\end{framev}


\begin{framei}[fs=footnotesize]{ML vs. Stats}
\splitV[0.72]{
\item Historically developed as different fields, but many methods and concepts are pretty much the same
\item ML: Rather accurate predictions with more complex models
\item Stats: More interpreting relationships and sound inference
\item Now: Both basically work on same problems with same tools
\item Communities are still divided
\item Often different terminology for the same concepts
\item Most parts of ML we could also call:\\Nonparametric statistics plus efficient numerical optimization
\item Personal opinion: Nowadays few practical differences, seeing differences instead of commonalities mainly holds you back
}{
\vfill
\image[1]{figure_man/ml_vs_stats} 
}
\end{framei}


\begin{framei}{Unsupervised learning}
\item Data without labels $y$
\item Search for patterns within the inputs $\xv$
\item \textit{Unsupervised} as there is no ``true'' output we can optimize against
\vfill
\splitV[0.45]{
\imageL[0.9]{figure/unsup.pdf}
}{
\item Dimensionality reduction (PCA, autoencoders ...):\\
compress information in $\mathcal X$
\item Clustering: group similar observations
\item Outlier detection, anomaly detection
\item Association rules
}
\end{framei}


\begin{framei}{Reinforcement learning}
\item General-purpose framework:
At each time step \emph{agent} interacts with \emph{environment}: observes state, receives reward, selects action
\imageC[0.45]{figure_man/state_action_reward_diagram.png} %image from rcourses
\item Goal: Select actions to maximize future reward
\item Reward signals may be sparse, noisy and delayed
\end{framei}


\begin{framei}{What comes next}
\item \textbf{Supervised learning} for regression and classification: predict labels $y$ through features $\xv$ based on training data
\item First we will go through fundamental concepts in supervised ML: 
\begin{itemizeL}
\item What kind of ``data'' do we learn from?
\item What is a ``prediction model''?
\item How can we quantify ``predictive performance''?
\item What is a ``learning algorithm'' 
\item How can we operationalize learning?
\end{itemizeL}
\item We will also introduce first concrete learning algorithms: Linear models, trees and forests
\item More complex stuff comes later
\end{framei}

\endlecture
\end{document}
