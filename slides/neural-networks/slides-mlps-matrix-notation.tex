\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{figure/sinlay_six.png}
\newcommand{\learninggoals}{
  \item Compact representation of neural network equations
  \item Vector notation for neuron layers
  \item Vector and matrix notation of bias and weight parameters
}

\title{Deep Learning}
\date{}

\begin{document}

\lecturechapter{MLP -- Matrix Notation}
\lecture{Deep Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%\frametitle{Lecture outline}
%\lz
%This chapter introduces a compact way for representing feedforward neural networks: the matrix formalism from linear algebra.
%\lz
%\lz
%
%\begin{enumerate}
%\item First, we explore networks with one hidden layer and one output unit.
%\lz
%\item Next, we investigate networks with one hidden layer but multiple output units.
%\lz
%\item Finally, we focus on multi-layer feedforward networks with an arbitrary number of hidden layers and output units.
%\end{enumerate}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%\section{Single Hidden Layer Networks for Regression and Binary Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Single Hidden Layer Networks: Notations}

\begin{itemize}
\item The input $\xv$ is a column vector with dimensions $p \times 1$. 
\item $\Wmat$ is a weight matrix with dimensions $p \times m$, where $m$ is the amount of hidden neurons:
$$\Wmat =
     \begin{pmatrix}
      w_{1,1} & w_{1,2} & \cdots & w_{1,m} \\
      w_{2,1} & w_{2,2} & \cdots & w_{2,m} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      w_{p,1} & w_{p,2} & \cdots & w_{p,m}
     \end{pmatrix}$$
     \end{itemize}
     \framebreak
     \textbf{Hidden layer}:
     \begin{itemize}
\item For example, to obtain $z_1$, we pick the first column of $W$:
    $$\Wmat_1 =
     \begin{pmatrix}
      w_{1,1} \\
      w_{2,1} \\
      \vdots  \\
      w_{p,1}
     \end{pmatrix}$$
and compute 
$$z_1 = \sigma(\Wmat_1^T \xv + b_1)\enspace,$$
 where $b_1$ is the bias of the first hidden neuron and $\sigma: \mathbb{R} \to \mathbb{R}$ is an activation function. 
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s%%%%

\begin{vbframe}{Single Hidden Layer Networks: Notation}
% \textbf{Hidden layer}:
  \begin{itemize}
    \vspace{4mm}
    \item The network has $m$ hidden neurons $z_1, \dots, z_m$ with
    $$ z_j = \sigma(\mathbf{W}_j^T \xv + b_j)$$
    \vspace{-0.5cm}
    \begin{itemize}
    \item $z_{j,in}  = \mathbf{W}_j^T \xv + b_j$
    \vspace{2mm}
    \item $z_{j,out} = \sigma(z_{j,in}) = \sigma(\mathbf{W}_j^T \xv + b_j)$
    \end{itemize}
    \vspace{4mm}
    for $j \in \{1,\ldots,m\}$.
    \vspace{4mm}
    %\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Vectorized notation:
\begin{itemize}
\item $ \hidz_{in} = (z_{1,in}, \dots, z_{m,in})^T = \Wmat^T \xv + \biasb$ \\ (Note: $\Wmat^T \xv$ = $(\xv^T \Wmat)^T$)
\item $ \hidz = \hidz_{out} = \sigma(\hidz_{in}) = \sigma(\Wmat^T \xv + \biasb)$, where the (hidden layer) activation function $\sigma$ is applied element-wise to $\hidz_{in}$.  
\end{itemize}
\framebreak
\item \textbf{Bias term}:         
\begin{itemize}
\item We sometimes omit the bias term by adding a constant feature to the input $\tilde{\xv} = (1, x_1, ..., x_p)$ and by adding the bias term to the weight matrix 
$$\tilde{\Wmat} = (\biasb, \Wmat_1, ..., \Wmat_p).$$ 
\item \textbf{Note}: For simplification purposes, we will not explicitly represent the bias term graphically in the following. However, the above \enquote{trick} makes it straightforward to represent it graphically. 
\end{itemize}
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \textbf{Output layer}:
  \begin{itemize}
    \vspace{4mm}
    \item For regression or binary classification: one output unit $f$ where
      \begin{itemize}
        \item $f_{in} = \wtu^T \hidz + c$ , i.e. a linear combination of derived features plus the bias term $c$ of the output neuron, and
        \vspace{2mm}
        \item $\fx= f_{out} = \tau(f_{in}) = \tau(\wtu^T \hidz + c)$ , where $\tau$ is the output activation function.
      \end{itemize}
    \item For regression $\tau$ is the identity function.
    \item For binary classification, $\tau$ is a sigmoid function.
    \item \textbf{Note}: The purpose of the hidden-layer activation function $\sigma$ is to introduce non-linearities so that the network is able to learn complex functions whereas the purpose of $\tau$ is merely to get the final score to the same range as the target.
  \end{itemize}

\framebreak 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \textbf{%General notation: 
  Multiple inputs:}
  \begin{itemize}
    \item It is possible to feed multiple inputs to a neural network simultaneously.
    \vspace{2mm}
    \item The inputs $\xi$, for $i \in \nset$, are arranged as rows in the \textbf{design matrix} $\Xmat$.
    \begin{itemize}
      \item $\Xmat$ is a ($n \times p$)-matrix.
    \end{itemize}
    \vspace{2mm}
    \item The weighted sum in the hidden layer is now computed as $\Xmat\Wmat + \bm{B}$, where,
      \begin{itemize}
        \item $\Wmat$, as usual, is a ($p \times m$) matrix, and,
        \vspace{2mm}
        \item $\bm{B}$ is a ($n \times m$) matrix containing the bias vector $\biasb$ (duplicated) as the rows of the matrix.
      \end{itemize}
    \vspace{2mm}
    \item The \textit{matrix} of hidden activations $\bm{Z} = \sigma(\Xmat\Wmat + \bm{B})$
    \begin{itemize}
      \item $\bm{Z}$ is a ($n \times m$) matrix.
    \end{itemize}
  \end{itemize}

\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \begin{itemize}
    \vspace{15mm}
    \item The final output of the network, which contains a prediction for each input, is $\tau(\bm{Z}\wtu + \bm{C})$, where
      \begin{itemize}
        \vspace{2mm}
        \item $\wtu$ is the vector of weights of the output neuron, and,
        \vspace{2mm}
        \item $\bm{C}$ is a ($n \times 1$) matrix whose elements are the (scalar) bias $c$ of the output neuron.
      \end{itemize}
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture
\end{document}