\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-nn}

\title{Deep Learning}
\date{}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Neural Networks
  }{% Lecture title  
  Introduction
}{% Relative path to title page image: Can be empty but must not start with slides/
figure/bird4.png
}{% Learning goals, wrapped inside itemize environment
  \item Relationship of DL and ML
  \item Concept of representation or feature learning
  \item Use-cases and data types for DL methods
}

\lecturechapter{Introduction}
\lecture{I2ML}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {What is Deep Learning}
\begin{center}
\includegraphics[width=0.95\textwidth]{figure/learning.pdf}
\end{center}
\vspace{-.5cm}
\begin{itemize}
\item Deep learning is a subfield of ML based on artificial neural networks.
%construct models on large amounts of %(unstructured) data.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Deep Learning and Neural Networks}
\begin{itemize}
%\item Deep learning is a subfield of ML originally based on (deep) artificial neural networks.
\vspace{.3cm}
\item Deep learning itself is not \textit{new}:
\begin{itemize}
\item Neural networks have been around since the 70s.
\item \textit{Deep} neural networks, i.e., networks with multiple hidden layers, are not much younger.
\end{itemize}
\vspace{.3cm}
\item Why everybody is talking about deep learning now:
\begin{enumerate}
\vspace{.1cm}
\item Specialized, powerful hardware allows training of huge neural networks to push the state-of-the-art on difficult problems.
\vspace{.2cm}
\item Large amount of data is available.
\vspace{.2cm}
\item Special network architectures for image/text data.
\vspace{.2cm}
\item Better optimization and regularization strategies.
\end{enumerate}
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Image Classification with Neural Networks}
\begin{aquote}{Y. Bengio}
Machine learning algorithms, inspired by the brain, based on learning multiple levels of representation/abstraction.   
\end{aquote}
\begin{overlayarea}{\textwidth}{\textheight}
\centering
\only<1>{\includegraphics[width=0.95\textwidth]{figure/bird1.pdf}}
\only<1>{\\ \footnotesize{}}
\only<2>{\includegraphics[width=0.95\textwidth]{figure/bird2.pdf}}
\only<2>{\\ \footnotesize{}}
\only<3>{\includegraphics[width=0.95\textwidth]{figure/bird3.pdf}}
\only<3>{\\ \footnotesize{}}
\only<4>{\includegraphics[width=0.95\textwidth]{figure/bird4.pdf}}
\only<4>{\\ \footnotesize{}}
\end{overlayarea}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-cases}
\textbf{Deep learning can be extremely valuable if the data has these properties:}
\vspace{.2cm}
\begin{itemize}
\item It is high dimensional.
\item Each single feature itself is not very informative but only a combination of them might be.
\item There is a large amount of training data.
\end{itemize}
\vspace{.7cm}
\textbf{This implies that for tabular data, deep learning is rarely the correct model choice.}
\vspace{.2cm}
\begin{itemize}
\item Without extensive tuning, models like random forests or gradient boosting will outperform deep learning most of the time.
\item One exception is data with categorical features with many levels.
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Images}
\begin{itemize}
\item \textbf{High Dimensional}: A color image with $255 \times 255$ (3 Colors) pixels already has $195075$ features.
\vspace{.1cm}
\item \textbf{Informative}: A single pixel is not meaningful in itself.
\vspace{.1cm}
\item \textbf{Training Data}: Depending on applications huge amounts of data are available.
\end{itemize}
\vspace{.3cm}
Architecture: \textbf{C}onvolutional \textbf{N}eural \textbf{N}etworks (CNN)
\begin{figure}
\centering
\scalebox{.85}{\includegraphics{figure/three_channels.png}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Images}
\begin{figure}
\centering
\scalebox{.7}{\includegraphics{figure/classification.png}}
\caption{Example for image classification (Krizhevsky, 2009)}
\end{figure}
\textbf{Image classification} tries to predict a single label for each image.
\footnotesize CIFAR-10 is a well-known dataset used for image classification. It consists of $60,000$ $32x32$ color images containing one of $10$ object classes, with $6000$ images per class. 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Images}
\begin{figure}
\centering
\scalebox{0.7}{\includegraphics{figure/maskrcnn.png}}
\caption{Example for object detection (He et al., 2017)}
\end{figure}
\textbf{Object Detection}
\footnotesize Mask R-CNN is a general framework for instance segmentation, that efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Images}
\begin{figure}
\centering
\scalebox{.7}{\includegraphics{figure/segmentation.png}}
\caption{Example for image segmentation (Noh et al., 2015))} 
\end{figure}
\textbf{Image segmentation} partitions the image into (multiple) segments.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Text}
\begin{itemize}
\item \textbf{High Dimensional}: Each word can be a single feature (300000 words in the German language).
\vspace{.1cm}
\item \textbf{Informative}: A single word does not provide much context.
\vspace{.1cm}
\item \textbf{Training Data}: Huge amounts of text data available.
\end{itemize}
\vspace{.3cm}
Architecture: \textbf{R}ecurrent \textbf{N}eural \textbf{N}etworks (RNN)
\begin{figure}
\centering
\scalebox{.70}{\includegraphics{figure/hierarchical_sequence.png}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Text Classification}
\vspace{1cm}
\begin{figure}
\centering
\scalebox{1}{\includegraphics{figure/sentiment_analysis.png}}
\end{figure}

\vspace{.5cm}

\textbf{Sentiment Analysis} is the application of natural language processing to systematically identify the emotional and subjective information in texts.

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Possible use-case: Text}
\begin{figure}
\centering
\scalebox{.7}{\includegraphics{figure/nmt.png}}
\end{figure}
\textbf{Machine Translation} (e.g. google translate) 
Neural machine translation exploits neural networks to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Applications of Deep Learning: Speech}
\begin{figure}
\centering
\scalebox{.7}{\includegraphics{figure/speech_goog.jpg}}
\caption{Example for speech recognition and generation (Google)}
\end{figure}
\textbf{Speech Recognition and Generation} (e.g. google assistant)
Neural network extracts features from audio data for downstream tasks, e.g., to classify emotions in speech.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Krizhevsky, 2009)]{1}
Krizhevsky, A. (2009). \textit{Learning Multiple Layers of Features from Tiny Images.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(He et al., 2017)]{2}
He, K., Gkioxari, G., Doll√°r, P., \& Girshick, R. (2017). \textit{Mask R-CNN}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Noh et al., 2015)]{3}
Noh, H., Hong, S., \& Han, B. (2015). \textit{Learning Deconvolution Network for Semantic Segmentation}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Google)]{4}
Google. (n.d.). \textit{Smart speaker mit google assistant}. Google. \url{https://assistant.google.com/intl/de_de/platforms/speakers/}
\end{thebibliography}
}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\endlecture
\end{document}
