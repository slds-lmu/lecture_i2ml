\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-nn}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Neural Networks
  }{% Lecture title  
  Single Neuron / Perceptron
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/step2-zin.jpg
}{% Learning goals, wrapped inside itemize environment
  \item Graphical representation of a single neuron
  \item Affine transformations and non-linear activation functions
  \item Hypothesis spaces of a single neuron
  \item Typical loss functions
}

%\lecturechapter{Single Neuron / Perceptron}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame} {A Single Neuron}
%\begin{itemize}
%\item To illustrate the types of functions that neural networks can represent, let us begin with a simple model: logistic regression.
%\vspace{5mm}
%\item The hypothesis space of logistic regression can be written as follows, where $\tau(z) = (1 + \exp(-z))^{-1}$ is the logistic sigmoid function:
%\begin{small} 
%$$\Hspace = \left\{f: \R^p \to [0, 1] ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p w_j x_j + b\right), \wtw \in \R^p, b \in \R \right\},$$ \end{small}
%\vspace{3mm}
%\item It is straightforward to represent $\fx$ graphically as a neuron.
%\vspace{5mm}
%\item Note: $\wtw$ and $b$ together constitute $\thetab$.
%\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\vfill
\item The perceptron is a single artificial neuron and the basic computational unit of neural networks.
\vfill
\item It is a weighted sum of input values, transformed by $\tau$:
\vfill
$$f(x) = \tau(w_1x_1 + ... + w_px_p +  b) = \tau(\mathbf{w}^T \mathbf{x}+b)$$
\vfill
\imageC[0.5]{figure/perceptron_tau.png}
{\footnotesize Perceptron %$z$, 
with \textbf{input features} $x_1, x_2, ... ,x_p$, \textbf{weights} $w_1, w_2,... ,w_p$, \textbf{bias term} $b$, and \textbf{activation function} $\tau$.}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{A Single Neuron}
\begin{columns}
\begin{column}{0.6\textwidth}
\item \textbf{Activation function $\tau$:} a single neuron represents different functions depending on the choice of activation function.
\item The identity function gives us the simple \textbf{linear regression}:
$$f(x) = \tau(\mathbf{w}^T \mathbf{x}) = \mathbf{w}^T \mathbf{x}$$
\item The logistic function gives us the \textbf{logistic regression}:
$$f(x) = \tau(\mathbf{w}^T \mathbf{x}) = \frac{1}{1 + \exp(-\mathbf{w}^T \mathbf{x})}$$
\end{column}
\begin{column}{0.4\textwidth}
\begin{center}
\imageC{figure/activation_functions_comparison.png}
\end{center}
\end{column}
\end{columns}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\item We consider a perceptron with $3$-dimensional input, i.e. $\fx = \tau(w_1\textcolor{red}{x_1} + w_2\textcolor{red}{x_2} + w_3\textcolor{red}{x_3} + b)$.
\item %First, 
Input features $\xv$ are represented by nodes in the \enquote{input layer}.
\imageC[0.5]{figure/neurep_one.png}
\item In general, a $p$-dimensional input vector $\xv$ will be represented by $p$ nodes in the input layer.
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\item Weights $\mathbf{w}$ are connected to edges from the input layer.
\imageC[0.6]{figure/neurep_two.png}
\item The bias term $b$ is implicit here. It is often not visualized as a separate node.
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\item For an explicit graphical representation, we do a simple trick: 
\item Add a constant feature to the inputs $\tilde{\xv} = (1, x_1, ..., x_p)^T$
\item and absorb the bias into the weight vector $\tilde{\bm{w}} = (b, w_1, ..., w_p)$.
The graphical representation is then: 
\imageC[0.6]{figure/neurep_bias.png}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\item The computation $\tau(w_1x_1 + w_2x_2 + w_3x_3 + b)$ is represented by the neuron in the \enquote{output layer}.
\imageC[0.6]{figure/neurep_three.png}
%\item Because this single neuron represents exactly the same hypothesis space as logistic regression, it can only learn linear decision boundaries.
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\item %A nice thing about this graphical representation of functions is that 
You can picture the input vector being "fed" to neurons on the left followed by a sequence of computations performed from left to right. This is called a \textbf{forward pass}.
\imageC[0.6]{figure/forward_pass.png}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron}
\item A neuron performs a 2-step computation:
\item \textbf{Affine Transformation:} weighted sum of inputs plus bias.
\imageC[0.4]{figure/step1-zin.jpg}
\item \textbf{Non-linear Activation:} a non-linear transformation applied to the weighted sum.
\imageC[0.4]{figure/step2-zin.jpg}
\vfill
\item \textbf{Notation:}
\item $\sigma$: activation functions in hidden/middle layers
\item $\tau$: output activation/transformation (final layer)
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{vbframe} {A Single Neuron}
%\begin{itemize}
%\item Even though all neurons compute a weighted sum in the first step, there is considerable flexibility in the type of activation function used in the second step.
%\item For example, setting the activation function to the logistic sigmoid function allows a neuron to represent logistic regression. The following architecture with two neurons represents two logistic regressions using the same input features:
%\end{itemize}
%\begin{figure}
%\includegraphics[width=5cm]{figure/logistic_regression.png}
%\end{figure}
%\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{A Single Neuron: Hypothesis Space}
\item The hypothesis space that is formed by single neuron %architectures 
is 
{\small
$$\Hspace  = \left\{f: \R^p \to \R ~\bigg|~ \fx = \tau\left(\sum_{j = 1}^p w_j x_j + b\right), \wtw \in \R^p, b \in \R\right\}.$$}
%\item Both logistic regression and linear regression are subspaces of $\Hspace$ (if $\tau$ is the logistic sigmoid / identity function).  
\item If $\tau$ is the logistic sigmoid or identity function, $\Hspace$ corresponds to the hypothesis space of logistic or linear regression, respectively.

\vfill
\imageC{figure/neuron_regcls.png}
\centerline{\tiny \textit{Left}: A regression line learned by a single neuron. \textit{Right}: A decision-boundary learned by a single neuron in a binary classification task.}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron: Optimization}
\item To optimize this model, we minimize the empirical risk 
$$\riske = \frac{1}{n} \sumin \Lxyi,$$
where $\Lxy$ is a loss function. It compares the network's predictions $\fx$ to the ground truth $y$. 
\item For regression, we typically use the L2 loss (rarely L1): $$\Lxy = \frac{1}{2}(y - \fx)^2$$
\item For binary classification, we typically apply the cross entropy loss (also known as Bernoulli loss): $$\Lxy = -(y \log \fx + (1 - y) \log(1 - \fx))$$
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei} {A Single Neuron: Optimization}
\item For a single neuron and both choices of $\tau$ the loss function is convex.
\item The global optimum can be found with an iterative algorithm like gradient descent. 
\item A single neuron with logistic sigmoid function trained with the Bernoulli loss %does not only have 
%has the same hypothesis space as a logistic regression %and is therefore the same model, but
 %will also 
 yields the %very 
 same result as logistic regression when trained until convergence.
\item Note: In the case of regression and the L2-loss, the solution can
also be found analytically using the “normal equations”. However, in other cases a closed-form solution is usually not available.
\end{framei} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}