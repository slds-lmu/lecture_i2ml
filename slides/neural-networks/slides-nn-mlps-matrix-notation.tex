\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-nn}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Neural Networks
  }{% Lecture title  
  MLP: Matrix Notation
}{% Relative path to title page image: Can be empty but must not start with slides/
figure/sinlay_six.png
}{% Learning goals, wrapped inside itemize environment
  \item Compact representation of neural network equations
  \item Vector notation for neuron layers
  \item Vector and matrix notation of bias and weight parameters
}

%\lecturechapter{MLP: Matrix Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%\frametitle{Lecture outline}
%\lz
%This chapter introduces a compact way for representing feedforward neural networks: the matrix formalism from linear algebra.
%\lz
%\lz
%
%\begin{enumerate}
%\item First, we explore networks with one hidden layer and one output unit.
%\lz
%\item Next, we investigate networks with one hidden layer but multiple output units.
%\lz
%\item Finally, we focus on multi-layer feedforward networks with an arbitrary number of hidden layers and output units.
%\end{enumerate}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%\section{Single Hidden Layer Networks for Regression and Binary Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}[sep=L]{Single Hidden Layer Networks: Input Notation}

\item The input $\xv$ is a column vector with dimensions $p \times 1$. 
\item $\Wmat$ is a weight matrix with dimensions $p \times m$, where $m$ is the amount of hidden neurons:
$$\Wmat =
     \begin{pmatrix}
      w_{1,1} & w_{1,2} & \cdots & w_{1,m} \\
      w_{2,1} & w_{2,2} & \cdots & w_{2,m} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      w_{p,1} & w_{p,2} & \cdots & w_{p,m}
     \end{pmatrix}$$
     
     \end{framei}
     
\begin{framei}{Single Hidden Layer Networks: Hidden Layer Notation}

\item For example, to obtain $z_1$, we pick the first column of $W$:
    $$\Wmat_1 =
     \begin{pmatrix}
      w_{1,1} \\
      w_{2,1} \\
      \vdots  \\
      w_{p,1}
     \end{pmatrix}$$
and compute 
$$z_1 = \sigma(\Wmat_1^T \xv + b_1)\enspace,$$
 where $b_1$ is the bias of the first hidden neuron and $\sigma: \mathbb{R} \to \mathbb{R}$ is an activation function. 
    
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{framei}{Single Hidden Layer Networks: Hidden Layer Notation}

\item The network has $m$ hidden neurons $z_1, \dots, z_m$ with
    $$ z_j = \sigma(\mathbf{W}_j^T \xv + b_j), j \in \{1,\ldots,m\}$$
    \begin{itemize}
    \item $z_{j,in}  = \mathbf{W}_j^T \xv + b_j$
    \item $z_{j,out} = \sigma(z_{j,in}) = \sigma(\mathbf{W}_j^T \xv + b_j)$
    \end{itemize}
\item $ \hidz_{in} = (z_{1,in}, \dots, z_{m,in})^T = \Wmat^T \xv + \biasb$ \\ (Note: $\Wmat^T \xv$ = $(\xv^T \Wmat)^T$)
\item $ \hidz = \hidz_{out} = \sigma(\hidz_{in}) = \sigma(\Wmat^T \xv + \biasb)$, where the (hidden layer) activation function $\sigma$ is applied element-wise to $\hidz_{in}$.  

\end{framei}


\begin{framei}[sep=L]{Single Hidden Layer Networks: Bias Term Notation}

\item We sometimes omit the bias term by adding a constant feature to the input $\tilde{\xv} = (1, x_1, ..., x_p)$ and by adding the bias term to the weight matrix 
$$\tilde{\Wmat} = (\biasb, \Wmat_1, ..., \Wmat_p).$$ 
\item \textbf{Note}: For simplification purposes, we will not explicitly represent the bias term graphically in the following. However, the above \enquote{trick} makes it straightforward to represent it graphically. 

\end{framei}

\begin{framei}{Single Hidden Layer Networks: Output Layer Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \item For regression or binary classification: one output unit $f$ where
      \begin{itemize}
        \item $f_{in} = \wtu^T \hidz + c$ , i.e. a linear combination of derived features plus the bias term $c$ of the output neuron, and

        \item $\fx= f_{out} = \tau(f_{in}) = \tau(\wtu^T \hidz + c)$ , where $\tau$ is the output activation function.
      \end{itemize}
    \item For regression $\tau$ is the identity function.
    \item For binary classification, $\tau$ is a sigmoid function.
    \item \textbf{Note}: The purpose of the hidden-layer activation function $\sigma$ is to introduce non-linearities so that the network is able to learn complex functions whereas the purpose of $\tau$ is merely to get the final score to the same range as the target.
\end{framei}

\begin{framei}{Single Hidden Layer Networks: Multiple Inputs Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

    \item It is possible to feed multiple inputs to a neural network simultaneously.

    \item The inputs $\xi$, for $i \in \nset$, are arranged as rows in the \textbf{design matrix} $\Xmat$.
    \begin{itemize}
      \item $\Xmat$ is a ($n \times p$)-matrix.
    \end{itemize}

    \item The weighted sum in the hidden layer is now computed as $\Xmat\Wmat + \bm{B}$, where,
      \begin{itemize}
        \item $\Wmat$, as usual, is a ($p \times m$) matrix, and,

        \item $\bm{B}$ is a ($n \times m$) matrix containing the bias vector $\biasb$ (duplicated) as the rows of the matrix.
      \end{itemize}

\end{framei}    

\begin{framei}[sep=L]{Single Hidden Layer Networks: Multiple Inputs Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item The \textit{matrix} of hidden activations $\bm{Z} = \sigma(\Xmat\Wmat + \bm{B})$
    \begin{itemize}
      \item $\bm{Z}$ is a ($n \times m$) matrix.
    \end{itemize}
    
    \item The final output of the network, which contains a prediction for each input, is $\tau(\bm{Z}\wtu + \bm{C})$, where
      \begin{itemize}

        \item $\wtu$ is the vector of weights of the output neuron, and,

        \item $\bm{C}$ is a ($n \times 1$) matrix whose elements are the (scalar) bias $c$ of the output neuron.
      \end{itemize}

\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\endlecture
\end{document}