\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}


\newcommand{\titlefigure}{figure/neuralnet_new.png}
\newcommand{\learninggoals}{
  \item Neural network architectures for multi-class classification 
  \item Softmax activation function 
  \item Softmax loss}

\title{Deep Learning}
\date{}

\begin{document}

\lecturechapter{Single Hidden Layer Networks for Multi-Class Classification}
\lecture{I2DL}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Single Hidden Layer Networks for Multi-Class Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Multi-class Classification}
\vspace{20mm}
\begin{itemize}
\item We have only considered regression and binary classification problems so far.
\vspace{5mm}
\item How can we get a neural network to perform multiclass classification?
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Multi-class Classification}
\begin{itemize}
\item The first step is to add additional neurons to the output layer.
\item Each neuron in the layer will represent a specific class (number of neurons in the output layer = number of classes).
\begin{figure}
\centering
\scalebox{0.75}{\includegraphics[width=10.2cm]{figure/neuralnet_new.png}}
\caption{\footnotesize Structure of a single hidden layer, feed-forward neural network for g-class classification problems (bias term omitted).}
\end{figure}
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Multi-class Classification}
\vspace{5mm}
\begin{blocki}{Notation:}
\item For $g$-class classification, $g$ output units: $$\mathbf{f} = (f_1, \dots, f_g)$$
%\vspace{4mm}
\item $m$ hidden neurons $z_1, \dots, z_m$, with
    $$ z_j = \sigma(\mathbf{W}_j^T \xv), \quad j = 1,\ldots,m. $$
\item Compute linear combinations of derived features $z$:
    $$ f_{in,k} = \bm{U}_k^T \hidz, \quad \hidz=(z_1,\dots, z_m)^T, \quad k = 1,\ldots,g$$
\end{blocki}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Multi-class Classification}
  \begin{itemize}
    \item The second step is to apply a \textbf{softmax} activation function to the output layer.
    \vspace{4mm}
    \item This gives us a probability distribution over $g$ different possible classes:
    $$ f_{out,k} = \tau_k(f_{in,k}) = \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})}$$
    \vspace{2mm}
    \item This is the same transformation used in softmax regression!
    \vspace{4mm}
    \item Derivative $ \frac{\partial \tau(\mathbf{f}_{in})}{\partial \mathbf{f}_{in}} = \text{diag}(\tau(\mathbf{f}_{in})) - \tau(\mathbf{f}_{in}) \tau(\mathbf{f}_{in})^T $
    \vspace{4mm}
    \item It is a \enquote{smooth} approximation of the argmax operation,
        so $\tau((1, 1000, 2)^T) \approx (0, 1, 0)^T$ (picks out 2nd element!).
  \end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Multi-class Classification: Example}
Forward pass (Hidden: Sigmoid, Output: Softmax).
  \begin{figure}
    \centering
      \only<1>{\scalebox{0.95}{\includegraphics{figure/softie_one.png}}}
      \only<2>{\scalebox{0.95}{\includegraphics{figure/softie_two.png}}}
      \only<3>{\scalebox{0.95}{\includegraphics{figure/softie_three.png}}}
      \only<4>{\scalebox{0.95}{\includegraphics{figure/softie_four.png}}}
      \only<5>{\scalebox{0.95}{\includegraphics{figure/softie_five_a.png}}}
      \only<6>{\scalebox{0.95}{\includegraphics{figure/softie_five_b.png}}}
      \only<7>{\scalebox{0.95}{\includegraphics{figure/softie_six.png}}}
      \only<8>{\scalebox{0.95}{\includegraphics{figure/softie_seven.png}}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Optimization: Softmax Loss}
  \begin{itemize}
\vspace{5mm}
\item The loss function for a softmax classifier is
$$L(y, \fx) = - \sum_{k = 1}^g [y = k] \log \left( \frac{\exp(f_{in,k})}{\sum_{k'=1}^g\exp(f_{in,k'})}\right)$$
where $[y = k] = \begin{cases} 1 & \text{ if } y = k \\
0 & \text{ otherwise }
\end{cases}$. 
\vspace{5mm}
\item This is equivalent to the cross-entropy loss when the label vector $\bm{y}$ is one-hot coded (e.g. $\mathbf{y} = (0,0,1,0)^T$). 
\item Optimization:  Again, there is no analytic solution.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}