\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-nn}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Neural Networks
  }{% Lecture title  
  MLP -- Multi-Layer Feedforward Neural Networks
}{% Relative path to title page image: Can be empty but must not start with slides/
figure/deepnet_one.png
}{% Learning goals, wrapped inside itemize environment
  \item Architectures of deep neural networks
  \item Deep neural networks as  chained functions
}

\lecturechapter{MLP -- Multi-Layer Feedforward Neural Networks}

%\section{Multi-Layer Feedforward Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}[sep=L]{Feedforward neural networks I}
\item We will now extend the model class once again, such that we allow an arbitrary amount $l$ of hidden layers.
\vfill
\item The general term for this model class is (multi-layer) \textbf{feedforward networks} (inputs are passed through the network from left to right, no feedback-loops are allowed)
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Feedforward neural networks II}
\item We can characterize those models by the following chain structure: $$\fx = \tau \circ \phi \circ \sigma^{(l)} \circ \phi^{(l)} \circ \sigma^{(l-1)} \circ \phi^{(l-1)} \circ \ldots \circ \sigma^{(1)} \circ \phi^{(1)}$$ where $\sigma^{(i)}$ and $\phi^{(i)}$ are the activation function and the weighted sum of hidden layer $i$, respectively. $\tau$ and $\phi$ are the corresponding components of the output layer.
\vfill
\item Each hidden layer has: 
\begin{itemize}
\vfill
\item an associated weight matrix $\Wmat^{(i)}$, bias $\biasb^{(i)}$, and activations $\hidz^{(i)}$ for $i \in \{ 1 \ldots l\}$.
\vfill
\item $\hidz^{(i)} = \sigma^{(i)}(\phi^{(i)}) = \sigma^{(i)}(\Wmat^{(i)T}\hidz^{(i - 1)} + \biasb^{(i)})$ , where $\hidz^{(0)} = \xv$.
\end{itemize}
\vfill
\item Again, without non-linear activations in the hidden layers, the network can only learn linear decision boundaries.
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Feedforward neural networks III}
\item Structure of a deep neural network with $l$ hidden layers (bias terms omitted):
\imageC[0.8]{figure/deepneuralnet_new.png}
\end{framei}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Feedforward neural networks: Example}
\begin{figure}
\centering
\only<1>{\scalebox{0.95}{\includegraphics{figure/deepnet_one.png}}}
\only<2>{\scalebox{0.95}{\includegraphics{figure/deepnet_two.png}}}
\only<3>{\scalebox{0.95}{\includegraphics{figure/deepnet_three.png}}}
\only<4>{\scalebox{0.95}{\includegraphics{figure/deepnet_four.png}}}
\only<5>{\scalebox{0.95}{\includegraphics{figure/deepnet_five.png}}}
\only<6>{\scalebox{0.95}{\includegraphics{figure/deepnet_six.png}}}
\only<7>{\scalebox{0.95}{\includegraphics{figure/deepnet_seven.png}}}
\only<8>{\scalebox{0.95}{\includegraphics{figure/deepnet_eight.png}}}
\only<9>{\scalebox{0.95}{\includegraphics{figure/deepnet_nine.png}}}
\only<10>{\scalebox{0.95}{\includegraphics{figure/deepnet_ten.png}}}
\only<11>{\scalebox{0.95}{\includegraphics{figure/deepnet_eleven.png}}}
\only<12>{\scalebox{0.95}{\includegraphics{figure/deepnet_twelve.png}}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Why add more layers?}
\item Multiple layers allow for the extraction of more and more abstract
representations.
%\lz
\item Each layer in a feed-forward neural network adds its own degree of non-linearity to the model.
%\lz
\imageC[0.6][MONTUFAR2014]{figure/folding}
\centerline{\footnotesize An intuitive, geometric explanation of the exponential advantage of deeper networks formally.}
\end{framei}
%%%%%%%%%%%%%%}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Deep neural networks I}

\item Neural networks today can have %dozens or even 
hundreds of hidden layers. The greater the number of layers, the "deeper" the network. Historically %, however, 
DNNs were very challenging to train and not popular until the late '00s for several reasons:
\vfill

\begin{itemize}
\item %For one thing, the 
The use of sigmoid activations (e.g., logistic sigmoid and tanh) significantly slowed down training due to a phenomenon known as \enquote{vanishing gradients}. The introduction of the ReLU activation largely solved this problem.
\item Training DNNs on CPUs was too slow to be practical. Switching over to GPUs cut down training time by more than an order of magnitude.
\item When dataset sizes are small, other models (such as SVMs) and techniques (such as feature engineering) often outperform them. 
\end{itemize}

\end{framei}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{framei}[sep=L]{Deep neural networks II}

\begin{itemize}
\item The availability of large datasets and novel architectures that are capable of handling even complex tensor-shaped data (e.g. CNNs for image data), faster hardware, and better optimization and regularization methods made it feasible to successfully implement deep neural networks.% in the last decade.

\item An increase in depth often translates to an increase in performance on a given task. State-of-the-art neural networks, however, are much more sophisticated than the simple architectures we have encountered so far.
\end{itemize}

\item The term "\textbf{deep learning}" encompasses all of these developments and refers to the field as a whole.
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\endlecture
\end{document}