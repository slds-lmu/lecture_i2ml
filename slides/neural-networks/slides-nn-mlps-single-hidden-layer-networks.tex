\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-nn}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Neural Networks
  }{% Lecture title  
  Single hidden layer neural networks
}{% Relative path to title page image: Can be empty but must not start with slides/
figure/newrep_n_f.png
}{% Learning goals, wrapped inside itemize environment
  \item Architecture of  single hidden layer neural networks
  \item Representation learning/ understanding the advantage of  hidden layers
  \item Typical (non-linear) activation functions
}

%\lecturechapter{Single hidden layer neural networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Motivation}
\item The graphical way of representing simple functions/models, like logistic regression. Why is that useful?
\item Because individual neurons can be used as building blocks of more complicated functions.
\item Networks of neurons can represent extremely complex hypothesis spaces.
\item Most importantly, it allows us to define the \enquote{right} kinds of hypothesis spaces to learn functions that are %more 
common in our universe in a data-efficient way (see Lin, Tegmark et al. 2016).
\end{framei}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Motivation}
\item Can a single neuron perform binary classification of these points?
%\vspace{2cm}
\imageC[0.5]{figure/cartesian.png}
\end{framei}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Motivation}
\item As a single neuron is restricted to learning only linear decision boundaries, its performance on the following task is quite poor:
\imageC[0.2]{figure/cartesian.png}
\item However, the neuron can easily separate the classes if the original features are transformed (e.g., from Cartesian to polar coordinates): 
\imageC[0.2]{figure/polar.png}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Motivation}
\item Instead of classifying the data in the original representation,
\imageC[0.9]{figure/repold_f.png}
\item we classify it in a new feature space.
\imageC[0.9]{figure/repnew_f.png}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Motivation}
\item Analogously, instead of a single neuron, 
\imageC[0.5]{figure/oldrep_n_f.png}
\item we use more complex networks.
\imageC[0.5]{figure/newrep_n_f.png}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Representation Learning}
    \item It is %therefore 
    \textit{very} critical to feed a classifier the \enquote{right} features in order for it to perform well.
    \item Before deep learning took off, features for tasks like machine vision and speech recognition were \enquote{hand-designed} by domain experts. This step of the machine learning pipeline is called \textbf{feature engineering}.
    \item %The single biggest reason
     DL %is so important is that it 
    automates feature engineering. This is called \textbf{representation learning}.
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Single Hidden Layer Networks}
\item \textbf{Single neurons} perform a 2-step computation:
\begin{enumerate}
\item \textbf{Affine Transformation:} a weighted sum of inputs plus bias.
\item \textbf{Activation:} a non-linear transformation on the weighted sum.
\end{enumerate}
\vfill
\item \textbf{Single hidden layer networks} consist of two layers (without input layer):
\begin{enumerate}
\item \textbf{Hidden Layer:} having a set of neurons.
\item \textbf{Output Layer:} having one or more output neurons.
\end{enumerate}
\vfill

\item Multiple inputs are simultaneously fed to the network.
\vfill
\item Each neuron in the hidden layer performs a 2-step computation.
\vfill
\item The final output of the network is then calculated by another 2-step computation performed by the neuron in the output layer.

\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  5

\begin{frame} {Single Hidden Layer Networks: Example}
Each neuron in the hidden layer performs an \textbf{affine transformation} on the inputs:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_1.png}} $$\begin{array}{l}
z_{\text {1,in}}=w_{11} x_1+w_{21} x_2+w_{31} x_3+b_1 \\
z_{\text {1,in}}= 3 * (-3) + (-9) * 1 + 2 * 5 + 5 = -3\end{array}$$}
\only<2>{\scalebox{.8}{\includegraphics{figure/singlelay_2.png}} $$\begin{array}{l}
z_{\text {2,in}}=w_{12} x_1+w_{22} x_2+w_{32} x_3+b_2 \\
z_{\text {2,in}}= 11 * (-3) + (-2) * 1 + 7 * 5 + 2 = 2\end{array}$$}
\only<3>{\scalebox{.8}{\includegraphics{figure/singlelay_3.png}}$$\begin{array}{l}
z_{\text {3,in}}=w_{13} x_1+w_{23} x_2+w_{33} x_3+b_3 \\
z_{\text {3,in}}= (-6) *(-3) + 3 * 1 + (-4) * 5 - 1 = 0\end{array}$$}
\only<4>{\scalebox{.8}{\includegraphics{figure/singlelay_4.png}}$$\begin{array}{l}
z_{\text {4,in}}=w_{14} x_1+w_{24} x_2+w_{34} x_3+b_4 \\
z_{\text {4,in}}= 6 * (-3) + (-1) * 1 + 5 * 5 + 1 = 7\end{array}$$}
\only<5>{\scalebox{.8}{\includegraphics{figure/singlelay_5.png}}}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   1

\begin{frame} {Single Hidden Layer Networks: Example}
Each hidden neuron performs a non-linear \textbf{activation} transformation on the weight sum:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_6.png}}$$\begin{array}{l}
z_\text{i,out}=\sigma\left(z_{\text {i,in}}\right)=\frac{1}{1+e^{-z_{\text {in }}^{(i)}}}\end{array}$$}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  2

\begin{frame} {Single Hidden Layer Networks: Example}
The output neuron performs an \textbf{affine transformation} on its inputs:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_7.png}}$$\begin{array}{l}
f_\text {in}=u_1 z_\text{1,out}+u_2 z_\text{2,out}+u_{3} z_\text{3,out}+u_{4} z_\text{4,out}+c\end{array}$$}
\only<2>{\scalebox{.8}{\includegraphics{figure/singlelay_8.png}}$$\begin{array}{l}
f_\text {in}=u_1 z_\text{1,out}+u_2 z_\text{2,out}+u_{3} z_\text{3,out}+u_{4} z_\text{4,out}+c \\ f_{\text {in}}= 3 * 0.05 + (-12) * 0.88 + 8* 0.50 + 1 * 0.99 + 6= 0.57\end{array}$$}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   1

\begin{frame} {Single Hidden Layer Networks: Example}
The output neuron performs a non-linear \textbf{activation} transformation on the weight sum:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_9.png}}$$\begin{array}{l}
f_\text{out}=\sigma (f_\text{in})= \frac{1}{1+e^{-f_\text{in}}} \\ f_\text{out}=\frac{1}{1+e^{-0.57}}=0.64\end{array}$$}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{framei} {Hidden Layer: Activation Function}
\item If the hidden layer does not have a non-linear activation, the network can only learn linear decision boundaries.
\item A lot of different activation functions exist.
%\item For simplification purposes, we drop the bias terms in notation and let $\sigma %= \text{id}$. Then:
 %   \begin{eqnarray*}
      % \fx & = & \tau(\\hidz) \\
      %   & = & \tau(\wtu^T \sigma(\Wmat^T \xv)) \\
      %  & = & \tau(\wtu^T\Wmat^T \xv) = \tau(\mathbf{v}^T \xv)
 %     \end{eqnarray*}
  %    where $ \mathbf{v} = \Wmat\wtu$. It can be seen that $\fx$ can only yield a linear decision boundary.
\end{framei}


\begin{framei}{Hidden Layer: Activation Function I}
%\textbf{Note:} if the hidden layer does not have a non-linear activation, the network can only learn linear decision boundaries.
\item \textbf{ReLU Activation:}
Currently the most popular choice is the ReLU (rectified linear unit).
$$ \sigma (v) = \max(0,v) $$
\imageC[0.9]{figure_man/relu.png}
\end{framei}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{blocki}{Hyperbolic Tangent Activation:}
%\item Another choice might be the hyperbolic tangent function:
%$$ \sigma (v) = \text{tanh}(v) = \frac{\text{sinh}(v)}{\text{cosh}(v)} = 1 - \frac{2}{\exp(2v) + 1}$$
%\end{blocki}
%\begin{figure}
%\scalebox{0.9}{\includegraphics{figure_man/tanh.png}}
%\end{figure}
%\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{framei}{Hidden Layer: Activation Function II}
\item \textbf{Sigmoid Activation Function:}
The sigmoid function can be used even in the hidden layer.
$$ \sigma(v) = \frac{1}{1+\exp (-v)} $$
\imageC{figure_man/sigmoid.png}
\end{framei}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\endlecture
\end{document}