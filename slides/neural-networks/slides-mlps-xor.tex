\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}


\newcommand{\titlefigure}{figure/xor1.png}
\newcommand{\learninggoals}{
  \item Example problem a single neuron can not solve but a single hidden layer net can
}

\title{Deep Learning}
\date{}

\begin{document}


\lecturechapter{XOR-Problem}
\lecture{I2DL}

\begin{vbframe}{Example: XOR Problem}
  \begin{itemize}
    \item Suppose we have four data points $$X = \{(0,0)^\top, (0,1)^\top, (1,0)^\top, (1,1)^\top \}$$
    \item The XOR gate (exclusive or) returns true, when an odd number of inputs are true:
  \end{itemize}
  \begin{table}
    \centering
      \begin{tabular}{ccc}
        \textbf{$x_1$}  & \textbf{$x_2$}  & \textbf{XOR} $= y$ \\
        \hline
        \hline
        $0$             &   $0$           &  $0$ \\
        $0$             &   $1$           &  $1$ \\
        $1$             &   $0$           &  $1$ \\
        $1$             &   $1$           &  $0$
      \end{tabular}
  \end{table}
  \begin{itemize}
    \item Can you learn the target function with a logistic regression model? \\
    % (Aside from statistical generalization, we just want to learn the training data!)
  \end{itemize}
\framebreak
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item Logistic regression can not solve this problem. %and will always output $0.5$. \\
      In fact, any model using simple hyperplanes for separation can not (including a single neuron).
      \lz
      \item A small neural net can easily solve the problem by transforming the space!
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \includegraphics{figure/xor1.png}%
  \end{minipage}\hfill
\framebreak
  \begin{itemize}
    \item Consider the following model:
  \end{itemize}
    \begin{figure}
      \centering
        \includegraphics[width=8cm]{figure/xor_rep.png}%
        \caption{A neural network with two neurons in the hidden layer. The matrix $\Wmat$ describes the mapping from $\xv$ to $\hidz$. The vector $\wtu$ from $\hidz$ to $y$.}
    \end{figure}
\framebreak
  \begin{itemize}
    \item Let use ReLU $\sigma(z) = \max\left\{0, z \right\}$ as activation function and a simple thresholding function $\tau(z) = [ z > 0 ] =  \begin{cases} 1 & \text{ if } z > 0 \\ 0 & \text{otherwise} \end{cases} $ \\ as output transformation function. We can represent the architecture of the model by the following equation: 
  \end{itemize}
  \begin{eqnarray*}
    f(\xv~|~\thetah) &=& f(\xv~|~ \Wmat, \biasb, \wtu, \biasc) = \tau\left(\wtu^\top\sigma(\Wmat^\top \xv+\biasb)+\biasc\right) \\
                &=& \tau\left(\wtu^\top \max\{0, \Wmat^\top \xv+\biasb\} + \biasc\right)
  \end{eqnarray*}
  % \begin{footnotesize}
  % \textbf{Note:} To simplify calculations, we \enquote{falsely} treat the problem as a regression problem (no final output transformation $\tau$ to map scores to $[0, 1]$). We will see later that transformation to $[0, 1]$ is not necessary in this very special case.
  % \end{footnotesize}
  \begin{itemize}
    \item So how many parameters does our model have?
    \begin{itemize}
      \item In a fully connected neural net, the number of connections between the nodes equals our parameters: $$\underbrace{(2 \times 2)}_{W} + \underbrace{(2 \times 1)}_{\biasb} + \underbrace{(2 \times 1)}_{\wtu} + \underbrace{(1)}_{c} = 9$$
    \end{itemize}
  \end{itemize}
\framebreak
  \begin{eqnarray*}
   \text{Let} \ \Wmat = \begin{pmatrix}
      1 & 1 \\
      1 & 1
    \end{pmatrix}, \
      \biasb = \begin{pmatrix}
      0 \\
      -1
    \end{pmatrix}, \
      \wtu = \begin{pmatrix}
      1 \\
      -2
    \end{pmatrix}, \
      c = - 0.5
  \end{eqnarray*}
  \begin{eqnarray*}
    \Xmat = \begin{pmatrix}
      0 & 0 \\
      0 & 1 \\
      1 & 0 \\
      1 & 1
    \end{pmatrix}, \
    \Xmat \Wmat = \begin{pmatrix}
      0 & 0 \\
      1 & 1 \\
      1 & 1 \\
      2 & 2
    \end{pmatrix}, \
      \Xmat \Wmat + \bm{B} = \begin{pmatrix}
        0 & -1 \\
        1 & 0 \\
        1 & 0 \\
        2 & 1
    \end{pmatrix}
  \end{eqnarray*}
%\vspace{4mm}
 \footnotesize{Note: $\Xmat$ is a $(n \times p)$ design matrix in which the \textit{rows} correspond to the data points. $\Wmat$, as usual, is a $(p \times m)$ matrix where each \textit{column} corresponds to a single (hidden) neuron. $\bm{B}$ is a ($n \times m$) matrix with $\biasb$ duplicated along the rows.}
 \begin{figure}
    \centering
      \scalebox{0.6}{\includegraphics{figure/rowcol.png}}
  \end{figure}
 \framebreak
 \normalsize{
 \begin{eqnarray*}
   \text{Let} \ \Wmat = \begin{pmatrix}
      1 & 1 \\
      1 & 1
    \end{pmatrix}, \
      \biasb = \begin{pmatrix}
      0 \\
      -1
    \end{pmatrix}, \
      \wtu = \begin{pmatrix}
      1 \\
      -2
    \end{pmatrix}, \
      c = - 0.5
  \end{eqnarray*}
  \begin{eqnarray*}
  \Xmat = \begin{pmatrix}
      0 & 0 \\
      0 & 1 \\
      1 & 0 \\
      1 & 1
    \end{pmatrix}, \
    \Xmat\Wmat = \begin{pmatrix}
      0 & 0 \\
      1 & 1 \\
      1 & 1 \\
      2 & 2
    \end{pmatrix}, \
      \Xmat \Wmat + \bm{B} = \begin{pmatrix}
        0 & -1 \\
        1 & 0 \\
        1 & 0 \\
        2 & 1
    \end{pmatrix}
  \end{eqnarray*}
  \begin{eqnarray*}
    \bm{Z} = \max\{0, \Xmat \Wmat+\bm{B}\}
    &=&
    \begin{pmatrix}
      0 & 0 \\
      1 & 0 \\
      1 & 0 \\
      2 & 1
    \end{pmatrix}
  \end{eqnarray*}
  \begin{itemize}
    \item Note that we computed all examples at once.
  \end{itemize}

\framebreak
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item The input points are mapped into transformed space to
        \begin{eqnarray*}
          \bm{Z} = \begin{pmatrix}
              0 & 0 \\
              1 & 0 \\
              1 & 0 \\
              2 & 1
          \end{pmatrix}
        \end{eqnarray*}
    %  \item[] which is easily separable.
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \includegraphics<1>{figure/xor2_2.png}%
  \end{minipage}\hfill
  
\framebreak
  \begin{minipage}{0.45\textwidth}
    \begin{itemize}
      \item The input points are mapped into transformed space to
        \begin{eqnarray*}
          \bm{Z} = \begin{pmatrix}
              0 & 0 \\
              1 & 0 \\
              1 & 0 \\
              2 & 1
          \end{pmatrix}
        \end{eqnarray*}
      \item[] which is easily separable.
    \end{itemize}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
    \includegraphics{figure/xor2.png}%
  \end{minipage}\hfill
  
\framebreak
  \begin{itemize}
    \item In a final step we have to multiply the activated values of matrix $\bm{Z}$ with the vector $\wtu$ and add the bias term $c$:
  \end{itemize}
  \begin{eqnarray*}
    f(\xv ~|~ \Wmat, \biasb, \wtu, c) &=&
    \begin{pmatrix}
      0 & 0 \\
      1 & 0 \\
      1 & 0 \\
      2 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 \\
      -2
    \end{pmatrix} + 
    \begin{pmatrix}
      - 0.5 \\
      - 0.5 \\
      - 0.5 \\
      - 0.5
    \end{pmatrix}
    =
    \begin{pmatrix}
      - 0.5 \\
      0.5 \\
      0.5 \\
      - 0.5
    \end{pmatrix}
  \end{eqnarray*}
  \begin{itemize}
    \item And then apply the step function $\tau(z) = [z > 0 ]$.  This solves the XOR problem perfectly!
  \end{itemize}
  \begin{table}
    \centering
      \begin{tabular}{ccc}
        \textbf{$x_1$}  & \textbf{$x_2$}  & \textbf{XOR} = y\\
        \hline
        \hline
        $0$             &   $0$           &  $0$ \\
        $0$             &   $1$           &  $1$ \\
        $1$             &   $0$           &  $1$ \\
        $1$             &   $1$           &  $0$
      \end{tabular}
  \end{table}
  }
\end{vbframe}



\begin{frame} {Neural Networks : Optimization}
  \begin{itemize}
    \item In this simple example we actually \enquote{guessed} the values of the parameters for $\textbf{W}$, $\biasb$, $\wtu$ and $c$.
    \vspace{3mm}
    \item That won't work for more sophisticated problems!
    \vspace{3mm}
    %\item To learn the right weights (and biases), we once again have to rely on iterative algorithms like gradient descent.
    \item We will learn later about iterative optimization algorithms for automatically adapting weights and biases.
    \vspace{3mm}
    \item An added complication is that the loss function is no longer convex. Therefore, there might not exist a single minimum. 
    %Therefore, gradient descent can only find a local minimum. 
 %   \vspace{3mm}
  %  \item An extremely efficient method to compute gradients called backpropogation will be covered in the next lecture.
  \end{itemize}
\end{frame}

\endlecture
\end{document}