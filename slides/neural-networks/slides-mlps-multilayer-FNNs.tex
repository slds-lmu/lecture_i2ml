\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\newcommand{\titlefigure}{figure/deepnet_one.png}
\newcommand{\learninggoals}{
  \item Architectures of deep neural networks
  \item Deep neural networks as  chained functions
  %\item  ...
}

\title{Deep Learning}
\date{}

\begin{document}

\lecturechapter{MLP -- Multi-Layer Feedforward Neural Networks}
\lecture{Deep Learning}


%\section{Multi-Layer Feedforward Neural Networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Feedforward neural networks}
\begin{itemize}
\vspace{15mm}
\item We will now extend the model class once again, such that we allow an arbitrary amount $l$ of hidden layers.
\vspace{5mm}
\item The general term for this model class is (multi-layer) \textbf{feedforward networks} (inputs are passed through the network from left to right, no feedback-loops are allowed)
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item We can characterize those models by the following chain structure: $$\fx = \tau \circ \phi \circ \sigma^{(l)} \circ \phi^{(l)} \circ \sigma^{(l-1)} \circ \phi^{(l-1)} \circ \ldots \circ \sigma^{(1)} \circ \phi^{(1)}$$ where $\sigma^{(i)}$ and $\phi^{(i)}$ are the activation function and the weighted sum of hidden layer $i$, respectively. $\tau$ and $\phi$ are the corresponding components of the output layer.
\vspace{5mm}
\item Each hidden layer has: 
\begin{itemize}
\vspace{2mm}
\item an associated weight matrix $\Wmat^{(i)}$, bias $\biasb^{(i)}$, and activations $\hidz^{(i)}$ for $i \in \{ 1 \ldots l\}$.
\vspace{2mm}
\item $\hidz^{(i)} = \sigma^{(i)}(\phi^{(i)}) = \sigma^{(i)}(\Wmat^{(i)T}\hidz^{(i - 1)} + \biasb^{(i)})$ , where $\hidz^{(0)} = \xv$.
\end{itemize}
\vspace{5mm}
\item Again, without non-linear activations in the hidden layers, the network can only learn linear decision boundaries.
  \end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\lz
\begin{figure}
\centering
\includegraphics[width=10.5cm]{figure/deepneuralnet_new.png}
\caption{Structure of a deep neural network with $l$ hidden layers (bias terms omitted).}
  \end{figure}
\end{vbframe}  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame} {Feedforward neural networks: Example}
\begin{figure}
\centering
\only<1>{\scalebox{0.95}{\includegraphics{figure/deepnet_one.png}}}
\only<2>{\scalebox{0.95}{\includegraphics{figure/deepnet_two.png}}}
\only<3>{\scalebox{0.95}{\includegraphics{figure/deepnet_three.png}}}
\only<4>{\scalebox{0.95}{\includegraphics{figure/deepnet_four.png}}}
\only<5>{\scalebox{0.95}{\includegraphics{figure/deepnet_five.png}}}
\only<6>{\scalebox{0.95}{\includegraphics{figure/deepnet_six.png}}}
\only<7>{\scalebox{0.95}{\includegraphics{figure/deepnet_seven.png}}}
\only<8>{\scalebox{0.95}{\includegraphics{figure/deepnet_eight.png}}}
\only<9>{\scalebox{0.95}{\includegraphics{figure/deepnet_nine.png}}}
\only<10>{\scalebox{0.95}{\includegraphics{figure/deepnet_ten.png}}}
\only<11>{\scalebox{0.95}{\includegraphics{figure/deepnet_eleven.png}}}
\only<12>{\scalebox{0.95}{\includegraphics{figure/deepnet_twelve.png}}}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Why add more layers?}
\begin{itemize}
\item Multiple layers allow for the extraction of more and more abstract
representations.
%\lz
\item Each layer in a feed-forward neural network adds its own degree of non-lnearity to the model.
\end{itemize}
%\lz
\begin{figure}
\centering
\includegraphics[width=10.5cm]{figure/folding}
\caption{An intuitive, geometric explanation of the exponential advantage of deeper networks formally (Mont\'{u}far et al., 2014).}
\end{figure}
\end{vbframe}
%%%%%%%%%%%%%%}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Deep neural networks}

Neural networks today can have %dozens or even 
hundreds of hidden layers. The greater the number of layers, the "deeper" the network. Historically %, however, 
DNNs were very challenging to train and not popular until the late '00s for several reasons:
\lz
\begin{itemize}
\item %For one thing, the 
The use of sigmoid activations (e.g., logistic sigmoid and tanh) significantly slowed down training due to a phenomenon known as \enquote{vanishing gradients}. The introduction of the ReLU activation largely solved this problem.
\item Training DNNs on CPUs was too slow to be practical. Switching over to GPUs cut down training time by more than an order of magnitude.
\item When dataset sizes are small, other models (such as SVMs) and techniques (such as feature engineering) often outperform them. 
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item The availability of large datasets and novel architectures that are capable of handling even complex tensor-shaped data (e.g. CNNs for image data), faster hardware, and better optimization and regularization methods made it feasible to successfully implement deep neural networks.% in the last decade.
\lz

\item An increase in depth often translates to an increase in performance on a given task. State-of-the-art neural networks, however, are much more sophisticated than the simple architectures we have encountered so far.

\lz
%\item 
\end{itemize}
The term "\textbf{deep learning}" encompasses all of these developments and refers to the field as a whole.
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%          REFERENCES          %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}
\frametitle{References}
\footnotesize{
\begin{thebibliography}{99}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibitem[(Mont\'{u}far et al., 2014)]{1} Mont\'{u}farr, G., Pascanu, R., Cho, K., \& Bengio, Y. (2014). \textit{On the Number of Linear Regions of Deep Neural Networks}. 
\end{thebibliography}
}
\end{vbframe}
\endlecture
\end{document}