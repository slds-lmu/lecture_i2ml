\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  }{% Lecture title  
  The Two Cultures of Statistical Modeling
}{% Relative path to title page image: Can be empty but must not start with slides/
}{% Learning goals, wrapped inside itemize environment, can be empty
}


  
% \begin{vbframe}{Modeling: two cultures}
% \begin{center}
% <<fig.height=3, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.5, width = 0.4, height = 0.8, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% @
% \end{center}
% System: nature, organism, chemical reaction, technical process, human behavior, \ldots
% \end{vbframe}
%
% \begin{vbframe}{two cultures: classical statistics}
% We assume that one can sufficiently describe the unknown system by the stochastic model
% $$y = f(x, \theta) + \epsilon$$
% and a \emph{parametric} function class for $f()$ is given. \\
% \lz
% \emph{Under the assumption} that the model is correct, now the usual operation of statistical inference can be built up:
% \begin{itemize}
%  \item Hypothesis tests, variance analysis, model comparison
%  \item Confidence intervals for parameters and predictions
% \end{itemize}
%
% \newpage
% Many articles in JASA, the Annals of Statistics etc. start with
% \enquote{Assume that the data are generated by the following model \ldots}.
%
% \begin{itemize}
% \item \textbf{Advantages:}
%   \begin{itemize}
%   \item Parameter can be interpreted if the model is easy enough
%   \item Good theory for model diagnostics
%   \end{itemize}
%  \item \textbf{Disadvantages (Breiman 2001):}
%    \begin{itemize}
%   \item \enquote{irrelevant theory}
%   \item \enquote{questionable scientific conclusions}
%   \end{itemize}
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Two cultures: machine learning}
% \begin{center}
% <<fig.height=6, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.75, width = 0.4, height = 0.4, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.roundrect(0.5, 0.2, 0.7, 0.3, name = "rr", gp = gpar(lwd = 4))
% grid.text("Approximation by\nneuronal network, tree, ...", y = 0.2,
%           gp = gpar(fontsize = 24, fontface = "bold"))
% grid.curve(0.1, 0.55, 0.15, 0.2,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% grid.curve(0.85, 0.2, 0.9, 0.55,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% @
% \end{center}
% The system is considered as unknown, explicit modelling is not even tried.
% Essential measure of goodness is the prediction quality.
%
% \framebreak
%
% Use of methods which are able to model many systems (universal approximations), development of methods by examples of nature, intuitive behavioral or because of computational attractivity.
% \vspace{0.25cm}
% \begin{itemize}
%  \item \textbf{Advantages:}
%    \begin{itemize}
%    \item Many more model classes are available, much quicker development and implementation of new ideas.
%    \item For predictions, knowledge about the distribution of the parameters,
%    diligent task, knowledge about the error distribution is sufficient.
%    \end{itemize}
%  \item \textbf{Disadvantages:}
%    \begin{itemize}
%    \item Models are mostly hard to interpret (\enquote{black box})
%    \end{itemize}
% \end{itemize}
% \vspace{0.25cm}
% Many models which originally come from machine learning are also statistical mainstream nowadays (and have in parts been considerably improved).


\begin{vbframe}{Modeling: two cultures}
\textbf{Statistics, the Data Modeling Culture}
\begin{center}
\vspace{1cm}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (nature) {nature};
    \node[left=of nature] (x) {x};
    \node[right=of nature] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (nature) edge node {} (y)
    (x) edge node  {} (nature)
    ;
  \end{tikzpicture} \\
\vspace{1cm}
\begin{itemize}
  \item In a strongly simplified world an arbitrary outcome $y$ is produced by \enquote{nature} from the features given in $x$
  \item The knowledge about nature's true mechanisms ranges from entirely unknown (or stochastic) to established (scientific), possibly deterministic explanations
  %\item Example: Outcome $y$ is the rent for apartments and covariates $x$ are size, number of bathrooms and location
\end{itemize}
\end{center}

\framebreak

\begin{itemize}
  \item Focus on the modeling of data, which can be reduced to two targets:
  \begin{enumerate}
    \item Learn a model to predict the outcome for new covariates
    \item Get a better understanding about the relationship between covariates and outcome
  \end{enumerate}
\end{itemize}
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {Logistic Regression, \\Cox Model, \\GEE, \\ \ldots};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      ;
    \end{tikzpicture}
  \end{center}
\begin{itemize}
  \item Find a stochastic model of the data-generating process:
  $$y = f(x, \text{parameters}, \text{random error})$$
\end{itemize}

\framebreak

In this \enquote{data modeling culture}, a stochastic model for the data- generating process is assumed
\begin{blocki}{Typical assumptions and restrictions}
  \item Specific stochastic model that generated the data
  \item Distribution of residuals
  \item Linearity, additivity (e.g. linear predictor)
  \item Manual specification of interactions
\end{blocki}

\framebreak

\textbf{Machine Learning, the Algorithmic Modeling Culture}
\lz
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {unknown};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \node[below=of natur, label=below:algorithm] (algo) {\includegraphics[width = 3cm]{figure_man/machine.png}};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      (x) edge[<->, bend right=30] node {} (algo)
      (y) edge[<->, bend left=30] node {} (algo)
      ;
    \end{tikzpicture}
  \end{center}
\lz
Find a function $\fx$ that minimizes the loss: $\Lxy$

\framebreak

\begin{itemize}
  \item In the \enquote{algorithmic modeling culture}, the true mechanism is treated as unknown
  \item The goal is not finding the true data-generating process but developing an algorithm that imitates/predicts (specific aspects of) a data-generating process as closely as possible
  \item Modeling is reduced to a mere problem of function optimization: Given the covariates $x$, outcome $y$ and a loss function, find a function $\fx$ which minimizes the loss for the prediction of the outcome
\end{itemize}
\begin{blocki}{Algorithm in Machine Learning}
  \item Boosting
  \item Support Vector Machines
  \item Artificial neural networks
  \item Random Forests
  \item Hidden Markov
  \item Bayes-Nets
  \item \ldots
\end{blocki}

\end{vbframe}

\begin{vbframe}{Prediction vs. Interpretation}
\vspace{1.5cm}
\begin{center}
  \begin{tikzpicture}
    \draw (0,0) -- (0,1.5) -- (9.9,0) -- (0,0);
    \draw  (0, 1.6) -- (10,1.6) -- (10, 0.1) -- (0, 1.6);
    \node at (1.5, 0.2) {Interpretability};
    \node at (8, 1.3) {Predictive accuracy};
    \node at (1.3, -0.5) {Tree};
    \node at (9, -0.5) {Random Forest};
    \node at (1.5, -1.2) {Logistic Regression};
    \node at (9, -1.2) {Neural networks};
    \node at (1.3, -1.9) {\ldots};
    \node at (9, -1.9) {\ldots};
  \end{tikzpicture}
\end{center}

\framebreak

\begin{itemize}
  \item There is a trade-off between interpretability and predictive accuracy: models that yield accurate predictions are often complex and models that are easy to interpret are often bad predictors
  \lz
  \item Example logistic regression and $k$ Nearest Neighbors: in LR, we can inspect each coefficient and understand how changes in a single feature affect the class probabilities. kNN offers no such interpretability, but if the class boundaries are very nonlinear, it will have much better predictive accuracy.
\end{itemize}

\end{vbframe}

\begin{frame}{Dimensionality of the data}
\begin{blocki}{}
  \item The higher the dimensionality of the data (\# covariates) the more difficult is the separation of signal and noise
  \item Common practice in data modeling: variable selection (by expert selection or data driven) and reduction of dimensionality (e.g. PCA)
  \item Common practice in algorithmic modeling: Engineering of new features (covariates) to increase predictive accuracy; algorithms robust for many covariates
\end{blocki}

\end{frame}

\begin{vbframe}{Modeling: Two Cultures}

\begin{blocki}{Problems and Blindspots of Data Modeling Culture:}
  \item Conclusions about assumed model are interpreted as being about nature (reification).
  \item Model assumptions often violated.
  \item Often improper model evaluation presuming model validity\\
  $\Rightarrow$ can lead to irrelevant theory and questionable statistical conclusions
  \item Data models fail in areas like image and speech recognition
\end{blocki}

\framebreak

\begin{blocki}{Problems and Blindspots of Algorithmic Modeling Culture:}
  \item Uncertainty quantification often difficult / impossible, almost always an afterthought.
  \item Models are often uninterpretable \enquote{black boxes}: \\
  $\Rightarrow$ Can you trust something you don't understand?
  \item Often ignores suitable sampling plans or issues with data provenance that can jeopardize generalizability
\end{blocki}

% \framebreak
%
% \begin{blocki}{Goodness of model}
%   \item Data modeling culture: Evaluation of goodness of fit often based on model assumptions (e.g. Likelihood) and calculated on training data
%   \item Algorithmic modeling culture: Evaluation of predictive accuracy with an extra test set or cross validation (more later)
% \end{blocki}
%
% How good is a statistical model if the predictive accuracy is weak?\\
% Are interpretations of parameters and p-values legitimate?


\framebreak
Different terminology for machine learning and statistics:

\begin{small}
  \begin{table}
    \begin{tabular}{lr}
      \hline
      Machine Learning & Statistics \\
      \hline
      Feature, Attribute & Covariate \\
      Label & Response \\
      Example, Instance & Observation \\[.3em]

      Weight & Parameter, Coefficient \\
      Bias term & Intercept \\[.3em]

      Minimizing loss & Maximizing likelihood / Estimating posterior \\
      Learning & Fitting, Estimation \\
      Hypothesis & (Fitted) Model \\
      Learner & Model (Class) \\[.3em]

      Supervised Learning & Regression / Classification \\
      Unsupervised Learning & Density estimation / Clustering\\
      Data Mining (good) & Data Mining (bad)\\
    \end{tabular}
  \end{table}
  \end{small}
  {\scriptsize{see also: \url{https://ubc-mds.github.io/resources_pages/terminology}}}

\framebreak

\lz
\structure{Summary}\\

Data modeling culture: \enquote{The model is true.}\\

\emph{Tries to estimate stochastic properties of the true data-generating process and focuses on parameters and their uncertainty.}\\
\vspace{1cm}

Algorithmic modeling culture: \enquote{The model is useful.}\\

\emph{Tries to minimize some measure of divergence between observations from the data-generating process and a function that imitates its behavior and focuses on predictive accuracy.}\\
\vspace{1cm}

These are broad generalizations, there is much overlap and synergy between the two perspectives.

\framebreak

\structure{Rashomon Effect}\\

\lz

\emph{In practice, many different models often describe a given set of data equally well, which makes it difficult to identify a \enquote{true} data-generating process. \\
\lz\lz

In practice, using different loss functions / evaluation schemes will yield different optimal models, which makes it difficult to identify the \enquote{most useful} model.}

\end{vbframe}

\begin{vbframe}{Parameters, Statistics and Supervised Machine Learning}

\begin{itemize}
\item Supervised ML additionally assumes that $f$ is of a certain \enquote{form}
or comes from a certain \emph{class of functions}.\\
This is necessary to make the problem of automatically finding a \enquote{good} model feasible at all.
\item The specific behavior of a mapping from this class can then be described by \textbf{parameters} which defines its shape.
\item Statistics also studies how to learn such functions (or, rather: their parameters) from example data and how to perform inference on them and interpret the results.
\item For historical reasons, statistics is mostly focused on fairly simple classes of mappings, like (generalized) linear models.
\item Supervised ML also includes more complex kinds of mappings that can often deal with more complicated and high-dimensional inputs.
\end{itemize}
\end{vbframe}

\endlecture
\end{document}
