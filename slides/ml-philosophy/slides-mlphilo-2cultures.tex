\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-ml.tex}

%! includes: basics-whatisml, basics-supervised

\lecturechapter{The Two Cultures of Statistical Modeling}
\lecture{Introduction to Machine Learning}

  
% \begin{vbframe}{Modeling: two cultures}
% \begin{center}
% <<fig.height=3, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.5, width = 0.4, height = 0.8, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.5, width = 0.14, height = 0.56, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.5, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% @
% \end{center}
% System: nature, organism, chemical reaction, technical process, human behavior, \ldots
% \end{vbframe}
%
% \begin{vbframe}{two cultures: classical statistics}
% We assume that one can sufficiently describe the unknown system by the stochastic model
% $$y = f(x, \theta) + \epsilon$$
% and a \emph{parametric} function class for $f()$ is given. \\
% \lz
% \emph{Under the assumption} that the model is correct, now the usual operation of statistical inference can be built up:
% \begin{itemize}
%  \item Hypothesis tests, variance analysis, model comparison
%  \item Confidence intervals for parameters and predictions
% \end{itemize}
%
% \newpage
% Many articles in JASA, the Annals of Statistics etc. start with
% \enquote{Assume that the data are generated by the following model \ldots}.
%
% \begin{itemize}
% \item \textbf{Advantages:}
%   \begin{itemize}
%   \item Parameter can be interpreted if the model is easy enough
%   \item Good theory for model diagnostics
%   \end{itemize}
%  \item \textbf{Disadvantages (Breiman 2001):}
%    \begin{itemize}
%   \item \enquote{irrelevant theory}
%   \item \enquote{questionable scientific conclusions}
%   \end{itemize}
% \end{itemize}
% \end{vbframe}
%
% \begin{vbframe}{Two cultures: machine learning}
% \begin{center}
% <<fig.height=6, fig.width=12>>=
% grid.newpage()
% pushViewport(viewport(x = 0.1, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("x", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.5, y = 0.75, width = 0.4, height = 0.4, angle = 0))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("System", gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% pushViewport(viewport(x = 0.9, y = 0.75, width = 0.14, height = 0.28, angle = 45))
% grid.rect(gp = gpar(lwd = 4))
% grid.text("y", rot = -45, gp = gpar(fontsize = 24, fontface = "bold"))
% popViewport()
% grid.lines(c(0.1 + sqrt(2*0.07^2), 0.3), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.lines(c(0.7, 0.9 - sqrt(2*0.07^2)), rep(0.75, 2), arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%            gp = gpar(lwd = 4))
% grid.roundrect(0.5, 0.2, 0.7, 0.3, name = "rr", gp = gpar(lwd = 4))
% grid.text("Approximation by\nneuronal network, tree, ...", y = 0.2,
%           gp = gpar(fontsize = 24, fontface = "bold"))
% grid.curve(0.1, 0.55, 0.15, 0.2,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% grid.curve(0.85, 0.2, 0.9, 0.55,
%    arrow = arrow(length = unit(0.2, "inch"), type = "closed"),
%    gp = gpar(lwd = 4))
% @
% \end{center}
% The system is considered as unknown, explicit modelling is not even tried.
% Essential measure of goodness is the prediction quality.
%
% \framebreak
%
% Use of methods which are able to model many systems (universal approximations), development of methods by examples of nature, intuitive behavioral or because of computational attractivity.
% \vspace{0.25cm}
% \begin{itemize}
%  \item \textbf{Advantages:}
%    \begin{itemize}
%    \item Many more model classes are available, much quicker development and implementation of new ideas.
%    \item For predictions, knowledge about the distribution of the parameters,
%    diligent task, knowledge about the error distribution is sufficient.
%    \end{itemize}
%  \item \textbf{Disadvantages:}
%    \begin{itemize}
%    \item Models are mostly hard to interpret (\enquote{black box})
%    \end{itemize}
% \end{itemize}
% \vspace{0.25cm}
% Many models which originally come from machine learning are also statistical mainstream nowadays (and have in parts been considerably improved).


\begin{vbframe}{Modeling: two cultures}
\textbf{Statistics, the Data Modeling Culture}
\begin{center}
\vspace{1cm}
  \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
      thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
    \node[punkt] (nature) {nature};
    \node[left=of nature] (x) {x};
    \node[right=of nature] (y) {y};
    \path[every node/.style={font=\sffamily\small}]
    (nature) edge node {} (y)
    (x) edge node  {} (nature)
    ;
  \end{tikzpicture} \\
\vspace{1cm}
\begin{itemize}
  \item In a strongly simplified world an arbitrary outcome $y$ is produced by \enquote{nature} from the features given in $x$
  \item The knowledge about nature's true mechanisms ranges from entirely unknown (or stochastic) to established (scientific), possibly deterministic explanations
  %\item Example: Outcome $y$ is the rent for apartments and covariates $x$ are size, number of bathrooms and location
\end{itemize}
\end{center}

\framebreak

\begin{itemize}
  \item Focus on the modeling of data, which can be reduced to two targets:
  \begin{enumerate}
    \item Learn a model to predict the outcome for new covariates
    \item Get a better understanding about the relationship between covariates and outcome
  \end{enumerate}
\end{itemize}
\begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {Logistic Regression, \\Cox Model, \\GEE, \\ \ldots};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      ;
    \end{tikzpicture}
  \end{center}
\begin{itemize}
  \item Find a stochastic model of the data-generating process:
  $$y = f(x, \text{parameters}, \text{random error})$$
\end{itemize}

\framebreak

In this \enquote{data modeling culture}, a stochastic model for the data- generating process is assumed
\begin{blocki}{Typical assumptions and restrictions}
  \item Specific stochastic model that generated the data
  \item Distribution of residuals
  \item Linearity, additivity (e.g. linear predictor)
  \item Manual specification of interactions
\end{blocki}

\framebreak

\textbf{Machine Learning, the Algorithmic Modeling Culture}
\lz
  \begin{center}
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=1cm,
        thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
      \node[punkt] (natur) {unknown};
      \node[left=of natur] (x) {x};
      \node[right=of natur] (y) {y};
      \node[below=of natur, label=below:algorithm] (algo) {\includegraphics[width = 3cm]{figure_man/machine.png}};
      \path[every node/.style={font=\sffamily\small}]
      (natur) edge node {} (y)
      (x) edge node  {} (natur)
      (x) edge[<->, bend right=30] node {} (algo)
      (y) edge[<->, bend left=30] node {} (algo)
      ;
    \end{tikzpicture}
  \end{center}
\lz
Find a function $\fx$ that minimizes the loss: $\Lxy$

\framebreak

\begin{itemize}
  \item In the \enquote{algorithmic modeling culture}, the true mechanism is treated as unknown
  \item The goal is not finding the true data-generating process but developing an algorithm that imitates/predicts (specific aspects of) a data-generating process as closely as possible
  \item Modeling is reduced to a mere problem of function optimization: Given the covariates $x$, outcome $y$ and a loss function, find a function $\fx$ which minimizes the loss for the prediction of the outcome
\end{itemize}
\begin{blocki}{Algorithm in Machine Learning}
  \item Boosting
  \item Support Vector Machines
  \item Artificial neural networks
  \item Random Forests
  \item Hidden Markov
  \item Bayes-Nets
  \item \ldots
\end{blocki}

\end{vbframe}

\begin{vbframe}{Prediction vs. Interpretation}
\vspace{1.5cm}
\begin{center}
  \begin{tikzpicture}
    \draw (0,0) -- (0,1.5) -- (9.9,0) -- (0,0);
    \draw  (0, 1.6) -- (10,1.6) -- (10, 0.1) -- (0, 1.6);
    \node at (1.5, 0.2) {Interpretability};
    \node at (8, 1.3) {Predictive accuracy};
    \node at (1.3, -0.5) {Tree};
    \node at (9, -0.5) {Random Forest};
    \node at (1.5, -1.2) {Logistic Regression};
    \node at (9, -1.2) {Neural networks};
    \node at (1.3, -1.9) {\ldots};
    \node at (9, -1.9) {\ldots};
  \end{tikzpicture}
\end{center}

\framebreak

\begin{itemize}
  \item There is a trade-off between interpretability and predictive accuracy: models that yield accurate predictions are often complex and models that are easy to interpret are often bad predictors
  \lz
  \item Example logistic regression and $k$ Nearest Neighbors: in LR, we can inspect each coefficient and understand how changes in a single feature affect the class probabilities. kNN offers no such interpretability, but if the class boundaries are very nonlinear, it will have much better predictive accuracy.
\end{itemize}

\end{vbframe}

\begin{frame}{Dimensionality of the data}
\begin{blocki}{}
  \item The higher the dimensionality of the data (\# covariates) the more difficult is the separation of signal and noise
  \item Common practice in data modeling: variable selection (by expert selection or data driven) and reduction of dimensionality (e.g. PCA)
  \item Common practice in algorithmic modeling: Engineering of new features (covariates) to increase predictive accuracy; algorithms robust for many covariates
\end{blocki}

\end{frame}

\begin{vbframe}{Modeling: Two Cultures}

\begin{blocki}{Problems and Blindspots of Data Modeling Culture:}
  \item Conclusions about assumed model are interpreted as being about nature (reification).
  \item Model assumptions often violated.
  \item Often improper model evaluation presuming model validity\\
  $\Rightarrow$ can lead to irrelevant theory and questionable statistical conclusions
  \item Data models fail in areas like image and speech recognition
\end{blocki}

\framebreak

\begin{blocki}{Problems and Blindspots of Algorithmic Modeling Culture:}
  \item Uncertainty quantification often difficult / impossible, almost always an afterthought.
  \item Models are often uninterpretable \enquote{black boxes}: \\
  $\Rightarrow$ Can you trust something you don't understand?
  \item Often ignores suitable sampling plans or issues with data provenance that can jeopardize generalizability
\end{blocki}

% \framebreak
%
% \begin{blocki}{Goodness of model}
%   \item Data modeling culture: Evaluation of goodness of fit often based on model assumptions (e.g. Likelihood) and calculated on training data
%   \item Algorithmic modeling culture: Evaluation of predictive accuracy with an extra test set or cross validation (more later)
% \end{blocki}
%
% How good is a statistical model if the predictive accuracy is weak?\\
% Are interpretations of parameters and p-values legitimate?


\framebreak
Different terminology for machine learning and statistics:

\begin{small}
  \begin{table}
    \begin{tabular}{lr}
      \hline
      Machine Learning & Statistics \\
      \hline
      Feature, Attribute & Covariate \\
      Label & Response \\
      Example, Instance & Observation \\[.3em]

      Weight & Parameter, Coefficient \\
      Bias term & Intercept \\[.3em]

      Minimizing loss & Maximizing likelihood / Estimating posterior \\
      Learning & Fitting, Estimation \\
      Hypothesis & (Fitted) Model \\
      Learner & Model (Class) \\[.3em]

      Supervised Learning & Regression / Classification \\
      Unsupervised Learning & Density estimation / Clustering\\
      Data Mining (good) & Data Mining (bad)\\
    \end{tabular}
  \end{table}
  \end{small}
  {\scriptsize{see also: \url{https://ubc-mds.github.io/resources_pages/terminology}}}

\framebreak

\lz
\structure{Summary}\\

Data modeling culture: \enquote{The model is true.}\\

\emph{Tries to estimate stochastic properties of the true data-generating process and focuses on parameters and their uncertainty.}\\
\vspace{1cm}

Algorithmic modeling culture: \enquote{The model is useful.}\\

\emph{Tries to minimize some measure of divergence between observations from the data-generating process and a function that imitates its behavior and focuses on predictive accuracy.}\\
\vspace{1cm}

These are broad generalizations, there is much overlap and synergy between the two perspectives.

\framebreak

\structure{Rashomon Effect}\\

\lz

\emph{In practice, many different models often describe a given set of data equally well, which makes it difficult to identify a \enquote{true} data-generating process. \\
\lz\lz

In practice, using different loss functions / evaluation schemes will yield different optimal models, which makes it difficult to identify the \enquote{most useful} model.}

\end{vbframe}


\endlecture
\end{document}
