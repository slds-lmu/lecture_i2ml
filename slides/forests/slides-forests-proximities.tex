\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest
  }{% Lecture title
  Proximities
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/forest-prox-vis_1.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand how RF can be used to define proximities of observations
  \item Know how proximities can be used for visualization, outlier detection and imputation
}

\begin{vbframe}{Proximities}
RFs have built-in similarity measure for pairs of observations:

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2e20bede50d_0_0
\includegraphics[width=1\textwidth]{figure_man/forest-proximities.png}
\end{center}

\begin{itemize}
  \item After training, push all observations through each tree
  \item To calculate $\operatorname{prox}\left(\xi, \xi[j]\right)$:
    Percentage of how often both points are placed in \textbf{same terminal node of a tree}
  \item Here: $\operatorname{prox}\left(\xi[1], \xi[2]\right) = 2/3$
  \item All proximities are arranged in symmetric $n \times n$ matrix
\end{itemize}

\end{vbframe}


\begin{vbframe}{Visualizing Proximities}

\vspace{-5ex}
\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure_man/forest-prox-matrix.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/forest-prox-vis_1.png}
\end{minipage}
\end{figure}
\vspace{-3ex}
% {\small

\lz


Can visualize the proximity matrix by projecting it into lower-dim. space, e.g., via multidim. scaling
(might have to turn proximities into distances)
\begin{itemize}
  \item Samples from same class usually form \textbf{identifiable clusters}
    % , which suggests that they share a common structure.
  \item \textbf{Offers some error-inspection}, e.g., Adelie has high within-class variance and has overlaps with other classes
\end{itemize}
% }

\end{vbframe}

\begin{vbframe}{Outlier Detection}

\begin{itemize}
\item Can also be used to \textbf{locate outliers}
\item Or mislabeled points, especially in manually labeled data sets
\end{itemize}

\begin{center}
\includegraphics[width=0.55\textwidth]{figure/forest-prox-vis_2.png}
\end{center}

\end{vbframe}


\begin{frame}{Imputing missing data}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit
\only<1>{\includegraphics[width=400pt]{figure_man/forest-missing-value_1.png}}
\only<2>{\includegraphics[width=400pt]{figure_man/forest-missing-value_2.png}}
\only<3>{\includegraphics[width=400pt]{figure_man/forest-missing-value_3.png}}
\end{center}
\begin{enumerate}
\item Replace missings per feature by median (of available values)
\item Compute proximities (NB: data has changed)
\item Replace missings in $\xi$ by weighted average of non-missings; weights proportional to proximities
  % between observation $\xi$ and the observations with the non-missing values
\end{enumerate}

\lz

Steps 2 and 3 are iterated a few times. % (5 to 6 times)
\end{frame}

\endlecture
\end{document}
