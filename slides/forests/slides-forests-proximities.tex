\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.


% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...








% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

%! includes: forests-intro

\lecturechapter{Random Forests: Proximities}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Random Forest Proximities}
\begin{itemize}
\item One of the most useful tools in random forests
\item A measure of similarity ("closeness" or "nearness") of observations derived from random forests
\item Can be calculated for each pair of observations
\item Definition:
\begin{itemize}
\item The proximity between two observations $\xi$ and $\xi[j]$ is calculated by measuring the number of times that these two observations are placed in the same terminal node of the same tree of the random forest, divided by the number of trees in the forest
\item The proximity of observations $\xi$ and $\xi[j]$ can be written as $\operatorname{prox}\left(\xi, \xi[j]\right)$
\item The proximities form an intrinsic similarity measure between pairs of observations
\end{itemize}
\item The proximities of all observations form a symmetric $n \times n$ matrix.
\framebreak 
% 
% \item With large datasets, it is not possible to fit a $n \times n$ matrix into fast memory
% \begin{itemize}
% \item A modification is needed - reduce the required memory size to $n \times M$, where $M$ is the number of trees in the forest
% \end{itemize}

\item Algorithm:
\begin{itemize}
\item Once a random forest has been trained, all of the training data is put through each tree (both in- and out-of-bag).
\item Every time two observations $\xi$ and $\xi[j]$ end up in the same terminal node of a tree, their proximity is increased by one. 
\item Once all data has been put through all trees and the proximities have been counted, the proximities are normalized by dividing them by the number of trees.
\end{itemize}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Using Random Forest Proximities}

\begin{itemize}
\item Imputing missing data:
\begin{enumerate}
\item Replace missing values for a given variable using the median of the non-missing values
\item Get proximities
\item Replace missing values in observation $\xi$ by a weighted average of non-missing values, with weights proportional to the proximity between observation $\xi$ and the observations with the non-missing values
\end{enumerate}
Steps 2 and 3 are then iterated a few times. %(5 to 6 times)

\item Locating outliers: 
\begin{itemize}
\item An outlier is an observation whose proximities to all other observations are small
\item Measure of outlyingness can be computed for each observation in the training sample
\item If the measure is unusually large, the observation should be carefully inspected
\end{itemize}

\framebreak

\item Identifying mislabeled data:
\begin{itemize}
\item Instances in the training data set are sometimes labeled ambiguously or incorrectly, especially in \enquote{manually} created data sets.
\item Proximities can help in finding them: they often show up as outliers in terms of their proximity values. 
\end{itemize}


\item Visualizing the forest: 
\begin{itemize}
\item The values $1 - \operatorname{prox}\left(\xi, \xi[j]\right)$ can be thought of as distances in a high-dimensional space
\item They can be projected onto a low-dimensional space using metric multidimensional scaling (MDS)
\item Metric multidimensional scaling uses eigenvectors of a modified version of the proximity matrix to get scaling coordinates
\end{itemize}
\end{itemize}

\framebreak

\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/Proximity_plot.png}
\end{center}
\tiny image from G. Louppe (2014) \emph{Understanding Random Forests} \texttt{arXiv:1407.7502}. 
\framebreak

\begin{itemize}
\item \normalsize The figure depicts the proximity matrix learnt for a 10-class handwritten digit classification task
\begin{itemize}
\item proximity matrix distances projected onto the plane using multidimensional scaling
\item samples from the same class form identifiable clusters, which suggests that they share a common structure
\item also shows the fact for which classes errors occur, e.g.,  digits 1 and 8 have high within-class variance and have overlaps with other classes 
\end{itemize}
\end{itemize}
\end{vbframe}

\endlecture
\end{document}
