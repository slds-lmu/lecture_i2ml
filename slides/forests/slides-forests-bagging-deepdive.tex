\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/rf_majvot_averaging.png}
\newcommand{\learninggoals}{
\item Understand the theoretical foundation behind bagging
\item Know the mathematical justification for of bagging improvements}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}


\lecturechapter{Random Forests: Bagging Deep Dive}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{The Bagging Algorithm}

\begin{algorithm}[H]
  \small
  \caption*{Bagging algorithm}
  \begin{algorithmic}[1]
    \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
    \For {$m = 1 \to M$}
      \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
      \State Train base learner on $\D^{[m]}$ to obtain model $\bl$
    \EndFor
    \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
    \begin{align*}
    \fM &= \frac{1}{M} \sum_{m=1}^M \bl \\
    \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\bl = k\right)
    \end{align*}
  \end{algorithmic}
\end{algorithm}

\end{vbframe}

\begin{vbframe}{Why/when does Bagging help?}

\begin{scriptsize}
 Assume we use quadratic loss and measure instability of the ensemble with $\ambifM = \tfrac{1}{M}\sum^M_{m} \left(\bl- \fM \right)^2$:
 \vskip -2em
 \begin{align*}
 \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\bl- \fM\right)^2 \\
         &= \tfrac{1}{M}\sum^M_{m} \left(\left(\bl - y\right)  + \left(y - \fM\right)\right)^2\\
         &= \tfrac{1}{M}\sum^M_{m} L(y, \bl) + L(y, \fM) \underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m=1}\bl\right)\left(y - \fM\right)}_{- 2 L\left(y, \fM\right)} \\[-.5em] \intertext{So, if we take the expected value over the data's distribution:}
         \Exy\left[L\left(y, \fM\right)\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right] - \Exy\left[\ambifM\right] \end{align*}
\end{scriptsize}
\small
$\Rightarrow$ The expected loss of the ensemble is lower than the average loss of the single base learner by the amount of instability in the ensemble's base learners.\\ The more accurate and diverse the base learners, the better.
\normalsize
\framebreak
\end{vbframe}

\begin{vbframe}{Improving Bagging}
How to make $\Exy\left[\ambifM\right]$ as large as possible?
\begin{scriptsize}
\begin{align*}
\Exy\left[L\left(y, \fM \right)\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right] - \Exy\left[\ambifM\right] \\
\shortintertext{Assume $\Exy\left[\bl\right] = 0$ for simplicity, $\var_{xy}\left[\bl\right] = \Exy\left[(\bl)^2\right] = \sigma^2$, $\corr_{xy}\left[\bl, \bl{m'}\right] = \rho$ for all $m, m'$.}
\implies 
\var_{xy}\left[\fM\right] &= \tfrac{1}{M} \sigma^2 +  \tfrac{M-1}{M} \rho \sigma^2 \qquad\left(... = \Exy\left[(\fM)^2\right]\right)\\
 \Exy\left[\ambifM\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[\left(\bl- \fM\right)^2\right]\\
 & = \tfrac{1}{M}\left(M \Exy\left[(\bl)^2\right] + M \Exy\left[(\fM)^2\right] - 
     2 M \Exy\left[\bl\fM\right]\right) \\
  & = \sigma^2  + \Exy\left[(\fM)^2\right] - 2 \tfrac{1}{M}\sum^M_{m'} \underbrace{\Exy\left[\bl \bl{m'} \right]}_{\mathclap{\qquad\qquad\qquad\qquad= \cov_{xy}\left[\bl, \bl{m'} \right] + \Exy\left[\bl\right]\Exy\left[\bl{m'}\right]}} \\
  &=  \sigma^2  + \left(\tfrac{1}{M} \sigma^2 +   \tfrac{M-1}{M} \rho \sigma^2\right) - 2\left(\tfrac{M-1}{M} \rho\sigma^2 + \tfrac{1}{M}\sigma^2 + 0 \cdot 0 \right)\\
  &= \tfrac{M-1}{M} \sigma^2 (1-\rho)
\end{align*}
\end{scriptsize}

\begin{small}
\begin{align*}
\Exy\left[L\left(y, \fM\right)\right] &= \textcolor{blue}{\tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \bl \right)\right]} - \Exy\left[\ambifM\right]\\
\Exy\left[\ambifM\right] &\cong 
\textcolor{purple}{\frac{M-1}{M}} \textcolor{cyan}{\var_{xy}\left[\bl\right]} \textcolor{violet}{\left(1 - \corr_{xy}\left[\bl, \bl{m'}\right]\right)}
\end{align*}
\end{small}
\begin{itemize}
\item[$\Rightarrow$] \textcolor{blue}{\textbf{better base learners}} are better {\small (... duh)}
\item[$\Rightarrow$] \textcolor{purple}{\textbf{more base learners}} are better {\small (theoretically, at least...)}\\
\item[$\Rightarrow$] \textcolor{cyan}{\textbf{more variable base learners}} are better {\small(as long as their risk stays the same, of course!)}
\item[$\Rightarrow$] \textcolor{violet}{\textbf{less correlation between base learners}} is better:\\ bagging helps more if base learners are wrong in different ways so that their errors \enquote{cancel} each other out.\\
\end{itemize}


\end{vbframe}


\endlecture
\end{document}
