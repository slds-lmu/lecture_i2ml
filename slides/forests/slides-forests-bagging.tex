\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\newcommand{\titlefigure}{figure_man/rf_majvot_averaging.png}
\newcommand{\learninggoals}{
\item Understand the basic idea of bagging
\item Be able to explain the connection of bagging and bootstrap
\item Understand how a prediction is computed for bagging
\item Understand why bagging improves the predictive power}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}


\lecturechapter{Random Forests: Bagging Ensembles}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{Bagging}

\begin{itemize}
  \item Bagging is short for \textbf{B}ootstrap \textbf{Agg}regation.
  \item It's an \textbf{ensemble method}, i.e., it combines many models into one 
        big \enquote{meta-model}
  \item Such model ensembles often work much better than their members alone would.
  \item The constituent models of an ensemble are called \textbf{base learners} 
      % that improves instable / high variance learners by variance smoothing
\end{itemize}

\framebreak 
In a \textbf{bagging} ensemble, all base learners are of the same type. The only difference between the models is the data they are trained on.\\
Specifically, we train base learners $\blm, m = 1, \dots, M$ on $M$ \textbf{bootstrap} samples of training data $\D$:
\begin{itemize}
  \item Draw $n$ observations from $\D$ with replacement
  \item Fit the base learner on each of the $M$ bootstrap samples to get models $\fh(x) = \blmh, m = 1, \dots, M$
\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1xodP6ayu1Gay6mMKgzVWYEFmSoeG5kNuqsaTkFFmd78/edit
\includegraphics[width=0.55\textwidth]{figure_man/bagging.pdf}
\end{center}

\framebreak

\textbf{Aggregate} the predictions of the $M$ fitted base learners to get the
\textbf{ensemble model} $\fMh$:
  \begin{itemize}
    \item Aggregate via averaging (regression) or majority voting (classification)
    \item Posterior class probabilities $\pikxh$ can be estimated by calculating predicted class frequencies over the ensemble
  \end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width=0.6\textwidth]{figure_man/rf_majvot_averaging.png}
\end{center}
\end{vbframe}

% \begin{algorithm}[H]
%   \small
%   \setstretch{1.15}
%   \caption*{Bagging algorithm}
%   \begin{algorithmic}[1]
%     \State {\bf Input: } Dataset $\D$, base learner, number of bootstraps $M$
%     \For {$m = 1 \to M$}
%       \State Draw a bootstrap sample $\D^{[m]}$ from $\D$.
%       \State Train base learner on $\D^{[m]}$ to obtain model $\blm$
%     \EndFor
%     \State Aggregate the predictions of the $M$ estimators (via averaging or majority voting), to determine the bagging estimator:
%     \begin{align*}
%     \fM &= \frac{1}{M} \sum_{m=1}^M \blm \\
%     \text{or}\quad \fM &= \argmax_{k \in \Yspace} \sum_{m=1}^M \I\left(\blm = k\right)
%     \end{align*}
%   \end{algorithmic}
% \end{algorithm}

\begin{vbframe}{Why/when does Bagging help?}

In one sentence:\\
\lz

Because the variability of the average of the predictions of many base learner models is smaller than the variability of the predictions from one such base learner model.\\

If the error of a base learner model is mostly due to (random) variability and not due to structural reasons, combining many such base learners by bagging helps reducing this variability.


\framebreak
\begin{scriptsize}
 Assume we use quadratic loss and measure instability of the ensemble with $\ambifM = \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM \right)^2$:
 \vskip -2em
 \begin{align*}
 \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
         &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
         &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fM) \underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m=1}\blm\right)\left(y - \fM\right)}_{- 2 L\left(y, \fM\right)} \\[-.5em] \intertext{So, if we take the expected value over the data's distribution:}
         \Exy\left[L\left(y, \fM\right)\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \blm \right)\right] - \Exy\left[\ambifM\right] \end{align*}
\end{scriptsize}
\small
$\Rightarrow$ The expected loss of the ensemble is lower than the average loss of the single base learner by the amount of instability in the ensemble's base learners.\\ The more accurate and diverse the base learners, the better.
\normalsize
\framebreak
\end{vbframe}

\begin{vbframe}{Improving Bagging}
\begin{scriptsize}
\begin{align*}
\shortintertext{How to make $\Exy\left[\ambifM\right]$ as large as possible?}
\Exy\left[L\left(y, \fM \right)\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \blm \right)\right] - \Exy\left[\ambifM\right] \\
\shortintertext{Assume $\Exy\left[\blm\right] = 0$ for simplicity, $\var_{xy}\left[\blm\right] = \Exy\left[(\blm)^2\right] = \sigma^2$, $\corr_{xy}\left[\blm, \bl{m'}\right] = \rho$ for all $m, m'$.}
\implies 
\var_{xy}\left[\fM\right] &= \tfrac{1}{M} \sigma^2 +  \tfrac{M-1}{M} \rho \sigma^2 \qquad\left(... = \Exy\left[(\fM)^2\right]\right)\\
 \Exy\left[\ambifM\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[\left(\blm- \fM\right)^2\right]\\
 & = \tfrac{1}{M}\left(M \Exy\left[(\blm)^2\right] + M \Exy\left[(\fM)^2\right] - 
     2 M \Exy\left[\blm\fM\right]\right) \\
  & = \sigma^2  + \Exy\left[(\fM)^2\right] - 2 \tfrac{1}{M}\sum^M_{m'} \underbrace{\Exy\left[\blm \bl{m'} \right]}_{\mathclap{\qquad\qquad\qquad\qquad= \cov_{xy}\left[\blm, \bl{m'} \right] + \Exy\left[\blm\right]\Exy\left[\bl{m'}\right]}} \\
  &=  \sigma^2  + \left(\tfrac{1}{M} \sigma^2 +   \tfrac{M-1}{M} \rho \sigma^2\right) - 2\left(\tfrac{M-1}{M} \rho\sigma^2 + \tfrac{1}{M}\sigma^2 + 0 \cdot 0 \right)\\
  &= \tfrac{M-1}{M} \sigma^2 (1-\rho)
\end{align*}
\end{scriptsize}

\begin{small}
\begin{align*}
\Exy\left[L\left(y, \fM\right)\right] &= \textcolor{blue}{\tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \blm \right)\right]} - \Exy\left[\ambifM\right]\\
\Exy\left[\ambifM\right] &\cong 
\textcolor{purple}{\frac{M-1}{M}} \textcolor{cyan}{\var_{xy}\left[\blm\right]} \textcolor{violet}{\left(1 - \corr_{xy}\left[\blm, \bl{m'}\right]\right)}
\end{align*}
\end{small}
\begin{itemize}
\item[$\Rightarrow$] \textcolor{blue}{\textbf{better base learners}} are better {\small (... duh)}
\item[$\Rightarrow$] \textcolor{purple}{\textbf{more base learners}} are better {\small (theoretically, at least...)}\\
\item[$\Rightarrow$] \textcolor{cyan}{\textbf{more variable base learners}} are better {\small(as long as their risk stays the same, of course!)}
\item[$\Rightarrow$] \textcolor{violet}{\textbf{less correlation between base learners}} is better:\\ bagging helps more if base learners are wrong in different ways so that their errors \enquote{cancel} each other out.\\
\end{itemize}


\end{vbframe}

\begin{vbframe}{Bagging: Synopsis}

  \begin{itemize}
    \item Basic idea: fit the same model repeatedly on many \textbf{bootstrap} replications of the training data set and \textbf{aggregate} the results
    \item Gains performance by reducing the variance of predictions, but (slightly) increases the bias: it reuses training data many times, so small mistakes can get amplified. 
    \item Works best for unstable/high-variance base learners, where small changes in the training set can cause large changes in predictions:\\
    e.g., CART, neural networks, step-wise/forward/backward variable selection for regression\\
     \item Works best if base learners' predictions are only weakly correlated: they don't all make the same mistakes.
         \item Can degrade performance for stable methods like $k$-NN, LDA, Naive Bayes, linear regression
  \end{itemize}
\end{vbframe}

\endlecture
\end{document}
