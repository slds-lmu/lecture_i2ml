\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
\input{../../latex-math/ml-ensembles.tex}
\input{../../latex-math/ml-eval.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Random Forest
  }{% Lecture title
  Feature Importance
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/forest-fimp_perm.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand that the goal of feature importance is to enhance interpretability of RF
  \item Understand FI based on feature permutation
  \item Understand FI based on improvement in splits
}

\begin{vbframe}{PErmutation Feature Importance}

  \begin{footnotesize}

RFs improve accuracy by aggregating multiple decision trees but \textbf{lose interpretability} compared to a single tree. \textbf{Feature importance} mitigates this problem.
\begin{itemize}
  \item How much does performance \textit{decrease}, if feature is removed / rendered useless?
  \item We permute values of considered feature
  \item Removes association between feature and target, keeps marginal distribution
  \item Can obtain $\GEh$ of RF (without and with permuted features)
    by predicting OOB data, to \textbf{efficiently compute FI during training}
  \item Avoids not only new models (if feature would be removed) but can already use ``OOB test data''
    during training
\end{itemize}
\end{footnotesize}

\vspace{-1ex}
\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2ddcfb537bb_0_33
\includegraphics[width=0.7\textwidth]{figure_man/forest-fimp_idea.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Permutation importance}

\includegraphics[width = 0.95\textwidth]{figure_man/forest-permutation-imp.png}
\vspace{-1.2ex}
\begin{algorithm}[H]
\footnotesize
% \caption*{Measure based on permutations of OOB obs.}
\begin{algorithmic}[1]
  \State Calculate $\GEh_{\oob}$ using set-based metric $\rho$
  \For{features $x_j$, $j = 1 \to p$}
  \For{Some statistical repetitions}
    \State {Distort feature-target relation: permute $x_j$ with $\psi_j$}
	  \State Compute all $n$ OOB-predictions for permuted feature data, obtain all $\fh^{(i)}_{\oob,\psi_j}$
    \State Arrange predictions in $\bm{\hat{F}}_{\oob,\psi_j}$;
    Compute $\GEh_{\oob, j} = \rho(\yv, \bm{\hat{F}}_{\oob,\psi_j})$
  \State {Estimate importance of $j$-th feature: $\widehat{\text{FI}_j} = \GEh_{\oob,j} - \GEh_{\oob
  } $}
  \EndFor
  \State Average obtained $\widehat{\text{FI}_j}$ values over reps
  \EndFor
\end{algorithmic}
\end{algorithm}
\vspace{-3ex}
\end{vbframe}

\begin{vbframe}{Impurity importance}

{\small Alternative: Add up all \textit{improvements} in splits where feature $x_j$ is used.}

\vspace{-1ex}
\begin{center}
% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.g2ddcfb537bb_0_33
\includegraphics[width=0.8\textwidth]{figure_man/forest-fimp_impurity.png}
\end{center}

% \vspace{-3ex}
\begin{algorithm}[H]
\small
% \caption*{Measure based on improvement in split criterion}
\begin{algorithmic}[1]
  \For{features $x_j$, $j = 1 \to p$}
  \For{all models $\blh$, $m = 1 \to M$}
  \State {Find all splits in $\blh$ on $x_j$}
  \State {Extract improvement / risk reduction for these splits}
  \State {Sum them up}
  \EndFor
  \State {Add up improvements over all trees for FI of $x_j$}
  \EndFor
\end{algorithmic}
\end{algorithm}

\end{vbframe}

\begin{vbframe}{In practice / Outlook}

{\small
  Let's compare both FI variants on \texttt{mtcars}:
}
% \vspace{-1ex}
\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/forest-fimp_gini.png}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\textwidth]{figure/forest-fimp_perm.png}
\end{minipage}
\end{figure}
% \vspace{-1ex}


\lz
\begin{itemize}

  \item Both methods are \textbf{biased toward features with more levels} (i.e., continuous or categoricals with many categories) \citelink{STROBL2007}
  \item More advanced versions exist
  \item PFI and FI have been generalized, see our lecture on IML!

\end{itemize}
\end{vbframe}

\endlecture
\end{document}
