\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-ensembles.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
Random Forest
}{% Lecture title
Basics
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/forest-gpt4o.jpg
}{% Learning goals, wrapped inside itemize environment
  \item Know how random forests are defined by extending the idea of bagging
  \item Understand general idea to decorrelate trees
  \item Understand effects of hyperparameters
  \item RFs and overfitting
}

\begin{framev}{Motivation}

CARTs offer several appealing features:

\begin{itemize}
  \item \textbf{Interpretability}: Easy to understand and explain
  \item \textbf{Invariance to rank-preserving transformations}: \\
    E.g., unaffected by scaling or shifting of features
  \item \textbf{Versatility}: Work on categorical and numerical data
  \item \textbf{Robustness to missing values}: Can work with missings
\end{itemize}

\lz \lz

Despite these benefits, CARTs are not without drawbacks:

\lz

\furtherreading{HASTIE2001}
\begin{quotation}
"Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely inaccuracy."
\end{quotation}

% \vspace{1em}
% Bagging and some clever tricks create the random forest (RF), which \textbf{sacrifices interpretability} for significant performance gains.
\end{framev}

\begin{framei}{random forests \furtherreading{BREIMAN2001}}

% An extension of bagging creates the random forest (RF), which sacrifices interpretability for performance gains:

\item RFs use \textbf{bagging with CARTs as BLs} %$\bl$ applied to bootstrap samples of the data.
\item \textbf{Random feature sampling} decorrelates the base learners
% \item Since bootstrap samples are largely similar, the trained base learners $\blh$ tend to be correlated.
\item \textbf{Fully expanded trees} further increase variability of trees

\vfill

\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/forest.png}
\end{center}

\end{framei}


\begin{framei}{Intuition behind decorrelation}
  \item Since bootstrap samples are similar, models $\blh$ are correlated, affecting the variance of an ensemble $\fh$
  \item We would like variance to go down linearly with ensemble size, but because of correlation we cannot really expect that
  \item Assuming $\var(\blh) = \sigma^2$, $\text{Corr}(\blh, \blh[j]) = \rho$, semi-formal analysis,
    without proper analysis of prediction error:
\begin{align*}
\var\left(\fh\right) &= \var\left(\frac{1}{M} \sum_{m=1}^{M} \blh \right) = \frac{1}{M^2} \left( \sum_{m=1}^{M} \var(\blh) + 2 \sum_{m < j} \cov(\blh, \blh[j]) \right) \\
&= \frac{1}{M^2} \left( M \sigma^2 + 2 \frac{M(M-1)}{2}\rho \sigma^2 \right) = (1-\rho)\frac{\sigma^2}{M} + \rho \sigma^2
\end{align*}

\item Ensemble variance is ``convex-combo of linear-reduction and no-reduction, controlled by $\rho$''
\item Maybe we can decorrelate trees, to reduce ensemble variance? And get less prediction error?
\end{framei}

\begin{framev}{Random feature sampling}

RFs decorrelate trees with a simple randomization:

\begin{itemize}
  \item For each node of tree, randomly draw $\texttt{mtry} \le p$ features \\
    ($\texttt{mtry}$ = name in some implementations)
  \item Only consider these features for finding the best split
  \item Careful: Our previous analysis was simplified! The more we decorrelate by this,
    the more random the trees become!
    \\This also has negative effects!
\end{itemize}

% SOURCE: https://docs.google.com/presentation/d/1lDW9HEeNEGwJR1tmhAiOhySibzl6tXLxqePsKtpbb_M/edit#slide=id.p
\begin{center}
\includegraphics[width=0.55\textwidth]{figure_man/forest-feature-sampling.png}
\end{center}

% \lz

% $\texttt{mtry}$ is a hyperparameter we set before training the RF

\end{framev}

\begin{framev}{Effect of feature sampling}

\begin{center}
\includegraphics[width=1\textwidth]{figure/forest-mtry.png}
\end{center}

\begin{itemize}

\item Optimal $\texttt{mtry}$ typically larger for regression than for classification
\item Good defaults exist, but still most relevant tuning param
\item Rule of thumb:
\begin{itemize}
\item Classification: $\texttt{mtry}$ $ = \lfloor \sqrt{p} \rfloor$
\item Regression: $\texttt{mtry}$ $ = \lfloor p/3 \rfloor$
\end{itemize}
\end{itemize}

\end{framev}

\begin{framev}{Tree Size}

In addition to $\texttt{mtry}$, RFs have two other important HPs:

\begin{itemize}
  \item Min. nr. of obs. in each decision tree node \\
    Default (ranger): $\texttt{min.node.size = 5}$ \furtherreading{BREIMAN2001}
\begin{center}
\includegraphics[width=0.8\textwidth]{figure/forest-minnode.png}
\end{center}
  \item Depth of each tree \\
    Default (ranger): $\texttt{maxDepth} = \infty$
  \item There are more alternative HPs to control depth of tree:\\
    minimal risk reduction, size of terminal nodes, etc.
\end{itemize}

\lz

RF usually use fully expanded trees, without aggressive early stopping or pruning, to further \textbf{increase variability of each tree}. \furtherreading{LOUPPE2015}
\end{framev}

\begin{framei}{Ensemble size}
\item RFs usually better if ensemble is large \furtherreading{BREIMAN2001}
\item But: Increases computational costs, and diminishing returns
\item $100$ or $500$ is a sensible default
\item Can also inspect the OOB error (see later)

\vfill


\begin{center}
\includegraphics[width=330pt]{figure/forest-ntree.png}
\end{center}
\end{framei}

% TODO hier eigentlich mit vistool, leider funktioniert da der contour plot bei mir nicht

\begin{framev}{Effect of ensemble size}

  {\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_1}}
\end{framev}

\begin{framev}{Effect of ensemble size}
\addtocounter{framenumber}{-1}
{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_2}}
\end{framev}

\begin{framev}{Effect of ensemble size}
\addtocounter{framenumber}{-1}
{\centering \includegraphics[width=0.95\textwidth]{figure/cart_forest_intro_3}}
\end{framev}

\begin{framev}{Can RF overfit? \furtherreading{Probst2018}}

{\small
\begin{itemize}
  \item Just like any other learner, RFs \textbf{can} overfit!\\
  \item However, RFs generally \textbf{less} prone to overfitting than individual CARTs.
  \item Overly complex trees can \textit{still} lead to overfitting!\\
    If most trees capture noise, so does the RF.
  \item But randomization and averaging helps.
\end{itemize}

% \begin{center}
% \includegraphics[width=0.56\textwidth]{figure/forest-overfit.png}
% \end{center}
\begin{center}
\includegraphics[width=330pt]{figure/forest-ntree.png}
\end{center}

% \lz lz

Since each tree is trained \textit{individually and without knowledge of previously trained trees}, increasing $\texttt{ntrees}$ generally reduces variance \textbf{without increasing the chance of overfitting!}
}
\end{framev}

\begin{framev}{RF in practice}
\small{
Benchmarking bagged ensembles with 100 BLs each on $\texttt{spam}$
versus RF ($\texttt{ntrees} = 100$, $\texttt{mtry} = \sqrt{p}$, $\texttt{minnode} = 1$),
we see how well RF performs!
}

\begin{center}
\includegraphics[clip=true, trim={0 35 0 30}, width=0.85\linewidth]{figure/bagging-bench_RF.png}
\end{center}

{\footnotesize $\Rightarrow$ RFs combine the benefits of random feature selection and fully expanded trees.}

\end{framev}

\begin{framev}{Discussion}

Advantages:
\begin{itemize}
  \item Most advantages of trees also apply to RF: not much preprocessing required, missing value handling, etc.
  \item Easy to parallelize
  \item Often work well (enough)
  \item Works well on high-dimensional data
  \item Works well on data with irrelevant \enquote{noise} variables
\end{itemize}

\lz

Disadvantages:
\begin{itemize}
  % \item Often sub-optimal for regression
  \item Same extrapolation problem as for trees
  \item Harder to interpret than trees \\
    (but many extra tools are nowadays
    available for interpreting RFs)
  \item Implementation can be memory-hungry
  \item Prediction can be computationally demanding for large ensembles
\end{itemize}

\end{framev}

\endlecture
\end{document}
