% Introduction to Machine Learning
% Day 4

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../../style/preamble.Rnw")
library(methods)
library(rpart)
library(rpart.plot)
library(randomForest)
library(rattle)
library(smoof)
@

% Load all R packages and set up knitr
<<setup, child="../../style/setup.Rnw", include = FALSE>>=
@

%! includes: classification-knn, performance-*, forests-intro, forests-bagging

\lecturechapter{Random Forest: Benchmarking Trees, Forests, and Bagging K-NN}
\lecture{Introduction to Machine Learning}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) CART vs. (bagged) k-NN}

  \begin{itemize}
    \item Goal: Compare performance of random forest against (bagged) stable and (bagged) unstable methods
    \item Algorithms:
    \begin{itemize}
      \item classification tree (CART, implemented in \code{rpart}, \code{max.depth}: 30, \code{min.split}: 20, \code{cp}: 0.01)
      \item bagged classification tree using 50 bagging iterations (\code{bagged.rpart})
      \item k-nearest neighbors (k-NN, implemented in \code{kknn}, $k=7$)
      \item bagged k-nearest neighbors using 50 bagging iterations (\code{bagged.knn})
      \item random forest with 50 trees (implemented in \code{randomForest})
    \end{itemize}
    \item Method to evaluate performance: 10-fold cross-validation
    \item Performance measure: mean missclassification error on test sets
    \end{itemize}

    \framebreak

    \begin{itemize}
    \item Datasets from \pkg{mlbench}:
    \end{itemize}

\begin{table}
\footnotesize
\begin{tabular}{p{1.5cm}p{2cm}p{0.5cm}p{0.5cm}p{5cm}}
Name & Kind of data &  n & p & Task\\
\hline
Glass & Glass identification data & 214 & 10 & Predict the type of glass (6 levels) on the basis of the chemical analysis of the glasses represented by the 10 features\\
Ionosphere & Radar data & 351 & 35 & Predict whether the radar returns show evidence of some type of structure in the ionosphere (\enquote{good}) or not (\enquote{bad}) \\
Sonar & Sonar data & 208 & 61 & Discriminate between sonar signals bounced off a metal cylinder (\enquote{M}) and those bounced off a cylindrical rock (\enquote{R})\\
Waveform & Artificial data & 100 & 21 & Simulated 3-class problem which is considered to be a difficult pattern recognition problem. Each class is generated by the waveform generator.\\
\hline
\end{tabular}
\end{table}

\framebreak

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width=0.95\textwidth]{figure_man/bm_stable_vs_unstable.pdf}\end{center}{}

%\framebreak

\end{vbframe}

\begin{vbframe}{Benchmark: Random Forest vs. (bagged) CART vs. (bagged) k-NN}

  Bagging k-NN does not improve performance because:

  \begin{itemize}
    \item k-NN is stable w.r.t. perturbations
    \item In a 2-class problem, nearest neighbor based classification only changes under bagging if both
    \begin{itemize}
    \item the nearest neighbor in the learning set is \textbf{not} in at least half of the bootstrap samples, but the probability that any given observation is in the bootstrap sample is 63\% which is greater than 50\%,
    \item and, simultaneously, the \emph{new} nearest neighbor(s) all have a different label than the missing nearest neighbor in those bootstrap samples, which is unlikely for most regions of $\Xspace \times \Yspace$.
    \end{itemize}
\end{itemize}
\end{vbframe}


\endlecture
