\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Supervised Regression
  }{% Lecture title  
  Linear Models with $L2$ Loss
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/reg_l2_residual.pdf
}{% Learning goals, wrapped inside itemize environment
  \item Grasp the overall concept of linear regression
  \item Understand how $L2$ loss optimization results in SSE-minimal model
  \item Understand this as a general template for ERM in ML
}


% ------------------------------------------------------------------------------

\begin{frame}{Linear regression}

\begin{itemize}
    \item Idea: predict $y \in \R$ as \textbf{linear} combination of 
    features\footnote[frame]{\tiny
    Actually, special case of linear model, which is linear combo of 
    \textit{basis functions} of features $\rightsquigarrow$
    Polynomial Regression Models
    }:
    $$\yh = \fx = \thx = \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p$$
    $~\rightsquigarrow$ find loss-optimal params to describe relation 
    $y | \xv$
    \item Hypothesis space: $\Hspace = \{\fx = \thx\ |\ \thetav \in \R^{p+1} \}$
    
\end{itemize}
\vfill
\begin{minipage}{0.4\textwidth}
    \includegraphics[width=\textwidth]{figure/reg_l2_basic_lm.pdf} 
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.4\textwidth}
    \includegraphics[width=1.2\textwidth, trim=100 0 0 20, clip]{
    figure/reg_l2_basic_lm_biv.pdf} 
\end{minipage}

\end{frame} 

% ------------------------------------------------------------------------------

\begin{vbframe}{Design matrix}

\begin{itemize}
    \item Mismatch: $\thetav \in \R^{p + 1}$ vs $\xv \in \R^p$ due to intercept  
    term
    \item Trick: pad feature vectors with leading 1, s.t. 
    \begin{itemize}
        \item $\xv \mapsto \xv = (1, x_1, \dots, x_p)^\top$, and 
        \item $\thx = \theta_0 \cdot 1 + \theta_1 x_1 + \dots + \theta_p x_p$
    \end{itemize}
    \item Collect all observations in \textbf{design matrix} 
    $\Xmat \in \R^{n \times (p + 1)}$ \\
    $\rightsquigarrow$ more compact: single param vector incl. intercept
    \item Resulting linear model:
    \begin{align*}
    \hat \yv = \Xmat \thetav = 
        \left(
        \begin{smallmatrix}
            1 & x^{(1)}_1 & \ldots & x^{(1)}_p \\
            1 & x^{(2)}_1 & \ldots & x^{(2)}_p \\
            \vdots & \vdots & & \vdots \\
            1 & x^{(n)}_1 & \ldots & x^{(n)}_p \\
        \end{smallmatrix}
        \right)
        \left(
        \begin{smallmatrix}
            \theta_0 \\ \theta_1 \\ \vdots \\ \theta_p
        \end{smallmatrix}
        \right)
        &=
        \left(
        \begin{smallmatrix}
            \theta_0 + \theta_1 x_1^{(1)} + \dots + \theta_p x_p^{(1)} \\
            \theta_0 + \theta_1 x_1^{(2)} + \dots + \theta_p x_p^{(2)} \\
            \vdots \\
            \theta_0 + \theta_1 x_1^{(n)} + \dots + \theta_p x_p^{(n)} \\
        \end{smallmatrix}
        \right)
    \end{align*}
    \item We will make use of this notation in other contexts
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Effect interpretation}

\begin{itemize}
    \item Big plus of LM: immediately \textbf{interpretable} feature effects
    \item "Marginally increasing $x_j$ by 1 unit increases $y$ by $\theta_j$ 
    units" \\
    $\rightsquigarrow$ \textit{ceteris paribus} assumption: 
    $x_1, \dots, x_{j - 1}, x_{j + 1}, \dots, x_p$ fixed
\end{itemize}

\vfill
\includegraphics[width=0.4\textwidth]{figure/reg_l2_basic_lm_interpreted.pdf} 
\hfill
\includegraphics[width=0.55\textwidth]{figure_man/lm_summary} 

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Model fit}

\begin{itemize}
    \item How to determine LM fit? $\rightsquigarrow$ define risk \& optimize
    \item Popular: \textbf{$L2$ loss} / \textbf{quadratic loss} / 
    \textbf{squared error}
    $$\Lxy = (y-\fx)^2 ~~ \text{or} ~~ \Lxy = 0.5 \cdot (y-\fx)^2$$
    
    \includegraphics[width=0.35\textwidth]{figure/reg_l2_residual.pdf}
    \item Why penalize \textbf{residuals} $r = y - \fx$ quadratically?
    \begin{itemize}
        \item Easy to optimize (convex, differentiable)
        \item Theoretically appealing (connection to classical stats LM)
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{loss plots}

We will often visualize loss effects like this:

\vfill

\includegraphics[width=\textwidth]{figure/reg_l2_lossplot_quad.pdf}

\hspace{0.5cm}
\begin{minipage}[t]{0.45\textwidth}
    \footnotesize
    \begin{itemize}
        \item Data as $y \sim x_1$
        \item Prediction hypersurface \\$\rightsquigarrow$ here: line
        \item Residuals \textcolor{blue}{$r = y - \fx$}
        \\$\rightsquigarrow$ squares to illustrate loss
    \end{itemize}
\end{minipage}
\hfill
\begin{minipage}[t]{0.4\textwidth}
    \footnotesize
    \begin{itemize}
        \item Loss as function of residuals
        \\$\rightsquigarrow$ strength of penalty? 
        \\$\rightsquigarrow$ symmetric?
        \item Highlighted: loss for residuals shown on LHS
    \end{itemize}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{optimization}

\begin{itemize}
    \item Resulting risk equivalent to 
    \textbf{sum of squared errors (SSE)}:
    $$\risket = \sumin \left(\yi - \thetav^\top \xi \right)^2$$
    \item Consider example with $n = 5$ $\rightsquigarrow$ 
    different models with varying SSE
\end{itemize}

\vfill
\only<1>{
    \phantom{\includegraphics[width=0.25\textwidth]{
    figure/reg_l2_sse_1.pdf}}
}
\only<2>{
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_1.pdf}
}
\only<3>{
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_2.pdf}
}
\only<4>{
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_2.pdf}
    \includegraphics[width=0.3\textwidth]{figure/reg_l2_sse_3.pdf}
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{optimization}

% Only stuff below super tedious, but when leaving out the repetitions (defining
% table globally and only adding lines in only statements), there's weird
% indenting...

\vspace{0.2cm}
\begin{minipage}[c]{0.5\textwidth}
    % \only<1>{
    % \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf} \\
    % \phantom{
    %     \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    % }
    % \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}}
    % \vfill \vspace{0.5cm} \scriptsize
    % \begin{tabular}{r|r|r}
    %     Intercept $\theta_0$ & Slope $\theta_1$ & SSE
    %     \\ \hline 1.80 & 0.30 & 16.86\\ \hline && \\ \hline && \\ \hline &&
    % \end{tabular}
    % }
    % \only<2>{
    % \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf}
    % \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    % \phantom{\includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}}
    % \vfill \vspace{0.5cm} \scriptsize
    % \begin{tabular}{r|r|r}
    %     Intercept $\theta_0$ & Slope $\theta_1$ & SSE
    %     \\ \hline 1.80 & 0.30 & 16.86\\ \hline 1.00 & 0.10 & 24.29 \\ 
    %     \hline && \\ \hline &&
    % \end{tabular}
    % }
    \only<1>{
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}
    \vfill \vspace{0.5cm} \scriptsize
    \begin{tabular}{r|r|r}
        Intercept $\theta_0$ & Slope $\theta_1$ & SSE
        \\ \hline 1.80 & 0.30 & 16.86\\ \hline 1.00 & 0.10 & 24.29 \\
        \hline 0.50 & 0.80 & 10.61 \\ \hline &&
    \end{tabular}
    }
    \only<2>{
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_1.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_2.pdf} \\
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_3.pdf}
    \includegraphics[width=0.4\textwidth]{figure/reg_l2_sse_4.pdf}
    \vfill \vspace{0.5cm} \scriptsize
    \begin{tabular}{r|r|r}
        Intercept $\theta_0$ & Slope $\theta_1$ & SSE
        \\ \hline 1.80 & 0.30 & 16.86\\ \hline 1.00 & 0.10 & 24.29 
        \\ \hline 0.50 & 0.80 & 10.61 \\ \hline
        \textcolor{blue}{-1.65} & \textcolor{blue}{1.29} &
        \textcolor{blue}{5.88}
    \end{tabular}
    }
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
    % \only<1>{\includegraphics[width=1.2\textwidth, trim=130 30 80 20, clip]{
    % figure/reg_l2_sse_optim_1.pdf}}
    % \only<2>{\includegraphics[width=1.2\textwidth, trim=130 30 80 20, clip]{
    % figure/reg_l2_sse_optim_2.pdf}}
    \only<1>{\includegraphics[width=1.2\textwidth, trim=130 30 80 20, clip]{
    figure/reg_l2_sse_optim_3.pdf}}
    \only<2>{
    \includegraphics[width=1.2\textwidth, trim=130 10 80 20, clip]{
    figure/reg_l2_sse_optim_4.pdf}}
\end{minipage}

\only<2>{
\vfill

Instead of guessing, of course, use \textbf{optimization}!
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    \begin{align*}
        \thetavh \in 
        \argmin_{\thetav} \risket &=
        \argmin_{\thetav} \sumin \left(\yi - \thetav^\top \xi \right)^2  \\
        &= \argmin_{\thetav} \| \yv - \Xmat \thetav \|^2_2
    \end{align*}
    \normalsize
    \item Find via \textbf{normal equations}
    $$\pd{\risket}{\thetav} = 0$$
    \item Solution: \textbf{ordinary-least-squares (OLS)} estimator
    $$\thetavh = \olsest$$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Statistical properties}

% \footnotesize
\begin{itemize}
    % \small
    % \item In a nutshell: minimize quadratic residuals of form 
    % $\| \yv - \Xmat \thetav \|^2_2$
    \item LM with $L2$ loss intimately related to classical stats LM
    \item Assumptions
    \begin{itemize}
        % \small
        \item $\xi$ \textbf{iid} for $i \in \nset$
        \item \textbf{Homoskedastic} (equivariant) 
         \textbf{Gaussian} errors
        $$\yv = \Xmat \thetav + \bm{\epsilon}, ~ \bm{\epsilon} \sim 
        \normal(0, \sigma^2 \id)  $$
        $\rightsquigarrow$ $y_i$ conditionally independent \& normal:
        $\yv | \Xmat \sim \normal(\Xmat \thetav, \sigma^2 \id)$
        \item Uncorrelated features \\$\rightsquigarrow$ 
        multicollinearity destabilizes effect estimation 
    \end{itemize}
    \item If assumptions hold: statistical \textbf{inference} applicable
    \begin{itemize}
        % \small
            \item Hypothesis tests on significance of effects, incl. $p$-values
            \item Confidence \& prediction intervals via student-$t$ 
            distribution
        \item Goodness-of-fit measure
        $R^2 = 1 - \text{SSE} ~~ / \underbrace{\text{SST}}_{
        \sumin (\yi - \bar y)^2}$
        
        $\rightsquigarrow$ SSE = part of data variance \textit{not} explained 
        by model
    \end{itemize}
\end{itemize}

\end{vbframe}

\endlecture

\end{document}
