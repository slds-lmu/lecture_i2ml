\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Supervised Regression
  }{% Lecture title  
  Linear Models with $L1$ Loss
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/reg_l1_lossplot_abs_vs_quad.pdf
}{% Learning goals, wrapped inside itemize environment
  \item Understand difference between $L1$ and $L2$ regression
  \item See how choice of loss affects optimization \& robustness
}


% ------------------------------------------------------------------------------

\begin{vbframe}{Absolute loss}

\begin{itemize}
    \item $L2$ regression minimizes quadratic residuals -- wouldn't 
    \textbf{absolute} residuals seem more natural? 
    \vspace{0.2cm}
    \begin{center}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_residual_abs_vs_quad}
    \end{center}
    \item \textbf{$L1$ loss / absolute error / least absolute deviation (LAD)}
    $$\Lxy = |y - \fx|$$
    \begin{center}
    \includegraphics[width=0.55\textwidth]{figure/reg_l1_lossplot_abs}
    \end{center}
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- loss surface}

\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_l1_surface_abs.pdf}
\includegraphics[width=0.49\textwidth, trim=100 30 100 0, clip]{
figure/reg_l1_surface_quad.pdf}
\vfill

$L1$ loss (left) harder to optimize than $L2$ loss (right)
\begin{itemize}
    \item Convex but \textbf{not differentiable} in
    $y - \fx = 0$
    \item No analytical solution
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- estimated parameters}

\begin{itemize}
    \item Results of $L1$ and $L2$ regression often not that different
    \item Simulated data: $\yi = 1 + 0.5 x^{(i)}_1 + \epsi$, ~~ $\epsi \iid
    \normal(0, 0.01)$
    % \item Coefficients:
\end{itemize}
    
\vfill

\begin{minipage}[b]{0.65\textwidth}
    \hspace{0.7cm}
    \footnotesize
    \begin{tabular}{r|r|r}
        & intercept & slope \\ \hline
        \textcolor{blue}{$L1$} & 0.91 & 0.53 \\ \hline
        \textcolor{orange}{$L2$} & 0.91 & 0.57 
    \end{tabular}

    \vspace{0.5cm}
    \includegraphics[width=0.8\textwidth]{figure/reg_l1_comparison.pdf}
\end{minipage}
\begin{minipage}[b]{0.34\textwidth}
    \includegraphics[width=\textwidth, trim=80 0 100 80, clip]{
    figure/reg_l1_comparison_optim_abs.pdf}
    
    \vfill
    
    \includegraphics[width=\textwidth, trim=80 0 100 80, clip]{
    figure/reg_l1_comparison_optim_quad.pdf}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- robustness}

\begin{itemize}
    \item $L2$ quadratic in residuals $\rightsquigarrow$ outlying points 
    carry lots of weight
    \item E.g., $3 \times$ residual $\Rightarrow$ $9 \times$ loss contribution
    \item $L1$ more \textbf{robust} in presence of outliers (example ctd.):
\end{itemize}

\vfill
\includegraphics[width=\textwidth]{figure/reg_l1_comparison_outlier.pdf}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{$L1$ vs $L2$ -- optimization cost}

\begin{itemize}
    \item Real-world \texttt{weather} problem $\rightsquigarrow$ 
    predict mean temperature
    \item Compare \textbf{time} to fit $L1$ (\texttt{quantreg::rq()}) vs 
    $L2$ (\texttt{lm::lm()})
    for different dataset proportions (repeat $50\times$)
\end{itemize}

\vfill

\begin{minipage}[c]{0.54\textwidth}
    \centering
    \includegraphics[width=0.8\textwidth]{figure/reg_l1_benchmark.pdf}
\end{minipage}
\scriptsize
\begin{minipage}[c]{0.45\textwidth}

    Loss
    
    \begin{tabular}{l|r|r}
        & Fitted: \textcolor{blue}{$L1$}  &
        Fitted: \textcolor{orange}{$L2$} \\ \hline
        Total \textcolor{blue}{$L1$} loss & $8.98 \times 10^4$ & 
        $8.99 \times 10^4$ \\
        Total \textcolor{orange}{$L2$} loss & $5.83 \times 10^6$ & 
        $5.81 \times 10^6$ \\
    \end{tabular}
    
    \vspace{0.5cm}
    
    Estimated coefficients \\
    
    \begin{tabular}{l|r|r}
        $x_j$ & \textcolor{blue}{$L1$: $\hat \theta_j$}  &
        \textcolor{orange}{$L2$: $\hat \theta_j$} \\ \hline
        \texttt{Max\_temperature} & 0.553 & 0.563 \\
        \texttt{Min\_temperature} & 0.441 & 0.427 \\
        \texttt{Visibility} & 0.026 & 0.041 \\
        \texttt{Wind\_speed} & 0.002 & 0.010 \\
        \texttt{Max\_wind\_speed} & $-$0.026 & $-$0.039 \\
        \texttt{(Intercept)} & $-$0.380 & $-$0.102 \\
    \end{tabular}
    
    \vspace{0.5cm}
    \normalsize
    $L1$ \textbf{slower} to optimize!
\end{minipage}

\end{vbframe}

\endlecture
\end{document}
