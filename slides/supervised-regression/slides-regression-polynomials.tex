\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Lecture title
  Supervised Regression:\\Polynomial Regression Models
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/reg_poly_title
}{% Learning goals, wrapped inside itemize environment
  \item Learn about general form of linear model
  \item See how to add flexibility by using polynomials
  \item Understand that more flexibility is not necessarily better
}


% ------------------------------------------------------------------------------

\begin{vbframe}{Increasing flexibility}

\begin{itemize}
    \item Recall our definition of LM: model $y$ as linear combo of features
    \item But: isn't that pretty \textbf{inflexible}?
    \item E.g., here, $y$ does not seem to be a linear function of $x$...

    \vspace{0.5cm}
    \begin{center}
        \includegraphics[width=0.6\textwidth]{figure/reg_poly_yx3.pdf}
    \end{center}

    ... but relation to $x^3$ looks pretty linear!

    \item Many other trafos conceivable, e.g.,
    $\sin(x_1), ~ \max(0, x_2), ~ \sqrt{x_3}, \dots$
    \item Turns out we can use LM much more
    \textbf{flexibly} (and: it's still linear) \\
    $\rightsquigarrow$ interpretation might get less straightforward, though
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
    \item Recall what we previously defined as LM:
    \begin{equation} \label{simple_lm}
      f(x) = \theta_0 + \sumjp \theta_j x_j =
      \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p
    \end{equation}
    \item Actually, just special case of "true" LM
    \item \textbf{The linear model} with \textbf{basis functions}
    \textcolor{blue}{$\phi_j$}:
    \begin{equation*} \label{true_lm}
      \fx = \theta_0 + \sumjp \theta_j \textcolor{blue}{\phi_j} (x_j)
      = \theta_0 + \theta_1  \textcolor{blue}{\phi_1} \left( x_1 \right)
      + \dots + \theta_p \textcolor{blue}{\phi_p} \left(x_p \right)
    \end{equation*}
    \item In Eq.~\ref{simple_lm}, we implicitly use identity trafo:
    \textcolor{blue}{$\phi_j = \text{id}_x: x \mapsto x$}~~ $\forall j$ \\
    $\rightsquigarrow$ we often say LM and imply
    $\phi_j = \text{id}_x$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{the linear model}

\begin{itemize}
\item Are models like $\fx = \theta_0 + \theta_1 x^2$
  \textbf{really linear}?
\begin{minipage}[b]{0.7\textwidth}
  \vspace{0.3cm}
  \begin{itemize}
    \item Certainly not in covariates:
    \scriptsize
    \begin{align*}
      a \cdot f(x, \thetab) + b \cdot f(x_\ast, \thetab)
      &= \theta_0 (a + b) + \theta_1(a x^2 + b x_\ast^2) \\
      &\textcolor{red}{\neq} \theta_0 + \theta_1 (a x + b  x_\ast)^2\\
      &= f(a  x + b x_\ast, \thetab)
    \end{align*}
    \normalsize
    \item Crucially, however, \textbf{linear in params}:
    \scriptsize
    \begin{align*}
      a \cdot f(x, \textcolor{blue}{\thetab}) +
      b \cdot f(x, \textcolor{orange}{\thetab^\ast})
      &= a \theta_0 + b \theta_0^\ast + (a \theta_1 + b \theta_1^\ast) x^2 \\
      &= f(x, \textcolor{magenta}{a \thetab + b \thetab^\ast})
    \end{align*}
  \end{itemize}
\end{minipage}
\begin{minipage}[b]{0.2\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_linearity}
  \tiny \raggedleft
  \textcolor{blue}{$\thetab = (0.5, 0.4)^\top$} \\
  \textcolor{orange}{$\thetab = (1.0, 0.8)^\top$} \\
  \textcolor{magenta}{$\thetab = (1.5, 1.2)^\top$} \\
  \vspace{0.1cm}
\end{minipage}

\vfill

\item NB: we still call design matrix $\Xmat$, incorporating possible trafos:
\begin{align*}
  \Xmat =
  \left(
    \begin{smallmatrix}
        1 & \textcolor{black}{\phi_1} (x^{(1)}_1) & \ldots &
        \textcolor{black}{\phi_p} (x^{(1)}_p) \\
        % 1 & \textcolor{black}{\phi_2} (x^{(2)}_1) & \ldots &
        % \textcolor{black}{\phi_2} (x^{(2)}_p) \\
        \vdots & \vdots & & \vdots \\
        1 & \textcolor{black}{\phi_1} (x^{(n)}_1) & \ldots &
        \textcolor{black}{\phi_p} (x^{(n)}_p) \\
    \end{smallmatrix}
    \right)
\end{align*}
$\rightsquigarrow$ solution via normal equations as usual
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression}

\begin{itemize}
    \item Simple \& flexible choice for basis funs: \textbf{$d$-polynomials}
    \item Idea: map $x_j$ to (weighted) sum of its monomials up to order
    $d \in \N$
    $$ \phi^{(d)}: \R \rightarrow \R, ~~
    x_j \mapsto \sum_{k = 1}^d \beta_k x_j^k$$
    \includegraphics[width=0.9\textwidth]{figure/reg_poly_basis}
    \item How to estimate coefficients $\beta_k$?
    \begin{itemize}
      \item Both LM \& polynomials \textbf{linear} in their params
      $\rightsquigarrow$ merge
      \item E.g.,
      $\fx = \theta_0 + \theta_1 \phi^{(d)}(x)  =
      \theta_0 + \sum_{k = 1}^d \theta_{1, k} x^k$
      $$\rightsquigarrow \Xmat = \left(
      \begin{smallmatrix}
          1 & x^{(1)} & (x^{(1)})^2 & \hdots & (x^{(1)})^d \\
          \vdots & \vdots & \vdots & & \vdots \\
          1 & x^{(n)} & (x^{(n)})^2 & \hdots & (x^{(n)})^d \\
      \end{smallmatrix}
      \right),
      ~~ \thetab \in \R^{d + 1}
      $$
    \end{itemize}
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{polynomial regression -- examples}

Univariate regression, $d \in \{1, 5\}$

\begin{minipage}[c]{0.5\textwidth}
  \includegraphics[width=\textwidth]{figure/reg_poly_univ_2}
\end{minipage}
\begin{minipage}[c]{0.45\textwidth}
  \begin{itemize}
    \footnotesize
    \item
    Data-generating process:
    \begin{align*}
      y &= 0.5 \sin(x) + \epsilon, \\
      \epsilon &\sim \normal(0, 0.3^2)
    \end{align*}
    \item Model: $$f(x) = \theta_0 + \sum_{k = 1}^d \theta_{1, k} x^k$$
  \end{itemize}
\end{minipage}

\vfill

Bivariate regression, $d = 7$

\begin{minipage}[b]{0.5\textwidth}
  \includegraphics[width=0.8\textwidth, trim=100 0 60 60, clip]{
  figure/reg_poly_biv}
\end{minipage}
\begin{minipage}[b]{0.45\textwidth}
  \begin{itemize}
    \footnotesize
    \item
    Data-generating process: 
    \begin{align*}
      y &= 1 + 2 x_1 + x_2^3 + \epsilon, \\
      \epsilon &\sim \normal(0, 0.5^2)
    \end{align*}
    \item Model: $$f(x) = \theta_0 + \theta_1 x_1 + 
    \sum_{k = 1}^7 \theta_{2, k} x_2^k$$
  \end{itemize}
\end{minipage}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{complexity of polynomials}

\begin{itemize}
  \item Higher $d$ allows to learn more complex functions 
  
  $\rightsquigarrow$ richer hyp space / higher \textbf{capacity}
  
  \vfill
  \includegraphics[width=0.8\textwidth]{figure/reg_poly_univ_4}
  \item Should we then simply let $d \rightarrow \infty$?
  \begin{itemize}
    \item \textbf{No}: data contains random \textbf{noise} -- not part of true
    DGP
    \item Model with overly high capacity learns all those spurious patterns
    $\rightsquigarrow$ poor generalization to new data
    \item Also, higher $d$ can lead to oscillation esp. at bounds 
    
    (Runge's phenomenon\footnote[frame]{\scriptsize
    Interpolation of $m$ equidistant points with $d$-polynomial only 
    well-conditioned for $d < 2 \sqrt{m}$. Plot: 50 points, models with $d \geq
    14$ instable (under equidistance assumption).
    })
    \vfill
  \end{itemize}
\end{itemize}

\end{frame}


% ------------------------------------------------------------------------------

\begin{frame}{bike rental example}

\begin{itemize}
    \footnotesize
    \item OpenML task \href{https://www.openml.org/search?type=data&sort=runs&id=45103&status=active}{
    \texttt{dailybike}}: predict \texttt{rentals} from weather conditions
    \item Hunch: non-linear effect of \texttt{temperature} $\rightsquigarrow$
    include with polynomial:
    \[
        \fx = \sum_{k=1}^d \theta_{\text{temperature}, k}
        x_{\text{temperature}}^k + \theta_{\text{season}} x_{\text{season}} +
        \theta_{\text{humidity}} x_{\text{humidity}}
    \]
    \item Test error\footnote[frame]{
    \scriptsize
    Reliable insights about model performance only via separate test
    dataset not used during training (here computed via 10-fold
    \textit{cross validation}). Much more on this in Evaluation chapter.
    } confirms suspicion $\rightsquigarrow$ minimal
    for $d = 3$
    \vfill
    \includegraphics[width=0.9\textwidth]{figure/reg_poly_bike}
    \tiny
    \begin{tabular}{rrrrrrrrrr}
        $d$ & $\theta_0$ & $\theta_{\text{temp}, 1}$ & $\theta_{\text{temp}, 2}$
        & $\theta_{\text{temp}, 3}$ & $\theta_{\text{sSPRING}}$ &
        $\theta_{\text{sSUMMER}}$ & $\theta_{\text{sFALL}}$ &
        $\theta_{\text{hum}}$ & \textbf{test error} \\ \hline
        1 & 377.3 & 2778.2 & & & 101.0 & 57.0 & 80.1 & -230.0 & \textbf{121.9}\\
        \rowcolor{black!10}
        3 & 419.3 & 2645.8 & -963.1 & -430.9 & 71.9 & 75.8 & 56.6 & -283.8 &
        \textbf{117.6}
    \end{tabular}
    \vfill
    \footnotesize
    \item Conclusion: flexible effects can improve fit/performance
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
