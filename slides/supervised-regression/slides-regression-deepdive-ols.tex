\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Supervised Regression
  }{% Lecture title  
  Deep Dive: Proof OLS Regression
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/reg_l2_residual.pdf
}{% Learning goals, wrapped inside itemize environment
\item Understand analytical derivation of OLS estimator for LM
}


% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    \begin{align*}
        \thetavh \in 
        \argmin_{\thetav} \risket &=
        \argmin_{\thetav} \sumin \left(\yi - \thetav^\top \xi \right)^2  \\
        &= \argmin_{\thetav} \| \yv - \Xmat \thetav \|^2_2
    \end{align*}
    \normalsize
    \item Find via \textbf{normal equations}
    $$\pd{\risket}{\thetav} = 0$$
    \item Solution: \textbf{ordinary-least-squares (OLS)} estimator
    $$\thetavh = \olsest$$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization -- proof}

\scriptsize
$\risket = \textcolor{violet}{\sumin \big(\underbrace{\yi - \thetav^\top \xi}_{=:
\epsilon_i} \big)^2}
= \textcolor{teal}{\| \underbrace{\yv - \Xmat \thetav}_{=: \bm{\epsilon}} 
\|^2_2}$; ~~ $\thetav \in \R^{\tilde p}$ with $\tilde p := p + 1$

% \vfill

\begin{minipage}[t]{0.52\textwidth}
    \tiny 
    \begin{eqnarray*}
        0 &=& \textcolor{violet}{\frac{\partial \risket}{\partial \thetav}
        ~~ \text{(sum notation)}} \\
        0 &=& \frac{\partial}{\partial \thetav} \sumin \epsilon_i^2 
        ~~ \Big \rvert
        ~~ \text{sum \& chain rule} \\
        0 &=& \sumin \frac{\partial \epsilon_i^2}{\partial \epsilon_i}
        \frac{\partial \epsilon_i}{\partial \thetav} \\
        0 &=& \sumin \textcolor{black!30}{2} \epsilon_i 
        \textcolor{black!30}{(-1)} (\xi)^\top \\ 
        0 &=& \sumin (\yi - \thetav^\top \xi)(\xi)^\top \\
        \thetav^\top \sumin  \xi (\xi)^\top &=&\sumin \yi (\xi)^\top 
        ~~ \Big \rvert ~~ \text{transpose} \\
        \left( \sumin \underbrace{\xi (\xi)^\top }_{\tilde p \times \tilde p}\right) \thetav &=& 
        \sumin \xi \yi \\
        (\Xmat^\top \Xmat) \thetav &=&  \Xmat^\top \yv \\
        % \thetav &=& \sumin (\underbrace{\yi}_{1 \times 1} / \underbrace{
        % (\xi)^\top}_{1 \times p} \underbrace{\xi}_{p \times 1}
        \text{NB: } \sumin \xi (\xi)^\top &=& \Xmat^\top \Xmat \quad \text{is easy to show (try it!) -- and good to remember (this is basically the estimation of Cov(X))}
    \end{eqnarray*}
\end{minipage}
\hfill
% \vline
\begin{minipage}[t]{0.45\textwidth}
    \tiny 
    \begin{eqnarray*}
        0 &=& \textcolor{teal}{\frac{\partial \risket}{\partial \thetav}
        ~~ \text{(matrix notation)}} \\
        0 &=& \frac{\partial \| \bm{\epsilon} \|_2^2 }{\partial \thetav} \\
        0 &=& \frac{\partial \bm{\epsilon}^\top \bm{\epsilon} }{\partial 
        \thetav} 
        ~~ \Big \rvert ~~ \text{chain rule} \\
        0 &=& \frac{\partial \bm{\epsilon}^\top \bm{\epsilon}}{\partial 
        \bm{\epsilon}} \cdot \frac{\partial \bm{\epsilon}}{\partial \thetav} \\
        0 &=& \textcolor{black!30}{2}  \bm{\epsilon}^\top \cdot (
        \textcolor{black!30}{-1 \cdot} \Xmat) \\
        0 &=& (\yv - \Xmat \thetav)^\top \Xmat \\
        0 &=& \yv^\top \Xmat - \thetav^\top \Xmat^\top \Xmat \\
        \thetav^\top \Xmat^\top \Xmat &=& \yv^\top \Xmat 
        ~~ \Big \rvert ~~ \text{transpose} \\
        \Xmat^\top \Xmat \thetav &=& \Xmat^\top \yv \\
        \thetav &=& \underbrace{
        \underbrace{(\Xmat^\top \Xmat)^{-1}}_{\tilde p \times \tilde p}
        \underbrace{\phantom{(} \Xmat^\top}_{\tilde p \times n} 
        \underbrace{\yv}_{n \times 1}}_{\tilde p \times 1}
    \end{eqnarray*}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture

\end{document}
