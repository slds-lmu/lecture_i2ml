\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/reg_l2_residual.pdf}
\newcommand{\learninggoals}{
\item Understand analytical derivation of OLS estimator for LM
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{Supervised Regression:\\Deep Dive: Proof OLS Regression}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization}

\begin{itemize}
    \item Special property of LM with $L2$ loss: \textbf{analytical solution}
    available
    \begin{align*}
        \thetabh \in 
        \argmin_{\thetab} \risket &=
        \argmin_{\thetab} \sumin \left(\yi - \thetab^\top \xi \right)^2  \\
        &= \argmin_{\thetab} \| \yv - \Xmat \thetab \|^2_2
    \end{align*}
    \normalsize
    \item Find via \textbf{normal equations}
    $$\pd{\risket}{\thetab} = 0$$
    \item Solution: \textbf{ordinary-least-squares (OLS)} estimator
    $$\thetabh = \olsest$$
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{analytical optimization -- proof}

\scriptsize
$\risket = \textcolor{violet}{\sumin \big(\underbrace{\yi - \thetab^\top \xi}_{=:
\epsilon_i} \big)^2}
= \textcolor{teal}{\| \underbrace{\yv - \Xmat \thetab}_{=: \bm{\epsilon}} 
\|^2_2}$; ~~ $\thetab \in \R^{\tilde p}$ with $\tilde p := p + 1$

% \vfill

\begin{minipage}[t]{0.52\textwidth}
    \tiny 
    \begin{eqnarray*}
        0 &=& \textcolor{violet}{\frac{\partial \risket}{\partial \thetab}
        ~~ \text{(sum notation)}} \\
        0 &=& \frac{\partial}{\partial \thetab} \sumin \epsilon_i^2 
        ~~ \Big \rvert
        ~~ \text{sum \& chain rule} \\
        0 &=& \sumin \frac{\partial \epsilon_i^2}{\partial \epsilon_i}
        \frac{\partial \epsilon_i}{\partial \thetab} \\
        0 &=& \sumin \textcolor{black!30}{2} \epsilon_i 
        \textcolor{black!30}{(-1)} (\xi)^\top \\ 
        0 &=& \sumin (\yi - \thetab^\top \xi)(\xi)^\top \\
        \thetab^\top \sumin  \xi (\xi)^\top &=&\sumin \yi (\xi)^\top 
        ~~ \Big \rvert ~~ \text{transpose} \\
        \left( \sumin \underbrace{\xi (\xi)^\top }_{\tilde p \times \tilde p}\right) \thetab &=& 
        \sumin \xi \yi \\
        (\Xmat^\top \Xmat) \thetab &=&  \Xmat^\top \yv \\
        % \thetab &=& \sumin (\underbrace{\yi}_{1 \times 1} / \underbrace{
        % (\xi)^\top}_{1 \times p} \underbrace{\xi}_{p \times 1}
        \text{NB: } \sumin \xi (\xi)^\top &=& \Xmat^\top \Xmat \quad \text{is easy to show (try it!) -- and good to remember (this is basically the estimation of Cov(X))}
    \end{eqnarray*}
\end{minipage}
\hfill
% \vline
\begin{minipage}[t]{0.45\textwidth}
    \tiny 
    \begin{eqnarray*}
        0 &=& \textcolor{teal}{\frac{\partial \risket}{\partial \thetab}
        ~~ \text{(matrix notation)}} \\
        0 &=& \frac{\partial \| \bm{\epsilon} \|_2^2 }{\partial \thetab} \\
        0 &=& \frac{\partial \bm{\epsilon}^\top \bm{\epsilon} }{\partial 
        \thetab} 
        ~~ \Big \rvert ~~ \text{chain rule} \\
        0 &=& \frac{\partial \bm{\epsilon}^\top \bm{\epsilon}}{\partial 
        \bm{\epsilon}} \cdot \frac{\partial \bm{\epsilon}}{\partial \thetab} \\
        0 &=& \textcolor{black!30}{2}  \bm{\epsilon}^\top \cdot (
        \textcolor{black!30}{-1 \cdot} \Xmat) \\
        0 &=& (\yv - \Xmat \thetab)^\top \Xmat \\
        0 &=& \yv^\top \Xmat - \thetab^\top \Xmat^\top \Xmat \\
        \thetab^\top \Xmat^\top \Xmat &=& \yv^\top \Xmat 
        ~~ \Big \rvert ~~ \text{transpose} \\
        \Xmat^\top \Xmat \thetab &=& \Xmat^\top \yv \\
        \thetab &=& \underbrace{
        \underbrace{(\Xmat^\top \Xmat)^{-1}}_{\tilde p \times \tilde p}
        \underbrace{\phantom{(} \Xmat^\top}_{\tilde p \times n} 
        \underbrace{\yv}_{n \times 1}}_{\tilde p \times 1}
    \end{eqnarray*}
\end{minipage}

\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture

\end{document}
