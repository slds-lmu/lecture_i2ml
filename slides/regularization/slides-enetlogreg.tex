\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\titlefigure}{figure_man/elastic-net01.png}
\newcommand{\learninggoals}{
  \item \textcolor{blue}{XXX}
  \item \textcolor{blue}{XXX}
}

\title{Introduction to Machine Learning}
\date{}

\begin{document}

\lecturechapter{Elastic Net and Regularization for GLMs}
\lecture{Introduction to Machine Learning}



% \section{Elastic Net}

\begin{vbframe} {Elastic Net}


Elastic Net combines the $L_1$ and $L_2$ penalties:

$$
\mathcal{R}_{\text{elnet}}(\thetab) =  \sumin (\yi - \thetab^\top \xi)^2 + \lambda_1 \|\thetab\|_1 + \lambda_2 \|\thetab\|_2^2.
$$

\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/elastic-net01.png}\\
\end{figure}


\begin{itemize}
\item Correlated predictors tend to be either selected or zeroed out together.
\item Selection of more than $n$ features possible for $p>n$.
\end{itemize}


\framebreak
\footnotesize
Simulating two examples with each 50 data sets and 100 observations each: \\


$$\ydat =\Xmat \boldsymbol{\beta}+\sigma \epsilon, \quad \epsilon \sim N(0,1), \quad \sigma = 1$$
  
  \begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\textbf{Ridge} performs better for: \\ 
$\boldsymbol{\beta}=(\underbrace{2,\ldots,2}_{5},\underbrace{0,\ldots,0}_{5})$\\
$ \operatorname{corr}(\Xmat_{i},\Xmat_{j})=0.8^{|i-j|}$ for all $i$ and $j$
  \end{center}
\end{column}
\begin{column}{0.5\textwidth} 
\begin{center}
\textbf{Lasso} performs better for: \\
$\boldsymbol{\beta}=(2, 2, 2,\underbrace{0,\ldots,0}_{7})$ \\
$\operatorname{corr}(\Xmat_{i},\Xmat_{j})= 0$ for all $i \neq j$, otherwise 1
\end{center}
\end{column}
\end{columns}

\begin{figure}
\includegraphics[width=1\textwidth]{figure_man/elastic-net02.png}\\
\end{figure}

\framebreak

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure_man/elastic-net03.png}\\
\end{figure}


\normalsize
Since Elastic Net offers a compromise between Ridge and Lasso, it is suitable for both data situations.

\end{vbframe}


% \section{Regularized Logistic Regression}

\begin{vbframe}{Regularized Logistic Regression}

Regularizers can be added very flexibly to basically any model which is based on ERM.

\lz 

Hence, we can, e.g., construct $L_1$- or $L_2$-penalized logistic regression to enable coefficient shrinkage and variable selection in this model. 

% \lz 
% We can add a regularizer to the risk of logistic regression

\begin{align*}
\riskrt &= \risket + \lambda \cdot J(\thetab) \\
%&= \sumin \mathsf{log} \left[1 + \exp \left(-\yi f\left(\left.\xi~\right|~ \thetab\right)\right)\right] + \lambda \cdot J(\thetab) \\
&= \sumin \mathsf{log}\left[1 + \mathsf{exp}\left(-2\yi f\left(\left.\xi~\right|~ \thetab\right)\right)\right] + \lambda \cdot J(\thetab)
\end{align*}

% The other parts of the logistic regression remain exactly the same,
% except for the fitting algorithm to find \(\hat{\thetab}_{\text{reg}}\) (no closed-form solution, numerical optimization methods are necessary).

\end{frame}

\begin{frame}{Regularized Logistic Regression}


We fit a logistic regression model using polynomial features for \(x_1\)
and \(x_2\) with maximum degree of \(7\). We add an $L_2$ penalty. We
see for

\begin{itemize}

\item
  \(\lambda = 0\): The unregularized model seems to overfit.
\item
  \(\lambda = 0.0001\): Regularization helps to learn the underlying
  mechanism.
\item
  \(\lambda = 1\): The real data-generating process is captured very well.
\end{itemize}

\scriptsize

\begin{figure}
\includegraphics[width=0.8\textwidth]{figure_man/logistic-reg.png}\\
\end{figure}


\normalsize 

\end{vbframe}


\endlecture
\end{document}
