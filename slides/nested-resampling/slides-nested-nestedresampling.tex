\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  }{% Lecture title  
  Nested Resampling
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/Nested_Resampling.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand how the 3-way split of the data can be generalized to nested resampling
  \item Understand the goal of nested resampling
  \item Be able to explain how resampling allows to estimate the generalization error
}


\begin{vbframe}{Nested Resampling}
Just like we can generalize hold-out splitting to resampling to get more reliable estimates of the predictive performance, we can generalize the training/validation/test approach to \textbf{nested resampling}.
\vskip 2em
This results in two nested resampling loops, i.e., resampling strategies for both tuning and outer evaluation.
% \vskip 2em
% Let's look at this for a simple example: say we want to find out whether a logistic regression model or a $k$-NN classifier performs better on some data set. 

% \framebreak
% FIGURE SOURCE: https://docs.google.com/presentation/d/1W9nPHJa39Tkzzp37ZeRsEaSZjXMdFjH-rmIpimOKZKA/edit#slide=id.g61fd3b961b_0_1260
% \begin{center}\includegraphics[page = 1, width = \textwidth]{figure_man/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 2]{figure_man/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 3]{figure_man/Outer_CV.pdf}\end{center}

% \begin{center}\includegraphics[page = 4]{figure_man/Outer_CV.pdf}\end{center}

% \framebreak

\framebreak

Assume we want to tune over a set of candidate HP configurations $\lambda_i; i = 1, \dots$ with 4-fold CV in the inner resampling and 3-fold CV in the outer loop. The outer loop is visualized as the light green and dark green parts.

\vspace*{-0.3cm}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\framebreak

\begin{footnotesize}
In each iteration of the outer loop we:
\begin{itemize}
\item Split off the light green testing data
\item Run the tuner on the dark green part of the data, e.g.,
  evaluate each $\lambda_i$ through fourfold CV on the dark green part
\end{itemize}
\end{footnotesize}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\framebreak

\begin{footnotesize}
In each iteration of the outer loop we:
\begin{itemize}
\item Return the winning $\lambda^*$ that performed best on the grey inner test sets
\item Re-train the model on the full outer dark green train set
\item Evaluate it on the outer light green test set
\end{itemize}
\end{footnotesize}

% FIGURE SOURCE: No source
\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\framebreak

\begin{footnotesize}
The error estimates on the outer samples (light green) are unbiased because this data was strictly excluded from the model-building process of the model that was tested on.
\end{footnotesize}

% FIGURE SOURCE: No source

\begin{center}\includegraphics[width = 0.7\textwidth]{figure_man/Nested_Resampling.png}\end{center}

\end{vbframe}



\begin{vbframe}{Nested Resampling - Instructive Example}

Taking again a look at the motivating example and adding a nested resampling outer loop, we get the expected behavior:

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_tuning_nestresample_1} 
}
%FIGURE SOURCE: fig-cart_tuning_nestresample.R


\end{knitrout}


\end{vbframe}
\endlecture
\end{document}
