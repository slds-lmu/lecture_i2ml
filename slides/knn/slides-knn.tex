\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  }{% Lecture title  
  $k$-Nearest Neighbors
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/fig-title-knn-3d-contour.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand the basic idea of $k$-NN for regression and classification
  \item Understand that $k$-NN is a non-parametric, local model
  \item Know different distance measures for different scales of feature variables
}


% \begin{vbframe}{Nearest Neighbors: Intuition}
%
% \begin{itemize}
%  \item Say we know locations of cities in 2 different countries.
%  \item Say we know which city is in which country.
%  \item Say we don't know where the countries' border is.
%  \item For a given location, we want to figure out which country it belongs to.
%  \item Nearest neighbor rule: every location belongs to the same country as the closest city.
%  \item $k$-nearest neighbor rule: vote over the $k$ closest cities (smoother)
% \end{itemize}
% %\begin{center}
% %  \includegraphics[width=.5\textwidth]{figure_man/nn.png}
% %\end{center}
% \end{vbframe}



\begin{vbframe}{$k$-Nearest-Neighbors}

\begin{itemize}
\item $k$-\textbf{NN} can be used for regression and classification.
\item Generates "similar" predictions for $\xv$ to its $k$ closest neighbors.
\item "Closeness" requires a distance or similarity measure.
\item The subset of $\Dtrain$ that is at least as close to $\xv$ as its $k$-th closest neighbor $\xi[k]$ in $\Dtrain$ is called the $k$-\textbf{neighborhood} $N_k(\xv)$ of $\xv$:
\begin{footnotesize}
$$N_k(\xv) = \{ \xi \in \Dtrain \mid d(\xi, \xv) \leq d(\xi[k], \xv)\}$$
\end{footnotesize}
\end{itemize}


\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/reg_knn_1}

}



\end{knitrout}

\end{vbframe}



    %\item It is based on a special case of the $L_p$-norm: $||x||_p = (|x_1|^p + ... + |x_q|^p)^\frac{1}{p}$ with $p = 2$

% \framebreak
% \item Example:
% \begin{itemize}
%   \item Three data points with two metric features each: $a = (1, 3),
%   b = (4, 5)$ and $c = (7, 8)$
%   \item Which is the nearest neighbor of $b$ in terms of the Euclidean distance?
%   \item $d(b, a) = \sqrt{(4 - 1)^2 + (5 - 3)^2} = 3.61$
%   \item $d(b, c) = \sqrt{(4 - 7)^2 + (5 - 8)^2} = 4.24$
%   \item $\Rightarrow a$ is the nearest neighbor for $b$.
% \end{itemize}
%     \item Alternative distance measures are:
%     \begin{itemize}
%       \item Manhattan distance% based on the $L_1$-norm:
%         \begin{equation*}
%           d_{manhattan}(\xv, \tilde{\xv}) = \sumjp |x_j - \tilde{x}_j|
%         \end{equation*}
%       \item Mahalanobis distance (takes covariances in $\Xspace$ into account)
%     \end{itemize}
%   \end{itemize}

% \framebreak
%
% Comparison between Euclidean and Manhattan distance measures:\\
\begin{vbframe}{Distance Measures}







% \textbf{How to calculate distances?}
\begin{itemize}
  \item Popular for numerical features: \textbf{Minkowski} distances
  of the form $\| \xv - \tilde \xv \|_q = \left( \sumjp |x_j - \tilde x_j|^q
  \right)^{\frac{1}{q}}$ for $\xv, \tilde{\xv} \in \Xspace$ with $p$ numeric
  features
  \item Especially, \textbf{Manhattan} ($q = 1$) and \textbf{Euclidean}
  ($q = 2$) distance
\end{itemize}

\vspace{0.3cm}

\begin{minipage}{0.42\textwidth}
  \begin{itemize}
    \small
    \item[] $d_{Manhattan}\left(\xv, \tilde{\xv}\right) = \| \xv -
    \tilde \xv \|_1$ \\
    $=\sumjp |x_j - \tilde{x}_j| $
    \item[]
    \item[] $d_{Euclidean}\left(\xv, \tilde{\xv}\right) = \| \xv -
    \tilde \xv \|_2$ \\
    \vspace{0.1cm}
    $= \sqrt{\sumjp(x_j- \tilde{x}_j)^2}$
  \end {itemize}
\end{minipage}%
\begin{minipage}{0.58\textwidth}
 %\strut\vspace*{-\baselineskip}\newline
 \centering \includegraphics[width=0.9\textwidth,keepaspectratio]{figure/reg_knn_2}
\end{minipage}



\end{vbframe}




\begin{vbframe}{Prediction - Regression}
\begin{footnotesize}
Compute for each point the average output $y$ of the $k$-nearest neighbours in $N_k(\xv)$:
% \item For regression:
$$
\fxh = \frac{1}{k} \sum_{i: \xi \in N_k(\xv)} \yi \text  { or }
\fxh = \frac{1}{\sum_{i: \xi \in N_k(\xv)} w^{(i)}} \sum_{i: \xi \in N_k(\xv)} w^{(i)} \yi
$$
with neighbors weighted based on their distance to $\xv$: $w^{(i)} = \frac{1}{d(\xi, \xv)}$
%fig-reg_knn_1_3d.R
\vspace{-0.3cm}
\begin{center}
\includegraphics[width=0.65\textwidth,keepaspectratio]{figure/reg_knn_contour}

\vspace{-0.1cm}
%fig-reg_knn_1_contour.R
\includegraphics[width=0.25\textwidth]{figure/knn-reg-3d-15.png}
\includegraphics[width=0.25\textwidth]{figure/knn-reg-3d-7.png}
\includegraphics[width=0.25\textwidth]{figure/knn-reg-3d-3.png}
\end{center}

\end{footnotesize}

\end{vbframe}
\begin{vbframe}{Prediction - Classification}

\begin{footnotesize}
For classification in $g$ groups, a majority vote is used: \\
$$
\hxh = \argmax_{\ell \in \gset} \sum_{i: \xi \in N_k(\xv)} \I(\yi = \ell)
$$
And posterior probabilities can be estimated with:
$$
\hat{\pi}_{\ell}(\xv)= \frac{1}{k} \sum_{i: \xi \in N_k(\xv)} \I(\yi = \ell)
$$

\medskip
\begin{columns}[T]
  \begin{column}{0.5\textwidth}
  \centering
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.7\textwidth, keepaspectratio]{figure/reg_class_knn_1}

}



\end{knitrout}
  \end{column}
  \begin{column}{0.5\textwidth}
  \centering
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\fontsize{5}{5}\selectfont
\begin{tabular}{l|r|r|l|r}
\hline
  & SL & SW & Species & dist\\
\hline
52 & 6.4 & 3.2 & versicolor & 0.200\\
\hline
59 & 6.6 & 2.9 & versicolor & 0.224\\
\hline
\rowcolor[HTML]{DFECFF}  \textcolor{black}{75} & \textcolor{black}{6.4} & \textcolor{black}{2.9} & \textcolor{black}{versicolor} & \textcolor{black}{0.100}\\
\hline
76 & 6.6 & 3.0 & versicolor & 0.200\\
\hline
98 & 6.2 & 2.9 & versicolor & 0.224\\
\hline
104 & 6.3 & 2.9 & virginica & 0.141\\
\hline
\rowcolor[HTML]{DFECFF}  \textcolor{black}{105} & \textcolor{black}{6.5} & \textcolor{black}{3.0} & \textcolor{black}{virginica} & \textcolor{black}{0.100}\\
\hline
111 & 6.5 & 3.2 & virginica & 0.224\\
\hline
116 & 6.4 & 3.2 & virginica & 0.200\\
\hline
\rowcolor[HTML]{DFECFF}  \textcolor{black}{117} & \textcolor{black}{6.5} & \textcolor{black}{3.0} & \textcolor{black}{virginica} & \textcolor{black}{0.100}\\
\hline
138 & 6.4 & 3.1 & virginica & 0.100\\
\hline
148 & 6.5 & 3.0 & virginica & 0.100\\
\hline
\end{tabular}


\end{knitrout}
  \end{column}
\end{columns}
\medskip
\textbf{Example with subset of iris data ($k = 3$)} \\
$  \hat{\pi}_{setosa}(\xv_{new}) = \frac{0}{3} = 0\% $,
$  \hat{\pi}_{versicolor}(\xv_{new}) = \frac{1}{3} = 33\% $,
$  \hat{\pi}_{virginica}(\xv_{new}) = \frac{2}{3} = 67\% $,
$  \hh(\xv_{new}) = \textit{virginica}$
\end{footnotesize}
\end{vbframe}

\begin{vbframe}{$k$-NN: From small to large $k$}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth,height=7cm]{figure/reg_class_knn_2}

}



\end{knitrout}
Complex, local model vs smoother, more global model

\end{vbframe}




\begin{vbframe}{$k$-NN Summary}

  \begin{itemize}
  \item $k$-NN is a lazy classifier, it has no real training step, it simply stores the complete data
    - which are needed during prediction.
  \item Hence, its parameters are the training data, there is no real compression of information.
  \item As the number of parameters grows with the number of training points, we call
    $k$-NN a non-parametric model
  \item $k$-NN is not based on any distributional or functional assumption,
    and can, in theory, model data situations of arbitrary complexity.
  % \item $k$-NN has no optimization step and is a very local model.
  % \item We cannot simply use least-squares loss on the training data for picking $k$,
    % because we would always pick $k=1$.
  \item The smaller $k$, the less stable, less smooth and more \enquote{wiggly} the decision
    boundary becomes.
  \item Accuracy of $k$-NN can be severely degraded by the presence of noisy or irrelevant features,
    or when the feature scales are not consistent with their importance.
  \end{itemize}

% \framebreak
%
% \lz
%
% \textbf{Hypothesis Space:} Step functions over tesselations of $\Xspace$.\\
% Hyperparameters: distance measure $d(\cdot,\cdot)$ on $\Xspace$; size of neighborhood $k$.
%
% \lz
%
% \textbf{Risk:} Use any loss function for regression or classification.
%
% \lz
%
% \textbf{Optimization:} Not applicable/necessary.\\ But: clever look-up methods \& data structures to avoid computing all $n$ distances for generating predictions.

\end{vbframe}

\begin{vbframe}{Standardization and Weights}


\begin{itemize}
  \item \textbf{Standardization:} Features in k-NN are usually standardized or normalized. If two features have values on a very different range, most distances would place a higher importance on the one with a larger range, leading to an imbalanced influence of that feature.
  \item \textbf{Importance:} Sometimes one feature has a higher importance (maybe we know this via domain knowledge). It can now manually be upweighted to reflect this.

$$d^{weighted}_{Euclidean}\left(\xv, \tilde{\xv}\right) = \sqrt{\sumjp w_{j}(x_j- \tilde{x}_j)^2}$$

  \item If these weights would have to be learned in a data-driven manner,
    we could only do this by hyperparameter tuning in k-NN. This is inconvenient,
    and Gaussian processes handle this much better.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Gower distance}
\begin{itemize}
    \item A weighted mean of univ. distances in the j-th feature.
    \item It can handle categoricals, missings, and different ranges.
\end{itemize}


$$d_{gower}(\xv, \tilde{\xv}) = \frac{\sumjp \delta_{x_j,\tilde{x}_j} \cdot d_{gower}(x_j,\tilde{x}_j)}{
\sumjp \delta_{x_j,\tilde{x}_j}}.
$$

\begin{itemize}

  \item \(\delta_{x_j,\tilde{x}_j}\) is 0 or 1.
  It's 0 if $j$-th feature is \emph{missing} in at least one observation,
  or when the feature is asymmetric binary (where \enquote{1} is more
  important than \enquote{0}) and both values are zero. Otherwise 1.
  \item \(d_{gower}(x_j,\tilde{x}_j)\):
  For nominals it's 0 if both values are equal and 1 otherwise.
  For integers and reals, it's the absolute difference, divided by range.

\end{itemize}

\framebreak

Example of Gower distance with data on sex and income:

\begin{columns}[T]
  \begin{column}{0.4\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{table}[H]
\centering\begingroup\fontsize{10}{12}\selectfont

\begin{tabular}{>{\leavevmode\color{black}}l|>{\leavevmode\color{black}}l|>{\leavevmode\color{black}}r}
\hline
index & sex & salary\\
\hline
1 & m & 2340\\
\hline
2 & w & 2100\\
\hline
3 & NA & 2680\\
\hline
\end{tabular}
\endgroup{}
\end{table}


\end{knitrout}
  \end{column}
  \begin{column}{0.5\textwidth}
    \vspace{0.6cm}
    \fbox{
      $d_{gower}(\xv,\tilde{\xv}) = \frac{\sumjp \delta_{x_j,\tilde{x}_j} \cdot d_{gower}(x_j,\tilde{x}_j)}{
      \sumjp \delta_{x_j,\tilde{x}_j}}
      $}
  \end{column}
\end{columns}

\vfill

$d_{gower}(\xv^{(1)},\xv^{(2)}) = \frac{ 1 \cdot 1 + 1 \cdot \frac{\left| 2340 - 2100 \right|}
{\left| 2680 - 2100 \right|}}{1 + 1} = \frac{1 + \frac{240}{580}}{2} = \frac{1 + 0.414}{2} = 0.707
$

\vfill

$d_{gower}(\xv^{(1)},\xv^{(3)}) = \frac{ 0 \cdot 1 + 1 \cdot \frac{\left| 2340 - 2680 \right|}
{\left| 2680 - 2100 \right|}}{0 + 1} = \frac{0 + \frac{340}{580}}{1} = \frac{0 + 0.586}{1} = 0.586
$

\vfill

$d_{gower}(\xv^{(2)},\xv^{(3)}) = \frac{0 \cdot 1 + 1 \cdot \frac{\left| 2100 - 2680 \right|}
{\left| 2680 - 2100 \right|}}{0 + 1} = \frac{0 + \frac{580}{580}}{1} = \frac{0 + 1.000}{1} = 1
$

\vfill
\end{vbframe}





\endlecture
\end{document}
