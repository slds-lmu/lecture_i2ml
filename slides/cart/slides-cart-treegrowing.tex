\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  CART 
  }{% Lecture title  
  Growing a Tree  
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/tree_depth1_structure.png
  }{
  \item Understand how a tree is grown by an exhaustive search 
  \item Know where and how the split point is set 
}

\begin{vbframe}{Tree Growing}

\begin{itemize}

\item We start with an empty tree, a root node that contains all the data.\\
Trees are then grown by recursively applying \textbf{greedy} optimization to each node $\Np$.

\item Greedy means we do an \textbf{exhaustive search}: Ideally, all possible splits of $\Np$ on all possible points $t$ for all features $x_j$ are compared in terms of their empirical risk $\risk(\Np, j, t)$. 

\item The training data is then distributed to child nodes according to the optimal split and the procedure is repeated in the child nodes.

\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.65\textwidth]{figure/tree-classif-depth1-ann.pdf} 

}

\end{vbframe}


\begin{vbframe}{Tree Growing}

\begin{enumerate}
\item Start with a root node of all data.
\item Search for feature and split point that minimizes the empirical risk in child nodes --  makes label distribution more homogenous.
\end{enumerate}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/tree-classif-depth1.pdf} 

}

\end{vbframe}

\begin{vbframe}{Tree Growing}

\begin{enumerate}[3]
\item Proceed recursively for each child node:
%Iterate over all features, and for each feature over all possible split points. 
Select best split and divide data from parent node into left and right child nodes.
\end{enumerate}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/tree-classif-depth2.pdf} 

}

\end{vbframe}

\begin{vbframe}{Tree Growing}
\begin{enumerate}[4]
\item Repeat until we reach a stop criterion, e.g., until each leaf cannot be split further.
\end{enumerate}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/tree-classif-depth3.pdf} 

}

\end{vbframe}



\begin{vbframe}{Split placement}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.5\textwidth]{figure/split_point.pdf}

}



\end{knitrout}
\lz
Splits are usually placed at the mid-point of the observations they split: the large margin to the next closest observations makes better generalization on new, unseen data more likely.
\end{vbframe}

\begin{vbframe}{Finding the split}

Assume we split the data so that the misclassification error (MCE) is minimal through the splitting.

First, we check a set of potential splits for \texttt{Sepal.Width}

\begin{figure}
\includegraphics[width=0.6\textwidth]{figure/treegrowing-y.pdf}
\end{figure}

\end{vbframe}

\begin{vbframe}{Finding the split}

Then we check a set of potential splits for \texttt{Sepal.Length}

\begin{figure}
\includegraphics[width=0.6\textwidth]{figure/treegrowing-x.pdf}
\end{figure}

\end{vbframe}

\begin{vbframe}{Finding the split}

\begin{itemize}
\item We take the split with lowest MCE: \texttt{Sepal.Length} = $5.5$
\item In real life, we actually search over many more splitting points.
Common strategies involve: a) Searching over all possible split points (exhaustive search), b) searching quantile-wise 
\item MCE is rarely used, we will cover split criteria in detail later.
%\item We will introduce additional (better) criteria soon
\end{itemize}


\begin{figure}
\includegraphics[width=0.75\textwidth]{figure/tree-classif-depth1.pdf}
\end{figure}

\end{vbframe}



% \begin{vbframe}{Regression Example}
% \begin{columns}[T,onlytextwidth]
% \column{0.2\textwidth}
% <<out.width='\\textwidth'>>=
% modForrester = makeSingleObjectiveFunction(
%   name = "Modification Forrester et. all function",
%   fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
%   par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
%   noisy = TRUE
% )
% design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
% design$y = modForrester(design)
% ordered.design = design[order(design$x),]
% rownames(ordered.design) = NULL
% kable(ordered.design, digits = 3)
% @
% 
% \hspace{0.5cm}
% \column{0.7\textwidth}
% % FIGURE SOURCE: No source
% \includegraphics[height = 0.55\textheight]{figure_man/regression_tree}
% \end{columns}
% \vspace{0.5cm}
% Data points (red) were generated from the underlying function (black):
% 
% $ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $
% 
% % \framebreak
% 
% % BB: doesnt seem too useful to show this, nothing really new in here
% % <<fig.height=5>>=
% % regr.task = makeRegrTask(data = design, target = "y")
% % regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
% % regr.model = train(regr.rpart, regr.task)
% % fancyRpartPlot(regr.model$learner.model, sub="")
% % @
% \end{vbframe}






\endlecture
\end{document}
