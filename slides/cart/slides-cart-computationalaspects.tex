\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  CART 
  }{% Lecture title  
  Computational Aspects of Finding Splits
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/categoryplot-binarysmall
  }{
  \item Know how monotone feature transformations affect the tree
  \item Understand how categorical features can be treated effectively while growing a CART
  \item Understand how missing values can be treated in a CART
}

\begin{vbframe}{Monotone feature transformations}

Monotone transformations of one or several features will neither change the value of the splitting criterion nor the structure of the tree,  only the numerical value of the split point.
\vspace{0.5cm}
\begin{columns}[T]
\column{0.49\textwidth}
Original data
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|r|r|r}
\hline
x & 1.0 & 2.0 & 7.0 & 10.0 & 20.0\\
\hline
y & 1.0 & 1.0 & 0.5 & 9.0 & 11.0\\
\hline
\end{tabular}


\end{knitrout}
% FIGURE SOURCE: Use picture created in rsrc/monotone_trafo.R
\includegraphics[width = \textwidth]{figure/cart_splitcomp_1}
\column{0.49\textwidth}
Data with log-transformed $x$
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\begin{tabular}{l|r|r|r|r|r}
\hline
log(x) & 0.0 & 0.7 & 1.9 & 2.3 & 3.0\\
\hline
y & 1.0 & 1.0 & 0.5 & 9.0 & 11.0\\
\hline
\end{tabular}


\end{knitrout}
% FIGURE SOURCE: Use picture created in rsrc/monotone_trafo.R
\includegraphics[width = \textwidth]{figure/cart_splitcomp_2}
\end{columns}
\vspace{0.5cm}
\centering
\end{vbframe}

\begin{vbframe}{Categorical Features}
  \begin{itemize}
  \item A split on a categorical feature partitions the feature levels:
    $$x_j \in \{a,b,c\} \leftarrow \Np \rightarrow x_j \in \{d,e\} $$
  \end{itemize}
  \begin{figure}
   \includegraphics[width=0.8\textwidth]{figure/tree-categorical.pdf} 
  \end{figure}
  \end{vbframe}
  
  \begin{vbframe}{Categorical Features}
  \begin{itemize}
  \item A split on a categorical feature partitions the feature levels:
    $$x_j \in \{a,b,c\} \leftarrow \Np \rightarrow x_j \in \{d,e\} $$
  \item For a feature with $m$ levels,
  there are about $2^m$ different possible partitions of the $m$ values into two groups\\ ($2^{m-1} - 1$ because of symmetry and empty groups).
  \item Searching over all these becomes prohibitive for large values of $m$.
  \item For regression with L2 loss and for binary classification, we can define clever shortcuts.
  \end{itemize}

  \end{vbframe}
  
  \begin{frame}{Categorical Features}

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of $1$-outcomes for each category of the feature in $\Np$.

  \end{enumerate}
  \begin{columns}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-binary1.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \lz
  \end{column}
  \begin{column}{0.33\textwidth}
  \lz
  \end{column}
  \end{columns}

\end{frame}

  \begin{frame}[noframenumbering]{Categorical Features}

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of $1$-outcomes for each category of the feature in $\Np$.
  \item Sort the categories according to these proportions.
  \end{enumerate}
  \begin{columns}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-binary1.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-binary2.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \end{column}
  \end{columns}

\end{frame}

  \begin{frame}[noframenumbering]{Categorical Features}

For $0-1$ responses, in each node:
  \begin{enumerate}
  \item Calculate the proportion of $1$-outcomes for each category of the feature in $\Np$.
  \item Sort the categories according to these proportions.
  \item The feature can then be treated as if it was ordinal, so we only have to investigate at most $m-1$ splits.
  \end{enumerate}
  \begin{columns}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-binary1.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-binary2.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-binary3.pdf} 
  \end{figure}
  \end{column}
  \end{columns}

\end{frame}


\begin{vbframe}{Categorical Features}

  \begin{itemize}
  \item This procedure finds the optimal split.
  \item This result also holds for regression trees (with L2 loss) if the levels of the feature are ordered by increasing mean of the target
  \item The proofs are not trivial and can be found here:
    \begin{itemize}
    \item for 0-1 responses:
      \begin{itemize}
      \item \citebutton{Breiman, 1984, Chapter 4}{https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman}
      \item  \citebutton{Ripley, 1996, pp. 213 et seqq.}{https://www.cambridge.org/core/books/pattern-recognition-and-neural-networks/4E038249C9BAA06C8F4EE6F044D09C5C}
      \end{itemize}
    \item for continuous responses:
      \begin{itemize}
      \item \citebutton{Fisher, 1958}{http://csiss.ncgia.ucsb.edu/SPACE/workshops/2004/SAC/files/fisher.pdf}
      \end{itemize}
    \end{itemize}
  \item There are only heuristics for the multiclass case \citebutton{Wright and KÃ¶nig, 2019}{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6368971/}
  %\item The Algorithm prefers categorical variables with a large value
  %of categories $Q$
  \end{itemize}

\end{vbframe}

\begin{vbframe}{Categorical Features}

For continuous responses, in each node:
  \begin{enumerate}
  \item Calculate the mean of the outcome in each category
  \item Sort the categories by increasing mean of the outcome
  \end{enumerate}

  \begin{columns}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-cont1.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-cont2.pdf} 
  \end{figure}
  \end{column}
  \begin{column}{0.33\textwidth}
  \begin{figure}
  \includegraphics[width=0.8\textwidth]{figure/categoryplot-cont3.pdf} 
  \end{figure}
  \end{column}
  \end{columns}


\end{vbframe}

\begin{vbframe}{Missing feature values}
  \begin{itemize}
    \item When splits are evaluated, only observations for which the used feature is not missing are used. (This can actually bias splits towards using features with lots of missing values.) 
  \item \textbf{Surrogate splits} can deal with missing values during prediction.
  \item Surrogate splits are created during training. They define replacement splitting rules, using a different feature, that result in almost the same child nodes as the original split.
   \item When observations are passed down the tree, % (in training or prediction), 
   and the feature value used in a split is missing, we use the surrogate split instead to decide to which child the data should be assigned. 
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Surrogate Splits}
\begin{itemize}
\item Each surrogate split is a decision stump that tries to learn the actual splitting rule
\item Consider this tree with the primary split w.r.t. \texttt{Sepal.Length} where we perform binary classification (\texttt{setosa} vs. \texttt{virginica}):
\begin{figure}
\includegraphics[width=0.75\textwidth]{figure/tree-binary.pdf} 
\end{figure}
\item Our surrogate split should optimize a splitting criterion w.r.t. \texttt{Sepal.Length < 5.8}
\end{itemize}



\end{vbframe}

\begin{vbframe}{Surrogate Splits}
\begin{itemize}
\item Consider this subsample of the data used to fit the tree:
\begin{table}[ht]
\tiny
\centering
\begin{tabular}{rrrrrll}
  \hline
 & Sepal.Length & ... & Petal.Width & Species & Sepal.Length $<$ 5.8 \\ 
  \hline
1 & 5.10 & ... & 0.20 & setosa & TRUE \\ 
  4 & 4.60 & ... & 0.20 & setosa & TRUE \\ 
  9 & 4.40 & ... & 0.20 & setosa & TRUE \\ 
  15 & 5.80 & ... & 0.20 & setosa & FALSE \\ 
  18 & 5.10 & ... & 0.30 & setosa & TRUE \\ 
  52 & 5.80 & ... & 1.90 & virginica & FALSE \\ 
  57 & 4.90 & ... & 1.70 & virginica & TRUE \\ 
  62 & 6.40 & ... & 1.90 & virginica & FALSE \\ 
  77 & 6.20 & ... & 1.80 & virginica & FALSE \\ 
  99 & 6.20 & ... & 2.30 & virginica & FALSE \\ 
   \hline
\end{tabular}
\end{table}
\item Add column that indicates whether \texttt{Sepal.Length < 5.8}
%\item As this splitting rule is very good, we will have many instances where \texttt{Sepal.Length < 5.8} is \texttt{TRUE} and \texttt{Species} is \texttt{setosa}
\item Fit tree of depth 1 using all features but \texttt{Sepal.Length} %used 
to derive a split that explains 
\texttt{Sepal.Length < 5.8} best $\Rightarrow$ surrogate split
\item Typically, software stores the best and a few more surrogate splits
%\item A good surrogate tries to mimic the primary split this way

\end{itemize}


\end{vbframe}


\endlecture
\end{document}
