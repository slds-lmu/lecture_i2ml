\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  CART 
  }{% Lecture title  
    Advantages \& Disadvantages 
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/cart_dis_1
  }{
  \item Understand the advantages and disadvantages of CART
  \item Know when and where CART are applied
}

\begin{vbframe}{Advantages}
  \begin{itemize}
    \item Fairly easy to understand, interpret and visualize.
    \item Not much preprocessing required:
    \begin{itemize}
      \item Automatic handling of non-numerical features
      \item Automatic handling of missing values via surrogate splits
      \item No problems with outliers in features
      \item Monotone transformations do not affect the model fit: scaling becomes irrelevant
    \end{itemize}
    \item Interaction effects between features are easily possible
    \item Can model discontinuities and non-linearities
    \item Performs automatic feature selection
    \item Relatively fast, scales well with larger data
    \item Flexibility through the definition of custom split criteria or leaf-node prediction rules: clustering trees, semi-supervised trees, density estimation, etc.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Disadvantage: Linear Dependencies}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_dis_1} 

}


Linear dependencies must be modeled over several splits. 
Logistic regression would model this easily with fewer parameters.
\end{vbframe}

\begin{vbframe}{Disadvantages: Smooth functions and extrapolation}


{\centering \includegraphics[width=0.85\textwidth]{figure/not-smooth-extrapolation.pdf} 

}

Prediction functions of trees are never smooth as they are always step functions and do not extrapolate well beyond the training observations. % as the model a piecewise-constant function.

%In this case, the tree does not recover the smoothness of $y$ and does not extrapolate well beyond the training domain ($[0, 10]$).
\end{vbframe}

 \begin{vbframe}{Disadvantage: Instability}
 \begin{itemize}
 \item High instability (variance) of the trees
 \item Small changes in the data may lead to very different splits/trees
 \item This leads to a) less trust in interpretability b) is a reason why the prediction error of trees is usually comparably high
 \end{itemize}

Consider the Wisconsin Breast Cancer data set with 699 observations on 9 features and binary target (\enquote{benign} vs. \enquote{malignant}). We fit two trees: (A) with the full data set and (B) where we eliminated a single observation. Results in label flip for 17 observations of the training data:

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & benign & malignant \\ 
  \hline
benign & 445 &   6 \\ 
  malignant &  11 & 236 \\ 
   \hline
\end{tabular}
\end{table}

Rows: Predictions of (A), columns: Predictions of (B) % the learner with the complete data set, and the columns the reduced one.

% \begin{table}
% \begin{tabular}{ll}
% Feature name & Explanation\\
% \hline
% \code{Cl.thickness} & Clump Thickness\\
% \code{Cell.size} & Uniformity of Cell Size\\
% \code{Cell.shape} & Uniformity of Cell Shape\\
% \code{Marg.adhesion} & Marginal Adhesion\\
% \code{Epith.c.size} & Single Epithelial Cell Size\\
% \code{Bare.nuclei} & Bare Nuclei\\
% \code{Bl.cromatin} & Bland Chromatin\\
% \code{Normal.nucleoli} & Normal Nucleoli\\
% \code{Mitoses} & Mitoses\\
% \end{tabular}
% \end{table}

% \framebreak

% Tree fitted on complete Wisconsin Breast Cancer data
% <<results='hide', fig.height=5>>=
% # BB: i am using the BC dataset directly from mlbench
% # and convert features, see issue in i2ml nr 277
% library(mlbench)
% data(BreastCancer)
% d = BreastCancer
% d$Id = NULL
% for (i in 1:9)
%   d[,i] = as.integer(d[,i])
% print(str(d))
% lrn = makeLearner("classif.rpart")
% task1 = makeClassifTask(data = d, target = "Class")
% mod1 = train(lrn, task1)
% d2 = d[-13, ]
% task2 = makeClassifTask(data = d2, target = "Class")
% mod2 = train(lrn, task2)
% fancyRpartPlot(mod1$learner.model, sub = "")
% @
% \framebreak

% Tree fitted on Wisconsin Breast Cancer data without observation 13
% <<results='hide', fig.height=5>>=
% fancyRpartPlot(mod2$learner.model, sub = "")
% @
\end{vbframe}

\begin{vbframe}{Disadvantage: Instability}

The resulting decision trees look very different:

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.99\textwidth]{figure/instability_full.pdf} 
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.99\textwidth]{figure/instability_reduced.pdf} 
\end{figure}
\end{column}
\end{columns}

\end{vbframe}

\begin{vbframe}{CART in practice}

\begin{itemize}
\item Compared to other learners CART has suboptimal predictive performance, mainly because of the problems previously shown.
%\item Thus, it is rarely used in predictive tasks.
\item However, most disadvantages can be overcome when trained in ensembles: bagging or random forests.
\item Furthermore, trees are attractive tools if an interpretable model is desired or legally required.
\end{itemize}



\end{vbframe}

\begin{vbframe}{Further tree methodologies}

This lecture is mainly focused on Classification and Regression Trees (CART) \citebutton{Breiman, 1984}{https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman} \\
However, there are noteworthy other tree-based approaches as well:

\begin{itemize}
\item Automatic Interaction Detection (AID) \citebutton{Sonquist and Morgan, 1964}{https://scholar.googleusercontent.com/scholar.bib?q=info:VohIjpOwsj4J:scholar.google.com/&output=citation&scisdr=Cm0J5dajEP3hpuOm9tE:AGlGAw8AAAAAZGag7tFRhfgI-PwTnQdXQ3Pv4RA&scisig=AGlGAw8AAAAAZGag7i-yoiF8w79_qwEhGcqETy4&scisf=4&ct=citation&cd=-1&hl=de} and Chi-squared Automatic Interaction Detection (CHAID) \citebutton{Kass, 1980}{https://www.jstor.org/stable/2986296}: Creates all possible cross-tabulations for each categorical predictor until the best outcome is achieved and no further splitting can be performed
\item C4.5 \citebutton{Quinlan, 1993}{https://link.springer.com/article/10.1007/BF00993309}: Not limited to binary splitting
\item Unbiased Recursive Partitioning \citebutton{Hothorn et al., 2006}{https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf}: Improves the variable selection algorithm
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
