\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  CART 
  }{% Lecture title  
  Predictions with CART
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/cart_intro_annotated-tree
  }{
  \item Understand the basic structure of a tree model
  \item Understand that the basic idea of a tree model is the same for classification and regression 
  \item Understand how the label of a new observation is predicted via CART
  \item Know hypothesis space of CART
}

\begin{vbframe}{Binary Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_binary-tree_1.pdf}
    \end{figure}
  \begin{itemize}
    %\item Binary trees are a common data structure
    \item Binary trees represent a top-down hierarchy with binary splits
    %\item They can be used in many applications, including machine learning
    \item A tree is composed of different node types: a) the root node, b) internal nodes, and c) terminal nodes (also leaves).
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_binary-tree_2.pdf}
    \end{figure}
  \begin{itemize}
    \item Nodes have relative relationships, they can be:
    \begin{itemize}
    \item Parent nodes
    \item Child nodes
    \end{itemize}
    \item Root nodes don't have parents -- leaves don't have children
    
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Classification Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_annotated-tree.pdf}
    \end{figure}
  \begin{itemize}
    \item Classification trees use the structure of a binary tree
    \item Binary splits are constructed top-down in a \emph{data optimal} way
    \item Each split is a threshold decision for a single feature
    \item Each node contains the training points which follow its path
    \item Each leaf contains a constant prediction
  %  \item A splitting rule decides what is optimal and "coincides" with the lowest possible loss
  \end{itemize}
\end{vbframe}


\begin{vbframe}{Classification Tree Model and Prediction}
\begin{itemize}
\item When predicting new data (here*: \texttt{Sepal.Length} = 5, \texttt{Sepal.Width} = 3) we use the learned split points and pass an observation through the tree
\item Each observation is assigned to exactly one leaf
\item Classification trees can make hard-label predictions (here: \texttt{setosa}) or predict probabilities (here: 0.98, 0.02, 0.00)
\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.7\textwidth, keepaspectratio]{figure/no-points-classif-depth3.pdf}

}
\end{vbframe}

\begin{vbframe}{CART as a rule based model}

Leaf nodes can be expressed by a set of rules (left to right):

\begin{table}[]
\begin{tabular}{lllll}
Hard label prediction &  Label distribution & \texttt{Sepal.Width} & \texttt{Sepal.Length} \\
\hline
\texttt{setosa} & $0.98$, $0.02$, $0.00$ & $\geq 2.8$ & $< 5.5$ \\
\texttt{versicolor} & $0.14$, $0.71$, $0.14$ & $<2.8$ & $< 5.5$ \\
\texttt{setosa} & $0.71$, $0.29$, $0.00$ & $\geq 3.1$ & $\geq 5.5$ $\& < 6.2$ \\
\texttt{versicolor} & $0.00$, $0.72$, $0.28$ & $< 3.1$ & $\geq 5.5$ $\& < 6.2$\\
\texttt{virginica} & $0.00$, $0.29$, $0.71$ &  -- & $\geq 6.2$ \\

\end{tabular}
\end{table}

\begin{figure} 
\includegraphics[width=0.4\textwidth, keepaspectratio]{figure_man/tree_depth3_structure_wide.png}
\end{figure}


\end{vbframe}


\begin{vbframe}{Regression Tree Model and Prediction}
  \begin{itemize}
    \item Works the same way as for classification
    \item But predictions in leaf nodes are a numerical scalar

  \end{itemize}
\vspace{1cm}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.8\textwidth, keepaspectratio]{figure/tree-regr-depth3.pdf}

}

\end{vbframe}



\begin{vbframe}{Tree as an additive model}
Trees divide the feature space $\Xspace$ into \textbf{rectangular regions}: 
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(\xv \in Q_m),
  \end{align*}
  where a tree with $M$ leaf nodes defines $M$ \enquote{rectangles} $Q_m$.\\
  $c_m$ is the predicted numerical response, class label or class
  distribution in the respective leaf node.
  %For a regression task, CART is the set of all step functions over the rectangular partitions.
  \begin{figure}
\includegraphics[width=0.92\textwidth, keepaspectratio]{figure/combined-area-plot.pdf}
\end{figure}

\end{vbframe}

\begin{vbframe}{Tree as an additive model}

A 2D regression example: 
\lz
\begin{figure} 
\includegraphics[width=0.8\textwidth, keepaspectratio]{figure_man/tree-contin-surface3d.png}
\end{figure}


(For binary classification with probabilities, 2D surface looks similar.)

\end{vbframe}





% % BB: as we are not really talking too much about impurity anymore, I took this out
% % <<splitcriteria-plot, results='hide', fig.height=5>>=
% % Colors = pal_3
% % par(mar = c(5.1, 4.1, 0.1, 0.1))
% % p = seq(1e-6, 1-1e-6, length.out = 200)
% % entropy = function(p) (p * log(p) + (1 - p) * log(1 - p))/(2 * log(0.5))
% % gini = function(p) 2 * p * (1 - p)
% % missclassification = function(p) (1 - max(p, 1 - p))
% % plot(p, entropy(p), type = "l", col = Colors[1], lwd = 1.5, ylab = "",
% %   ylim = c(0, 0.6), xlab = expression(hat(pi)[Nk]))
% % lines(p, gini(p), col = Colors[2], lwd = 1.5)
% % lines(p, sapply(p, missclassification), col = Colors[3], lwd = 1.5)
% % legend("topright", c("Gini Index", "Entropy", "Misclassification Error"),
% %        col = Colors[1:3], lty = 1)
% % @





\endlecture
\end{document}
