\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  CART 
  }{% Lecture title  
  Stopping Criteria \& Pruning 
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/horizon.pdf
  }{
  \item Understand which problems arise when growing the tree until the end
  \item Know different stopping criteria
  \item Understand the idea of pruning
}

%\begin{vbframe}{Computational Complexity}

%The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
%\begin{itemize}
%\item Problem 1: This can become computationally expensive as the number of splits may \emph{grow exponentially} with the number of leaves in the trees.
%\end{itemize}

%While growing trees fully is computationally not efficient, in typical applications on modern computers, this does usually not imply infeasibly large computational costs.
%Single trees are almost always very fast to compute.
%However, high computational costs can be undesirable if many trees need to be fit (e.g. ensembles).

%\end{vbframe}

\begin{vbframe}{Overfitting Trees}

The recursive partitioning procedure used to grow a CART could run until every leaf only contains a single observation. Problem: Very complex trees will \emph{overfit the training data.} At some point we should stop splitting nodes into ever smaller child nodes: 

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figure/tree-overfitting-prediction.pdf}
\end{figure}


\end{vbframe}

\begin{vbframe}{Overfitting Trees}

We can reduce overfitting to some extent with a less deep tree:
\vspace{0.25cm}

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figure/tree-less-overfitting-prediction.pdf}
\end{figure}

\end{vbframe}

\begin{vbframe}{Stopping Criteria}
We can define different \textbf{stopping criteria}, e.g.: Don't split a node if
  \begin{itemize}
    \item a certain number of leaves if reached,
    \item it contains too few observations,
    \item splitting results in children with too few observations,
    \item splitting does not achieve a certain minimal improvement of the risk in the children, compared to the risk in the parent node,
    \item it already has the same target value (\textbf{pure node}) or identical feature values for all observations. 
  \end{itemize}
Selection of a stopping criterion and its concrete values are hyperparameters of CART.
%There are heuristics for selecting these criteria and software packages usually have considerate defaults that work well out-of-the-box.
\end{vbframe}

\begin{vbframe}{Horizon Effect}
It is hard to tell where we should stop while we're growing the tree: Before we have actually tried all possible additional splits further down a branch, we can't know whether any one of them will be able to reduce the risk by a lot (\emph{horizon effect}).

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figure/horizon.pdf}
\end{figure}


\end{vbframe}

%\begin{vbframe}{Problems with growing large trees}
%The \textbf{recursive partitioning} procedure used to grow a CART would run until every leaf only contains a single observation. 
%\begin{itemize}
%\item Problem 1: This can become computationally expensive as the number of splits may \emph{grow exponentially} with the number of leaves in the trees.
%\item Problem 2: At some point before that we should stop splitting nodes into ever smaller child nodes: very complex trees with lots of branches and leaves will \emph{overfit the training data.}
%\item Problem 3: However, it is very hard to tell where we should stop while we're growing the tree: Before we have actually tried all possible additional splits further down a branch, we can't know whether any one of them will be able to reduce the risk by a lot (\emph{horizon effect}).
%\end{itemize}
%\end{vbframe}



\begin{vbframe}{Pruning}

We try to tackle the horizon effect by \textbf{pruning}, a method to select the optimal size of a tree:

\begin{itemize}
%\item A method to select the optimal size of a tree.
\item Finding a combination of suitable strict stopping criteria (\enquote{pre-pruning}) is a hard problem (see chapter on \textbf{tuning}).
\item Alternative: Grow a large tree, then remove branches so that the resulting smaller tree has lower risk (\enquote{post-pruning}).
\item Often, post-pruning is meant when referring to pruning.

\end{itemize}
\end{vbframe}

\begin{vbframe}{Post-Pruning: CCP}
\begin{itemize}
\item Prominent pruning method: Cost-complexity pruning (CCP)
\item Idea: Grow a large tree and remove the least informative leaves
\item CCP is steered with a regularization parameter $\alpha$ that penalizes the number of leaves in a sub tree  % and adds this penalization onto the risk of the sub tree:
\end{itemize}

$$\riskr(T) = \sum_{m=1}^{|T|} \sum_{i: x^{(i)} \in Q_m} L(y^{(i)}, c_m) + \alpha |T|,$$

where $|T|$ is the number of leaves of sub tree $T$, $Q_m$ is the subset of the feature space related to the $m$-th terminal node, with its prediction $c_m$, and $T_0$ is the complete tree.

\end{vbframe}

\begin{vbframe}{CCP}
CCP performs a greedy backward search:
\begin{itemize}
\item Computes $\riskr(T)$ with a fixed $\alpha$ for all possible sub trees that can be created by replacing one internal node with a leaf.
\item By replacing a node we also eliminate all subsequent nodes.
\item We select the sub tree with lowest risk and repeat the procedure.
\item We stop if pruning does not further reduce the risk.
\end{itemize}
\begin{itemize}
\item This is proven to result in the pruned tree with the lowest risk.
\item For $\alpha = 0$, we would obviously select $T_0$.
\item Hyperparameter $\alpha$ is typically selected via cross-validation.
\item Other prominent post-pruning methods include, e.g., reduced error pruning (REP) or pessimistic error pruning (PEP).
\end{itemize}
\end{vbframe}


\begin{frame}{CCP}

We run the CCP algorithm step-by-step with $\alpha = 1.2$:
\vspace{0.25cm}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_1.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}

There are four possible nodes that we can eliminate to prune the tree.
We take the one replacement that results in the lowest risk (red).

\vspace{0.25cm}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_1_notes.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}


The first pruned sub tree has a lower risk than the full tree.
Thus, we prefer it over the full tree.
\vspace{0.25cm}

{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_2.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}

From here on, the risk increases.
\vspace{0.25cm}


{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_3.pdf} 

}

\end{frame}

\begin{frame}[noframenumbering]{CCP}


{\centering \includegraphics[width=0.95\textwidth]{figure/ccp_4.pdf} 

}


\end{frame}

\begin{frame}[noframenumbering]{CCP}

We select the first sub tree as it results in the lowest risk in the complete sequence of sub trees.
\vspace{0.25cm}


{\centering \includegraphics[width=0.99\textwidth]{figure/ccp_5.pdf} 

}


\end{frame}



\endlecture
\end{document}
