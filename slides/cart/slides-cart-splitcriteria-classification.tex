\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\titlemeta{% Lecture title
  CART: Splitting Criteria for Classification
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure/splitcrit-classif_optimal-constant-grid.pdf
  }{
    \item Understand how to define split criteria via ERM
    \item Understand how to find splits in regression with $L_2$ loss
}

\begin{vbframe}{Optimal Constant Models}

As losses in classification, we typically use: 

\begin{itemize}
\item (Multi-class) Brier score $\Lpiv =  \sumkg (\pik - o_k(y))^2$,\\ a.k.a. $L_2$ loss on probabilities
\item (Multi-class) Log loss $\Lpiv =  - \sumkg o_k(y) \log(\pik)$, \\
as in logistic regression
\end{itemize}

%\vspace{1cm}

Optimal constant predictions (in a node) for both losses are simply the proportions of the contained classes:

\begin{align*}
c_{\Np} &= (\pikNh[1], \dots, \pikNh[g]) \quad \text{with}\\
\pikNh &= \frac{1}{|\Np|} \sum\limits_{(\xv,y) \in \Np} \I(y = k) \quad \forall k \in \gset
\end{align*}


\end{vbframe}


\begin{frame}[b]{Finding the best split}

Let's compute the Brier score for all splits, with optimal constant probability vectors in both children

\vspace{0.5cm}

\only<1>{
  \includegraphics[width = 0.9\textwidth]{figure/splitcrit-classif_optimal-constant-sub1.pdf}
}
\only<2>{
  \includegraphics[width = 0.9\textwidth]{figure/splitcrit-classif_optimal-constant-sub2.pdf}
}
\end{frame}

\begin{frame}[b,noframenumbering]{Finding the best split}

The optimal split point typically creates greatest imbalance or purity of label distribution

%Let's compute the Brier score for all splits, with optimal constant probability vectors in both children


\vspace{0.5cm}

  \includegraphics[width = 0.9\textwidth]{figure/splitcrit-classif_optimal-constant-grid.pdf}
\end{frame}

\begin{vbframe}{Risk minimization vs. Impurity}

\begin{itemize}
\item Split crits are sometimes defined in terms of impurity reduction instead of ERM, where a measure of ``impurity'' is defined per node
\item For regression trees, \enquote{impurity} is simply defined as variance of $y$, which is quite obviously $L_2$ loss

\item Brier score is equivalent to Gini impurity
$$I(\Np) = \sumkg \pikNh \left( 1-\pikNh \right)$$
\item Log loss is equivalent to entropy
$$I(\Np) = -\sumkg \pikNh \log \pikNh$$
\item Trees can be understood completely through the lens of ERM, so this new terminology is unnecessary and perhaps confusing
\end{itemize}
\end{vbframe}

\begin{vbframe}{Splitting with misclassification loss}

\begin{itemize}
\item Often, we want to minimize the MCE in classification
\item Zero-One-Loss is not differentiable, but that is a non-issue in the tree-optimization based on loops
\item Brier score and Log loss more sensitive to changes in the node probs, often produce purer nodes, and are still preferred


\end{itemize}


\begin{small}
\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
\begin{center}
\textbf{Split 1:} \\
\vspace{0.25cm}
% latex table generated in R 4.0.1 by xtable 1.8-4 package
% Mon Aug 10 01:13:29 2020
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & class 0 & class 1 \\ 
  \hline
$\Nl$ & 300 & 100 \\ 
  $\Nr$ & 100 & 300 \\ 
   \hline
\end{tabular}
\end{table}

\end{center}
\column{0.5\textwidth}
\begin{center}
\textbf{Split 2:} \\
\vspace{0.25cm}
% latex table generated in R 4.0.1 by xtable 1.8-4 package
% Mon Aug 10 01:13:29 2020
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & class 0 & class 1 \\ 
  \hline
$\Nl$ & 400 & 200 \\ 
  $\Nr$ &   0 & 200 \\ 
   \hline
\end{tabular}
\end{table}

\end{center}
\end{columns}
\end{small}

\lz

\begin{itemize}
\item Both splits are equivalent in MCE
\item But: Split 2 results in purer nodes, both Brier score (Gini) and Log loss (Entropy) prefer 2nd split
\end{itemize}


%\framebreak


%Brier (Gini) and Bernoulli (Entropy) prefer 2nd split, \\
%\vspace{0.1cm}

%Gini $I(\Np) = \sum \limits_{k = 0}^1 \pikNh \left( 1-\pikNh \right)$

%\vspace{0.1cm}
%As Gini is defined based on the probability, we need to \\
%\vspace{0.05cm}
%apply reweighting with $\frac{|\Nl|}{|\Np|}$, or $\frac{|\Nr|}{|\Np|}$, respectively.

%\begin{alignat*}{6}
%\text{Formula}&:& \frac{|\Nl|}{|\Np|}\cdot 2 \cdot\pikNlh[0]\pikNlh[1] &+& \frac{|\Nr|}{|\Np|}\cdot 2 \cdot \pikNrh[0]\pikNrh[1] &=& \\[5ex]
%\text{Split 1}&:& \,\, \frac{1}{2} \,\cdot \, 2 \,\cdot\, \frac{3}{4} \,\cdot\, \frac{1}{4} \,&+&\,  \, \frac{1}{2} \,\cdot\, 2 \, \cdot\, \frac{1}{4} \,\cdot\, \frac{3}{4} &=&\;\, \frac{3}{8}\\[5ex]
%\text{Split 2}&:& \frac{3}{4}\, \cdot\, 2 \,\cdot\,\frac{2}{3}\,\cdot\,\frac{1}{3}\, &+& \frac{1}{4} \,\cdot\, 2 \,\cdot\, 0 \,\cdot\, 1 &=&\; \,\frac{1}{3}
% (Brier not introduced)
%$Split1: 300(0-\frac{1}{4})^2 + 100(1-\frac{1}{4})^2 + 100(0-\frac{3}{4})^2+300(1-\frac{3}{4})^2 = 150$\\ 
%$Split2: 400(0-\frac{1}{3})^2 + 200(1-\frac{1}{3})^2 = 133.3$
%\end{alignat*}

\end{vbframe}




\endlecture
\end{document}
