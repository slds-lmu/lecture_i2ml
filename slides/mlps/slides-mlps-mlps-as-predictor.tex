\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-nn}

\begin{document}

\lecturechapter{Single hidden layer neural networks}
\lecture{I2DL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Motivation}
\lz
\begin{itemize}
\item The graphical way of representing simple functions/models, like logistic regression. Why is that useful?
\lz
\item Because individual neurons can be used as building blocks of more complicated functions.
\lz
\item Networks of neurons can represent extremely complex hypothesis spaces.
\lz
\item Most importantly, it allows us to define the \enquote{right} kinds of hypothesis spaces to learn functions that are more common in our universe in a data-efficient way (see Lin, Tegmark et al. 2016).
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item As a single neuron is restricted to learning only linear decision boundaries, its performance on the following task is quite poor:
\begin{figure}
\centering
\scalebox{0.24}{\includegraphics{figure/cartesian.png}}
\end{figure}
\item However, the neuron can easily separate the classes if the original features are transformed (e.g., from Cartesian to polar coordinates): 
\begin{figure}
\centering
\scalebox{0.24}{\includegraphics{figure/polar.png}}
\end{figure}
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item Instead of classifying the data in the original representation,
\begin{figure}
\centering
\scalebox{1}{\includegraphics{figure/repold_f.png}}
\end{figure}
\item we classify it in a new feature space.
\begin{figure}
\centering
\scalebox{1}{\includegraphics{figure/repnew_f.png}}
\end{figure}
\end{itemize}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item Analogously, instead of a single neuron, 
\begin{figure}
\centering
\scalebox{0.6}{\includegraphics{figure/oldrep_n_f.png}}
\end{figure}
\item we use more complex networks.
\begin{figure}
\centering
\scalebox{0.6}{\includegraphics{figure/newrep_n_f.png}}
\end{figure}
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe} {Representation Learning}
  \begin{itemize}
    \vspace{5mm}
    \item It is %therefore 
    \textit{very} critical to feed a classifier the \enquote{right} features in order for it to perform well.
    \vspace{7mm}
    \item Before deep learning took off, features for tasks like machine vision and speech recognition were \enquote{hand-designed} by domain experts. This step of the machine learning pipeline is called \textbf{feature engineering}.
    \vspace{7mm}
    \item %The single biggest reason
     DL %is so important is that it 
    automates feature engineering. This is called \textbf{representation learning}.
  \end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Single Hidden Layer Networks}
\textbf{Single neurons} perform a 2-step computation:
\begin{enumerate}
\item \textbf{Affine Transformation:} a weighted sum of inputs plus bias.
\item \textbf{Activation:} a non-linear transformation on the weighted sum.
\end{enumerate}
\vspace{.5cm}
\textbf{Single hidden layer networks} consist of two layers (without input layer):
\begin{enumerate}
\item \textbf{Hidden Layer:} having a set of neurons.
\item \textbf{Output Layer:} having one or more output neurons.
\end{enumerate}
\vspace{.5cm}
\begin{itemize}
\item Multiple inputs are simultaneously fed to the network.
\vspace{.2cm}
\item Each neuron in the hidden layer performs a 2-step computation.
\vspace{.2cm}
\item The final output of the network is then calculated by another 2-step computation performed by the neuron in the output layer.
\end{itemize}
\end{vbframe}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  5

\begin{frame} {Single Hidden Layer Networks: Example}
Each neuron in the hidden layer performs an \textbf{affine transformation} on the inputs:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_1.png}} $$\begin{array}{l}
z_{\text {in }}^{(1)}=w_{11} x^{(1)}+w_{21} x^{(2)}+w_{31} x^{(3)}+b_1 \\
z_{\text {in }}^{(1)}= 3 * (-3) + (-9) * 1 + 2 * 5 + 5 = -3\end{array}$$}
\only<2>{\scalebox{.8}{\includegraphics{figure/singlelay_2.png}} $$\begin{array}{l}
z_{\text {in }}^{(2)}=w_{12} x^{(1)}+w_{22} x^{(2)}+w_{32} x^{(3)}+b_2 \\
z_{\text {in }}^{(2)}= 11 * (-3) + (-2) * 1 + 7 * 5 + 2 = 2\end{array}$$}
\only<3>{\scalebox{.8}{\includegraphics{figure/singlelay_3.png}}$$\begin{array}{l}
z_{\text {in }}^{(3)}=w_{13} x^{(1)}+w_{23} x^{(2)}+w_{33} x^{(3)}+b_3 \\
z_{\text {in }}^{(3)}= (-6) *(-3) + 3 * 1 + (-4) * 5 - 1 = 0\end{array}$$}
\only<4>{\scalebox{.8}{\includegraphics{figure/singlelay_4.png}}$$\begin{array}{l}
z_{\text {in }}^{(4)}=w_{14} x^{(1)}+w_{24} x^{(2)}+w_{34} x^{(3)}+b_4 \\
z_{\text {in }}^{(4)}= 6 * (-3) + (-1) * 1 + 5 * 5 + 1 = 7\end{array}$$}
\only<5>{\scalebox{.8}{\includegraphics{figure/singlelay_5.png}}}
\end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   1

\begin{frame} {Single Hidden Layer Networks: Example}
Each hidden neuron performs a non-linear \textbf{activation} transformation on the weight sum:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_6.png}}$$\begin{array}{l}
z_\text{out}^{(i)}=\sigma\left(z_{\text {in}}^{(i)}\right)=\frac{1}{1+e^{z_{\text {in }}^{(i)}}}\end{array}$$}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  2

\begin{frame} {Single Hidden Layer Networks: Example}
The output neuron performs an \textbf{affine transformation} on its inputs:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_7.png}}$$\begin{array}{l}
f_\text {in}=u_1 z_\text{out}^{(1)}+u_2 z_\text {out}^{(2)}+u_{3} z_{\text {out}}^{(3)}+u_{4} z_{\text {out }}^{(4)}+c\end{array}$$}
\only<2>{\scalebox{.8}{\includegraphics{figure/singlelay_8.png}}$$\begin{array}{l}
f_\text {in}=u_1 z_\text{out}^{(1)}+u_2 z_\text {out}^{(2)}+u_{3} z_{\text {out}}^{(3)}+u_{4} z_{\text {out }}^{(4)}+c \\ f_{\text {in}}= 3 * 0.05 + (-12) * 0.88 + 8* 0.50 + 1 * 0.99 + 6= 0.57\end{array}$$}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   1

\begin{frame} {Single Hidden Layer Networks: Example}
The output neuron performs a non-linear \textbf{activation} transformation on the weight sum:
\begin{figure}
\centering
\only<1>{\scalebox{.8}{\includegraphics{figure/singlelay_9.png}}$$\begin{array}{l}
f_\text{out}=\sigma (f_\text{in})= \frac{1}{1+e^{f_\text{in}}} \\ f_\text{out}=\frac{1}{1+e^{0.57}}=0.64\end{array}$$}
\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame} {Hidden Layer: Activation Function}
\begin{itemize}
\item If the hidden layer does not have a non-linear activation, the network can only learn linear decision boundaries.
\item A lot of different activation functions exist.
%\item For simplification purposes, we drop the bias terms in notation and let $\sigma %= \text{id}$. Then:
 %   \begin{eqnarray*}
      % \fx & = & \tau(\\hidz) \\
      %   & = & \tau(\wtu^T \sigma(\Wmat^T \xv)) \\
      %  & = & \tau(\wtu^T\Wmat^T \xv) = \tau(\mathbf{v}^T \xv)
 %     \end{eqnarray*}
  %    where $ \mathbf{v} = \Wmat\wtu$. It can be seen that $\fx$ can only yield a linear decision boundary.
 \end{itemize}
\end{frame}


\begin{vbframe}{Hidden Layer: Activation Function}
%\textbf{Note:} if the hidden layer does not have a non-linear activation, the network can only learn linear decision boundaries.
\begin{blocki}{ReLU Activation:}
\item Currently the most popular choice is the ReLU (rectified linear unit):
$$ \sigma (v) = \max(0,v) $$
\end{blocki}
\begin{figure}
\scalebox{0.9}{\includegraphics{figure_man/relu.png}}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocki}{Hyperbolic Tangent Activation:}
\item Another choice might be the hyperbolic tangent function:
$$ \sigma (v) = \text{tanh}(v) = \frac{\text{sinh}(v)}{\text{cosh}(v)} = 1 - \frac{2}{\exp(2v) + 1}$$
\end{blocki}
\begin{figure}
\scalebox{0.9}{\includegraphics{figure_man/tanh.png}}
\end{figure}
\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{blocki}{Sigmoid Activation Function:}
\item The sigmoid function can be used even in the hidden layer:
$$ \sigma(v) = \frac{1}{1+\exp (-v)} $$
\end{blocki}
\begin{figure}
\scalebox{1}{\includegraphics{figure_man/sigmoid.png}}
\end{figure}
\end{vbframe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\endlecture
\end{document}