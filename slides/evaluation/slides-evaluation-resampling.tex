\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/crossvalidation.png}
\newcommand{\learninggoals}{
\item Understand how resampling techniques extend the idea of simple train-test splits
\item Understand the ideas of cross-validation, bootstrap and subsampling
\item Understand what pessimistic bias means}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.


% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-automl.tex}
%! includes: evaluation-test

\lecturechapter{Evaluation: Resampling}
\lecture{Introduction to Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Resampling}

\begin{small}
\begin{itemize}
  \item \textbf{Goal}: assess performance of learning algorithm.
   \item Use the data at hand efficiently.
   \item Repeatedly split in train and test, then average results.
  \item Make training sets large (to keep the pessimistic bias small),
  and reduce variance introduced by smaller test sets through many repetitions / 
  averaging of results.
\end{itemize}
\end{small}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1q7WN1_YKHedIPNySiZBEraLtTkHRX12Ej6M6ISbfMD0/edit?usp=sharing
\includegraphics[width=0.6\textwidth]{figure_man/resampling_error.pdf}
% % FIGURE SOURCE: No source
% \includegraphics[width=0.7\textwidth]{figure_man/ml_abstraction-crop.pdf}
\end{center}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Resampling strategies}

\begin{itemize}
  \item Different \textbf{resampling strategies} exist to balance bias against 
  variance, e.g., holdout sampling or cross-validation.
  \item To ease notation, we represent our train and test sets by index
  vectors $J_\mathrm{train} \in \nset^{m_\mathrm{train}}$ and $J_\mathrm{test}
  \in \nset^{m_\mathrm{test}}$, and define a resampling strategy with 
  $B$ train-test splits by $$S = \left((J_{\mathrm{train}, 1}, 
  J_{\mathrm{test}, 1}), \dots, (J_{\mathrm{train} ,B}, J_{\mathrm{test}, B}) 
  \right).$$
  % \item With this, we can denote the corresponding vector of labels as
  % $\yv_J = \left(y^{(J^{(1)})},\dots, y^{(J^{(m)})} \right) \in \Yspace^m$ and
  % the matrix of prediction scores as
  % $\bm{F}_{J,f} = \left( f(\xv^{(J^{(1)})}), \dots, f(\xv^{(J^{(m)})}) \right)
  % \in \R^{m\times g}$.
  % TODO translate to latex-math once notation is updated there
  \item Based on $S$, we can express our \textbf{generalization error} 
  estimate for arbitrary resampling strategies as
  \begin{equation*}
    \begin{split}
      \small
      \widehat{\mathrm{GE}}_S(\inducer, \lambdav, \rho) =
      \textrm{agg} \Bigg(& \rho \left( \yv_{J_{\mathrm{test}, 1}},
      \bm{F}_{J_{\mathrm{test}, 1}, \fh_{\mathcal{D}_{\mathrm{train}, 1}, 
      \lambdav}} \right), \\
      &\dots, \rho \left(\yv_{J_{\mathrm{test}, B}},
      \bm{F}_{J_{\mathrm{test}, B}, \fh_{\mathcal{D}_{\mathrm{train}, B}, 
      \lambdav}}
      \right) \Bigg),
    \end{split}
  \end{equation*}
  where the aggregation $\textrm{agg}$ is typically chosen to be the mean.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Cross-Validation}

\begin{itemize}
  \item Split the data into $k$ roughly equally-sized partitions.
  \item Use each part once as test set and join the $k-1$ others for training.
  \item Obtain $k$ test errors and average.
\end{itemize}

\lz

Example: 3-fold cross-validation:

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1sKtnj5nIQrcOGU7rTisMsppUGOk7UX2gbjKhtQmTX7g/edit?usp=sharing
\includegraphics[width=8cm]{figure_man/crossvalidation.png}
\end{center}
\end{vbframe}

\begin{vbframe}{Cross-Validation - Stratification}

Stratification tries to preserve the distribution of the target class (or any specific categorical feature of interest) in each fold.

\lz

Example of stratified 3-fold cross-validation:

\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/eval_resample_1} 

}

\end{knitrout}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Cross-Validation}

\begin{itemize}
  \item 5 or 10 folds are common.%, they use 80\% and 90\% of data in training
  \item $k = n$ is known as leave-one-out (LOO) cross-validation.
  \item GE estimates tend to be pessimistically biased: 
  size of the training sets is $ n- (n/k) < n$. \\
  $\Rightarrow$ Bias increases as $k$ gets smaller.
  \item The $k$ performance estimates are dependent because
  of the structured overlap of the training sets.\\
  $\Rightarrow$ Variance of the estimator increases for very large $k$ 
  (approaching LOO), when training sets nearly completely overlap.
  % \item LOO is nearly unbiased, but has high variance.
  \item Repeated $k$-fold CV (multiple random partitions)
  can improve error estimation for small sample sizes.
\end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bootstrap}

The basic idea is to randomly draw $B$ training sets of size $n$ with
replacement from the original training set $\Dtrain$:
% \begin{eqnarray*}
% \Dtrain^1 &=& \{z^1_1, \ldots, z^1_n\}\\
% \vdots& \\
% \Dtrain^B &=& \{z^B_1, \ldots, z^B_n\}
% \end{eqnarray*}

\begin{center}
\begin{tikzpicture}[scale=1]
% style
\tikzstyle{rboule} = [circle,scale=0.7,ball color=red]
\tikzstyle{gboule} = [circle,scale=0.7,ball color=green]
\tikzstyle{bboule} = [circle,scale=0.7,ball color=blue]
\tikzstyle{nboule} = [circle,scale=0.7,ball color=black]
\tikzstyle{sample} = [->,thin]

% title initial sample
\path (3.5,3.75) node[anchor=east] {$\Dtrain$};

% labels
\path (3.5,3)   node[anchor=east] {$\Dtrain^1$};
\path (3.5,2.5) node[anchor=east] {$\Dtrain^2$};
\path (3.5,1.5) node[anchor=east] {$\Dtrain^B$};

\path (3.5,2) node[anchor=east] {$\vdots$};
\path[draw,dashed] (3.75,2.0) -- (4.5,2.0);

% initial sample
\path ( 3.75,3.75) node[rboule] (j01) {};
\path ( 4.00,3.75) node[gboule] (j02) {};
\path ( 4.25,3.75) node[bboule] (j03) {};
\path ( 4.5,3.75) node[nboule] (j20) {};

% bootstrap 1
\path ( 3.75, 3.0) node[rboule] {};
\path ( 4.00, 3.0) node[rboule] {};
\path ( 4.25, 3.0) node[bboule] {};
\path ( 4.5, 3.0) node[nboule] (b1) {};

% bootstrap 2
\path ( 3.75, 2.5) node[gboule] {};
\path ( 4.00, 2.5) node[bboule] {};
\path ( 4.25, 2.5) node[gboule] {};
\path ( 4.5, 2.5) node[rboule] (b2) {};

% bootstrap N
\path (3.75,1.5) node[gboule] {};
\path (4,1.5) node[rboule] {};
\path (4.25,1.5) node[nboule] {};
\path (4.5,1.5) node[nboule] (bN) {};

% arrows
\path[sample] (j20.east) edge [out=0, in=60] (b1.east);
\path[sample] (j20.east) edge [out=0, in=60] (b2.east);
\path[sample] (j20.east) edge [out=0, in=60] (bN.east);
\end{tikzpicture}
\end{center}

We define the test set in terms of out-of-bag observations
$\Dtest^b = \Dtrain \setminus \Dtrain^b$.

\framebreak

\begin{itemize}
  \item Typically, $B$ is between $30$ and $200$.
  \item The variance of the bootstrap estimator tends to be smaller than the
  variance of $k$-fold CV.
  \item The more iterations, the smaller the variance of the estimator.
  \item Tends to be pessimistically biased
  (because training sets contain only about $63.2 \%$ of the unique observations).
  \item Bootstrapping framework allows for inference 
  (e.g., detecting significant performance differences between learners).
  \item Extensions exist for very small data sets that also use the training error for  estimation: B632 and B632+.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Subsampling}

\begin{itemize}
  \item Repeated hold-out with averaging, a.k.a. Monte Carlo CV.
  \item Similar to bootstrap, but draws without replacement.
  \item Typical choices for splitting: $\frac{4}{5}$ or $\frac{9}{10}$ for 
  training.
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/drawings/d/1q7WN1_YKHedIPNySiZBEraLtTkHRX12Ej6M6ISbfMD0/edit?usp=sharing
\includegraphics[width=0.7\textwidth]{figure_man/resampling_error.pdf}
\end{center}
\begin{itemize}
  \item The smaller the subsampling rate, the larger the pessimistic bias.
  \item The more subsampling repetitions, the smaller the variance.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bias-Variance Analysis for Subsampling}

  \begin{itemize}
    \item Let's reconsider our hold-out experiment on the spiral data from the train-test unit (maybe re-read it again)
    \item Again, we use split-rates $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
    \item But now we compare 50 subsampling experiments with $50 \cdot 50$ hold-out experiments per split.
    \item Every subsampling experiment is the result of averaging 50 hold-out experiments, so each performance estimate is much more reliable (but also more expensive) than one computed by a hold-out experiment.
  \end{itemize}

\framebreak


\begin{center}
% FIGURE SOURCE: eval-resampling-example
\includegraphics[width=0.9\textwidth]{figure/eval-resampling-example-1}
\end{center}



\begin{itemize}
  \item Both experiments are compared to the "real" mmce (black line).
  \item Subsampling has the same pessimistic bias for small split rates, but much less variance overall.
  \item This allows to use much smaller test sets with good results.
\end{itemize}

\framebreak

\begin{center}
% FIGURE SOURCE: eval-resampling-example
\includegraphics[width=0.7\textwidth]{figure/eval-resampling-example-2}
\end{center}


\begin{itemize}
  \item The MSE is overall better for subsampling compared to hold-out
  \item The optimal split rate now is a higher $s \approx 0.9$
  \item We see an increase in variance at the end because the training sets become more overlapping and not independent
\end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Resampling discussion}

\begin{itemize}
 \item In ML we fit, at the end, a model on all our given data.\\

 \item \textbf{Problem:} We need to know how well this model performs in the future, but no data is left to reliably do this.\\
$\Rightarrow$ Approximate using hold-out / CV / bootstrap / resampling estimate\\ 

 \item \textbf{But:} pessimistic bias because we don't use all data points\\

 \item Final model is (usually) computed on all data points.

 \item Strictly speaking, resampling only produces one number, the performance estimator.
  It does NOT produce models, paramaters, etc. These are intermediate results and discarded.
  
  \item The model and parameters are obtained when we fit the learner finally on the complete data.

\end{itemize}

\framebreak

\begin{itemize}
  \item 5CV or 10CV have become standard
  \item Do not use hold-out, CV with few iterations, or subsampling with a low subsampling rate for small samples, since this can cause the estimator to be extremely biased, with large variance.
  \item For small data situation with less than 500 or 200 observations, use LOO or probably better repeated CV
  \item A $\D$ with $|\D| = 100.000$ can have small-sample properties if one class has few observations 
   \item For some models, computationally fast calculations or approximations for the LOO exist
  \item Research indicates that subsampling has better properties than
    bootstrapping. The repeated observations can cause problems in training,
    especially in nested setups where the \enquote{training} set is split up again.
\end{itemize}
\end{vbframe}

\endlecture
\end{document}
