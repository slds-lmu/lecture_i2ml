\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}
\input{../../latex-math/ml-hpo.tex}


\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
Evaluation 
}{% Lecture title  
Overfitting and Underfitting
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure/eval_ofit_1
}{% Learning goals, wrapped inside itemize environment
  \item Understand definitions of overfitting and underfitting
  % \item Relation of both to bias-variance of a learner
}

% ------------------------------------------------------------------------------

% overfitting can also happen with linear models
%& overftiing is a function of training data size and capacity and noise
%  grafik machen

\begin{vbframe}{Underfitting}

\begin{itemize}
  \item Occurs if model does not reflect true shape of underlying
      function
  \item Hence, predictions will be less good as they could be
  \item High train error and high test error
  \item Hard to detect, as we don't know what the Bayes error is for a task
\end{itemize}
\lz
\begin{columns}
\begin{column}{0.5\textwidth}
  \raggedright
  Underfitted model\\
  \includegraphics[width=\textwidth]{figure/eval_ofit_1u}
\end{column}
\begin{column}{0.5\textwidth}
  \raggedright
  Appropriate model\\
  \includegraphics[width=\textwidth]{figure/eval_ofit_1a}
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Overfitting}

\begin{itemize}
  \item Overfitting occurs when the model reflects noise or artifacts in training data,
      which do not generalize
  \item Small train error, at cost of test high error
  \item Hence, predictions of overfitting models cannot be trusted -
      but proper ML evaluation workflows should make it visible
\end{itemize}
\lz
\begin{columns}
\begin{column}{0.5\textwidth}
  \raggedright
  Overfitted model\\
  \includegraphics[width=\textwidth]{figure/eval_ofit_1o}
\end{column}
\begin{column}{0.5\textwidth}
  \raggedright
    Appropriate model\\
  \includegraphics[width=\textwidth]{figure/eval_ofit_1a}
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{Under- and Overfitting in Regression}
\begin{itemize}
\item Poly-Regression, on data from sinusoidal function
\item LM underfits, high-d overfits
\end{itemize}
\begin{center}
\includegraphics[width=0.7\textwidth]{figure/eval_train_2} 
\end{center}
\end{vbframe}


\begin{vbframe}{Mathematical Definitions}

\begin{itemize}
\item Nearly no reference does that, here is one approach
\item Underfitting
    $UF(\fh, L) : = \GEfL - \GE(\fbayes, L)$\\
    Diff in GE between $\fh$ and the Bayes optimal model
\item Overfitting
    $OF(\fh, L) : = \GEfL - \riske(\fh, L)$\\
    Diff between (theoretical) GE and training error
\end{itemize}

\lz

\begin{center}
\includegraphics[width=0.3\textwidth,trim={0.8cm 0.6cm 3cm 0},clip]{figure/eval_ofit_1a}
\includegraphics[width=0.3\textwidth,trim={0.8cm 0.6cm 3cm 0},clip]{figure/eval_ofit_1u}
\includegraphics[width=0.3\textwidth,trim={0.8cm 0.6cm 3cm 0},clip]{figure/eval_ofit_1o}
\end{center}
NB: Now, RHS is both UF and OF, let's say OF has "prio". 
\end{vbframe}


\begin{vbframe}{Overfitting trade-offs}

The potential for overfitting is influenced by:
\begin{itemize}
  \item Complexity of hypothesis space
  \item Amount of training data
  \item Dimensionality of feature space
  \item Irreducible noise 
\end{itemize}
\lz
Implications:
\begin{itemize}
\item The larger / more complex is $\Hspace$, 
    the more data we need to tell candidate models apart
\item The less data we have, the more we need to stick with "constrained" $\Hspace$
\item OF can happen for LMs too: If feature space is very high-dim
\item Tightly connected to the bias-var-noise decomposition of GE
of a learner ($\rightarrow$ which we study elsewhere).
\end{itemize}
\end{vbframe}


\begin{vbframe}{Complexity vs GE}

\begin{center}
\includegraphics[width = 0.9\textwidth]{figure/eval_ofit_3} 
\end{center}


\vfill

\begin{itemize}
\item Common U-shape of GE if complexity or train-rounds go up.
\item Optimal level of complexity:\\
    Simplest model for which GE is not significantly outperformed
\item We could also call "Point of OF" the point where GE goes up. 
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------
\begin{vbframe}{Avoiding Overfitting}

\begin{itemize}
  \item Use more or better data -- not always possible, but maybe can augment data, 
      e.g., for images
  \item Constrain $\Hspace$ directly by using less complex model classes
  \item Many learners come with HPs that can 
      constrain complexity
  \item Use "early-stopping"
\item Occam's razor in model selection: 
    If GE not strongly reduced for more complex class,
    use the simpler model.
\end{itemize}

\lz

All of the above are methods of regularization,
which we study in a dedicated chapter.

\end{vbframe}

\endlecture
\end{document}
