\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}

\newcommand{\titlefigure}{figure_man/cost-curves-2}
\newcommand{\learninggoals}{
\item Understand precision-recall curves
\item Understand cost curves
}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: PR and Cost Curves }
\lecture{Introduction to Machine Learning}
\sloppy

% ------------------------------------------------------------------------------

% \begin{vbframe}{Beyond AUC}
%
% Cost curves, precision-recall curves
%
% \begin{itemize}
%   \item Confusion matrices give rise to a plethora of metrics.
%   \item Besides ROC curves and AUC, many alternative perspectives on evaluation
%   can be derived from combining different ROC metrics.
%   \item For example, we can construct \textbf{precision-recall curves} which are
%   often preferred in cases of strong class imbalance.
%   \item \textcolor{blue}{Fictional example, mini case study, ...?}
% \end{itemize}
%
% \centering
% \includegraphics[width=0.3\textwidth]{figure_man/placeholder_precision_recall}
%
% \begin{itemize}
%   \item For practitioners it is vital to understand what should be
%   evaluated exactly, and which measure is appropriate.
% \end{itemize}
%
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Precision-Recall Curves}

%\textbf{Motivation}: ROC curves should mainly be used in binary classification
%problems.

\begin{footnotesize}

Might be more informative for highly imbalanced data ($\nn \gg \np$) than TPR-FPR
% , precision-recall
% (PR) curves may be more useful than ROC curves:
% of an algorithms performance.

\begin{itemize}
  \item Precision = $\rho_{PPV}$ = $\frac{TP}{TP + FP}$, recall = $\rho_{TPR}$ = $\frac{TP}{TP + FN}$
  (do not depend on TN).
  \item We might call them TPR-PPV curve
  %\item Note that both measures are not dependent on true negatives.
  \item Figure (a): ROC curve: both learner seem to perform well
  \item Figure (b): PR curve: room for improvement (top-right is optimal)
  \item PR reveals better that algo 2 has advantage over 1
  % which is due to imbalanced classes.
\end{itemize}

\end{footnotesize}

\begin{figure}
  \centering
  \scalebox{0.8}{\includegraphics{figure_man/roc-pr_edited.png}}
  \tiny
  \\Davis and Goadrich (2006): The Relationship Between Precision-Recall and
  ROC Curves (\href{https://www.biostat.wisc.edu/~page/rocpr.pdf}
  {\underline{URL}}).
  % \tiny{\\ Credit: Jesse Davis and Mark Goadrich \\}
\end{figure}

% {\tiny{Davis and Goadrich (2006): The Relationship Between Precision-Recall and
% ROC Curves (\href{https://www.biostat.wisc.edu/~page/rocpr.pdf}{\underline{URL}}).
% \emph{\url{https://www.biostat.wisc.edu/~page/rocpr.pdf}}}\par}

% Source: http://people.cs.bris.ac.uk/~flach/ICML04tutorial/ROCtutorialPartI.pdf
%\tiny{Source for material: https://www.biostat.wisc.edu/~page/rocpr.pdf}
%\tiny{Source for material: https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba}
%\tiny{Source for material: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/}
%\tiny{Source 6: http://binf.gmu.edu/mmasso/ROC101.pdf}
% Source: https://link.springer.com/content/pdf/10.1007/s10994-006-8199-5.pdf

\framebreak

\begin{itemize}
  \item In imbalanced classes with $\nn \gg \np$:
      Large change in FP yields small change in FPR
  \item PPV likely more informative
\end{itemize}

\lz

\begin{columns}
 \begin{column}{0.45\textwidth}
 \underline{FP=10}:\\
 %Proportion $\np/\nn = 1$\\
 \lz
 {
 \tiny
 \centering
 \tiny
 \begin{tabular}{|l|c|c|}
                 \hline
                & True +1 & True -1 \\ \hline
 Pred. Pos & 100            & 10            \\ \hline
 Pred. Neg & 10            & 9990           \\ \hline
 Total  & 110            & 10000           \\ \hline
 \end{tabular}
 }

 \medskip
 %$\text{E} = 35/100 = 0.35$\\
 $\text{FPR} = 0.001$\\
 $\text{PPV} = 1/11$
\end{column}
\begin{column}{0.45\textwidth}
 \underline{FP=100}:\\
%%  Proportion $\np/\nn = 2$\\
 \lz
 {
 \tiny
 \begin{tabular}{|l|c|c|}
                 \hline
                & True +1 & True -1 \\ \hline
 Pred. +1 & 100            & 100            \\ \hline
 Pred. -1 & 10            & 9900           \\ \hline
 Total  & 110            & 10000           \\ \hline
 \end{tabular}
 }

 \medskip
 $\text{FPR} = 0.01$\\
 $\text{PPV} = 1/2$
\end{column}
\end{columns}

\vfill

RHS: Given test says +1: It's now a coin flip that this is correct.

%\begin{center}
%  \includegraphics[width=0.8\textwidth]{figure_man/roc-confusion_matrix.png}
%\end{center}

% \begin{itemize}
%   \item ROC problem is $FPR$ in case of $\nn \gg \np$, which is often
%   very small as TNs are usually high \\
%   $\Rightarrow$ Large change in FP yields small change in $FPR$
%   \item PPV: compares FP to TP (not TN) and does not suffer from this
%   issue. Captures effect of large $\nn$.
% \end{itemize}

% \begin{center}
%   \includegraphics[width=0.8\textwidth]{figure_man/roc-confusion_matrix.png}
% \end{center}

\framebreak

% \begin{footnotesize}

\vspace{-0.2cm}
\begin{itemize}
  \item Top row: Imbal classes with $\pi = 0.003$
  \item Bottom: balanced with $\pi = 0.5$
  % \item Task and learners remain the same, only class distribution has changed.
  \item ROC curves (LHS) are similar
  \item PR curve (RHS) changes strongly from imbal to bal classes
  %\item Note that the superiority of classifiers in terms of performance can change with a skewed class distribution.
\end{itemize}

% \end{footnotesize}

%replaced graph: (too small)
% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figure_man/roc-pr-all.png}
%     \tiny{\\ Credit: Tom Fawcett \\}
% \end{figure}
% {\tiny{Tom Fawcett (2004): ROC Graphs: Notes and Practical Considerations for Researchers. \emph{\url{http://binf.gmu.edu/mmasso/ROC101.pdf}}}\par}

\begin{figure}
  \centering
  \scalebox{0.55}{\includegraphics[width=\textwidth]
  {figure_man/roc-pr-imbalanced_edited.png}}
  \tiny
  \\ Wissam Siblini et. al. (2004): Master your Metrics with Calibration
  (\href{https://arxiv.org/pdf/1909.02827.pdf}{\underline{URL}}).
  % \tiny{\\ Wissam Siblini et. al. (2004): Master your Metrics with Calibration.
  % \emph{\url{https://arxiv.org/pdf/1909.02827.pdf}}}\par
  % \tiny{\\ Credit: Wissam Siblini et. al. \\}
\end{figure}

% \vspace{-0.25cm}
%
% {\tiny{Wissam Siblini et. al. (2004): Master your Metrics with Calibration. \emph{\url{https://arxiv.org/pdf/1909.02827.pdf}}}\par}

\end{vbframe}

%\begin{vbframe}{PR Curves - Conclusion}

%\begin{itemize}
%  % \item ROC and PR curves for given algorithms contain the same points.
%  %\item A curve that dominates in ROC space iff it dominates in PR space.
%  % \item Analogous to the convex hull in ROC space, there is a convex hull in PR
%  space (same points omitted as in convex hull in ROC space).
%  \item An algorithm that optimizes the AUROC is not guaranteed to optimize
%  the AUPRC.
%  \begin{itemize}
%    \item Use ROC curves when the numbers of samples in both classes are
%    roughly equal.
%    \item Use PR curves when there is a moderate to large class
%    imbalance and predicting positive instances is more relevant.
%  \end{itemize}
%\end{itemize}
%\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Cost curves}

\begin{itemize}
  \item Directly plot the misclassif costs / error 
  % to determine the best classifier.
  %(ROC isometrics allow this only indirectly).
  %\item Determining the superiority of a classifier can be difficult using the ROC isometrics.
  \item Might be easier to interpret than ROC, especially in case of 
      different misclassif costs or priors
  % distributions. %, or more generally the skew are known.
\end{itemize}

\lz

%\tiny{Source for material: Evaluating Learning Algorithms Chapter 4.4.3}
%\vspace{-0.1cm}

\begin{columns}%[T]
\begin{column}{0.6\textwidth}
  \small
  \raggedright
  \textbf{Example:} %Interpretation of two ROC curves
  \begin{itemize}
  \item $f_1$ and $f_2$ with intersecting ROC curves 
      % at point (FPR, TPR) = (0.25, 0.69)
  \item $f_2$ dominates first, then $f_1$ 
  % \item After intersection, $f_1$ dominates $f_2$
  \end{itemize}

\lz

  \textbf{BUT:} Unclear for which thresholds, costs or class distribs $f_2$ better than $f_1$

  % $\Rightarrow$ Cost curves
\end{column}
\begin{column}{0.39\textwidth}
  %\begin{figure}
    \centering
    \tiny ROC curves for $f_1$ and $f_2$
    \includegraphics[width=\textwidth]{figure_man/cost-curves-1.png}
    {\tiny Nathalie Japkowicz (2004): Evaluating Learning Algorithms : A
    Classification Perspective. (p. 125)}
  %\end{figure}
\end{column}
\end{columns}

%\includegraphics[width=\textwidth]{figure_man/cost-curves-1.png}
% \end{minipage}
% \vspace{1.5 cm}
% {\tiny{Nathalie Japkowicz (2004): Evaluating Learning Algorithms : A Classification Perspective. (p. 125)}}

\end{vbframe}



% ------------------------------------------------------------------------------



\begin{vbframe}{Cost curves}
\small

%Let $\rp = \P(y = 1)$ be the proportion of positive instances.
With law total probab, can write misclassif rate as function of $\rp$:
\begin{align*}
\rho_{MCE}(\rp)
&= (1 - \rp) \cdot \P(\hat y = 1 | y = 0) + \rp \cdot \P(\hat y = 0 | y = 1) \\
&= (1 - \rp) \cdot FPR + \rp \cdot FNR \\
&= (FNR - FPR) \cdot \rp + FPR
\end{align*}
Can do the same for costs:
% Similarly, expected misclassification costs can be written as a function of $\rp$:
$$Costs(\rp) = (1 - \rp) \cdot FPR \cdot cost_{FP} + \rp \cdot FNR \cdot cost_{FN}$$
$$Costs_{norm}(\rp) = \tfrac{(1 - \rp) \cdot FPR \cdot cost_{FP} \; + \; \rp \cdot FNR \cdot cost_{FN}}{(1 - \rp) \cdot cost_{FP} \; + \; \rp \cdot cost_{FN}} \in [0,1]$$

\begin{columns}[T]
\begin{column}{0.64\textwidth}
\small
\begin{itemize}
\itemsep0em
% \item $Costs_{norm}$ (normalized costs) is a natural extension of MCE
\item Denominator = $\max(Costs)$ =  all obs misclassified (i.e., $FPR = FNR = 1$).
\item If $cost_{FN} = cost_{FP}$, than $Costs_{norm} = \rho_{MCE}$
\end{itemize}
\end{column}
\begin{column}{0.34\textwidth}
\tiny

%\begin{center}
\centerline{Confusion matrix}
\begin{tabular}{cc|cc}
    & &\multicolumn{2}{c}{True class} \\
    & & $y=1$ & $y=0$  \\
 \hline
    \multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & TP                 & FP\\
    & $\hat y$ = 0 & FN              & TN\\
    %& & P(y = 1) & P(y = 0)
\end{tabular}

\lz

\centerline{Cost matrix}
\begin{tabular}{cc|cc}
    & &\multicolumn{2}{c}{True class} \\
    & & $y=1$ & $y=0$  \\
 \hline
    \multirow{2}{*}{\parbox{0.3cm}{Pred.  class}}& $\hat y$ = 1     & 0                 & $cost_{FP}$\\
    & $\hat y$ = 0 & $cost_{FN}$              & 0\\
    %& & P(y = 1) & P(y = 0)
\end{tabular}
%\end{center}

\end{column}
\end{columns}

% \begin{columns}
% \begin{column}{0.5\textwidth}
% {\centering Confusion matrix
% \begin{center}
% \begin{tabular}{cc|cc}
%     & &\multicolumn{2}{c}{True class} \\
%     & & $y=1$ & $y=0$  \\
%  \hline
%     \multirow{2}{*}{\parbox{0.6cm}{Pred.  class}}& $\hat y$ = 1     & TP                 & FP\\
%     & $\hat y$ = 0 & FN              & TN\\
%     %& & P(y = 1) & P(y = 0)
% \end{tabular}
% \end{center}
% }
% \end{column}
% \begin{column}{0.5\textwidth}
% {\centering Cost matrix
% \begin{center}
% \begin{tabular}{cc|cc}
%     & &\multicolumn{2}{c}{True class} \\
%     & & $y=1$ & $y=0$  \\
%  \hline
%     \multirow{2}{*}{\parbox{0.6cm}{Pred.  class}}& $\hat y$ = 1     & 0                 & $cost_{FP}$\\
%     & $\hat y$ = 0 & $cost_{FN}$              & 0\\
%     %& & P(y = 1) & P(y = 0)
% \end{tabular}
% \end{center}
% }
% \end{column}
% \end{columns}


\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Cost curves}

\begin{footnotesize}

\begin{itemize}
  \item Simplifying assumption: equal misclassification costs, i.e.,
  $cost_{FN} = cost_{FP}$.
  \item Normalized costs (or error rate in the case of
  $cost_{FN} = cost_{FP}$) is plotted as a function of the proportion of
  positive instances, $\rp = \P(y = 1)$.
  \item Cost curves are pointâ€“line duals of ROC curves, i.e., a single
  classifier is represented by a point in the ROC space and by a line in the
  cost space.
\end{itemize}

\end{footnotesize}

\begin{figure}
  \centering
  \scalebox{0.8}{\includegraphics[width=\textwidth]
  {figure_man/cost-curves-0.png}}
  \tiny
  \\Chris Drummond and Robert C. Holte (2006): Cost curves: An improved
  method for visualizing classifier performance. \\Machine Learning, 65, 95-130
  (\href{https://www.semanticscholar.org/paper/Cost-curves\%3A-An-improved-method-for  -visualizing-Drummond-Holte/71708ce984e0896e7383435913547e770572410e}
  {\underline{URL}}).
  % \tiny{\\ Credit: Chris Drummond and Robert C. Holte  \\}
\end{figure}

% {\tiny{Chris Drummond and Robert C. Holte (2006): Cost curves: An improved
% method for visualizing classifier performance. Machine Learning, 65, 95-130.
% \emph{\url{https://www.semanticscholar.org/paper/Cost-curves\%3A-An-improved-method-for-visualizing-Drummond-Holte/71708ce984e0896e7383435913547e770572410e}}}\par}

\end{vbframe}


% ------------------------------------------------------------------------------



\begin{vbframe}{Cost curves - Example}

% \begin{itemize}
  %\item The convex hull of the ROC space is the lower envelope created of all classifier lines in the cost space.
  %\item The misclassification error is plotted as a function of the probability of an observation being from the positive class.
%   \item Functional form of the cost curve of a classifier:
%   $error = (FNR - FPR) \cdot \rp + FPR$ %, \;\;\;$ (Note: $P(+) = \rp$)
% \end{itemize}

Cost curve of a classifier with slope $(FNR - FPR)$ and intercept $FPR$:
$$\rho_{MCE}(\rp) = (FNR - FPR) \cdot \rp + FPR$$

\begin{columns}[T]
\begin{column}{0.49\textwidth}

%$error = (\rho_{FNR} - \rho_{FPR}) \cdot \rp + \rho_{FPR}$.
\begin{itemize}
\item Hard classifiers are points (TPR, FPR) in ROC space
\item The cost curve of a classifier connects $(\rp, \rho_{MCE})$-points at
$(0, FPR)$ and $(1, 1-TPR)$
\item Classifier 3 always dominates classifier 1
\item Classifier 3 is better than classifier 2 when $\rp < 0.7$
\end{itemize}

\end{column}

\begin{column}{0.49\textwidth}
\centering
Cost curves plot different values of $\rp$ vs. $\rho_{MCE}(\rp)$

\includegraphics[width=\textwidth]{figure_man/cost-curves-3.png}
\end{column}
\end{columns}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{frame}{Cost curves - Example}

\begin{footnotesize}

\begin{itemize}
  \item<1-> Horizontal dashed line: worst classifier (100\% error rate for all
  $\rp$).\\
  $\Rightarrow FNR = FPR = 1$
  \item<1-> x-axis: perfect classifier (0\% error rate for all
  $\rp$). $\Rightarrow FNR = FPR = 0$
  \item<2-> Dashed diagonal lines: trivial classifiers, i.e., ascending diagonal
  always predicts negative instances ($\Rightarrow FNR = 1$ and $FPR = 0$) and vice versa.
  \item<3-> Descending/ascending bold lines:
  two families of classifiers $A$ and $B$ (represented by points in their
  respective ROC curves).
\end{itemize}

\end{footnotesize}

% Animation: https://docs.google.com/presentation/d/1YWmJeb9etd8-dtwU8jfmJZPbJKoWTuz7CQtNnSEtu54/edit?usp=sharing
\begin{columns}[T]
\begin{column}{0.59\textwidth}
%\begin{center}
\includegraphics<1>[page=1, trim = 45 20 50 45, clip, width=\textwidth]{figure_man/cost-curves.pdf}
\includegraphics<2>[page=2, trim = 45 20 50 45, clip, width=\textwidth]{figure_man/cost-curves.pdf}
\includegraphics<3>[page=3, trim = 45 20 50 45, clip, width=\textwidth]{figure_man/cost-curves.pdf}
%\end{center}
\end{column}
\begin{column}{0.4\textwidth}
\footnotesize
$\rho_{MCE} = (FNR - FPR) \cdot \rp + FPR$
{\centering
\begin{center}
Confusion matrix
\begin{tabular}{cc|cc}
    & &\multicolumn{2}{c}{True class} \\
    & & $y=1$ & $y=0$  \\
 \hline
    \multirow{2}{*}{\parbox{0.6cm}{Pred.  class}}& $\hat y$ = 1     & TP                 & FP\\
    & $\hat y$ = 0 & FN              & TN\\
    %& & P(y = 1) & P(y = 0)
\end{tabular}
\end{center}
}
\end{column}
\end{columns}


%\begin{center}
% \only<1>{\centering\includegraphics[page=1, width=0.7\textwidth]{figure_man/cost-curves.pdf}}
% \only<2>{\centering\includegraphics[page=2, width=0.7\textwidth]{figure_man/cost-curves.pdf}}
% \only<3>{\centering\includegraphics[page=3, width=0.7\textwidth]{figure_man/cost-curves.pdf}}
  %\tiny{\\ Credit: Nathalie Japkowicz  \\}
%\end{center}

\end{frame}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Cost curves}
%
% \textbf{Example:} In position (0.4, 0.25) B classifier loses its
% performance to one of the A classifiers (point where ROC curves cross). But
% there is no practical information about when classifier A should be used
% over B. In contrast the cost graph tells us that for $0.26 \leq \rp < 0.4$
% classifier B is preferred and for $0.4 \leq \rp < 0.48$ classifier A1 is
% preferred.
%
% \begin{center}
% \includegraphics[width=0.5\textwidth]{figure_man/cost-curves-2.png}
% \end{center}
%
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{ROC curves vs. cost curves}

\begin{itemize}
  \item ROC curves do not indicate in which situations classifier A is superior to another classifier B.
  \item Cost curves can do exactly that and therefore provide practically more relevant information than ROC curves.
  \item For simplification, we focused on cost curves based on the misclassification error by assuming $cost_{FN} = cost_{FP}$.
  %\item
  However, cost curves can also be defined for different misclassification costs.
  %($\rightarrow$ only the axes will change to account for the costs).
  % ($\rightarrow$ simple modification where the identity of the axes is changed).
  %\item Then, the y-axis represents the normalized expected cost (NEC) or relative expected misclassification cost.
  \end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{ROC curves vs. cost curves}
% The general form is as follows:
%
% \vspace{-0.5cm}
% \begin{center}
% \begin{equation*}
% NEC = \text{FNR} \cdot P_C[+] + \text{FPR} \cdot (1 - P_C[+]),
% \end{equation*}
% \end{center}
%
% \begin{itemize}
% \item FNR/FPR are false-negative rate and false-positive rate respectively and
% $P_C[+]$, the probability cost function (modified version of $P[+]$ that takes costs into
% consideration).
%
% \vspace{-0.5cm}
% \begin{center}
% \begin{equation*}
% P_C[+] = \frac{P[+] \cdot C[+|-]}{P[+] \cdot C[+|-] + P[-] \cdot C[-|+]},
% \end{equation*}
% \end{center}
%
% \item $C[+|-]$ and $C[-|+]$ represent the cost of predicting a positive when the
% instance is actually negative and vice versa and $P[-]$ as the probability of
% being in the negative class.
% \end{itemize}
% \end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
