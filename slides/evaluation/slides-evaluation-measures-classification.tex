\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}
\input{../../latex-math/ml-hpo.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
Evaluation 
}{% Lecture title  
Simple Measures for Classification
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/conf_matrix
}{% Learning goals, wrapped inside itemize environment
  \item Know the definitions of misclassification error rate (MCE) and accuracy (ACC)
  \item Understand the entries of a confusion matrix
  \item Understand the idea of costs
  \item Know definitions of Brier score and log loss
}

% ------------------------------------------------------------------------------

\begin{vbframe}{Labels vs Probabilities}

\footnotesize

In classification we predict:

\begin{enumerate}
  \item Class labels: \\
  $$\bm{F} = \left( \hat{o}_k^{(i)} \right)_{i \in \{1, \dots, m\}, 
  k \in \{1, \dots, g\}}
  \quad \in \R^{m \times g},$$
  where $\hat{o}_k^{(i)} = [\yih = k], k = 1, \dots, g$ is the one-hot-encoded 
  class label prediction.
  % \begin{equation*}
  %   \begin{split}
  %     \bm{F} &= \left( \mathbb{I}_{[\hh(\xi) = k]} \right)_{i \in 
  %     \{1, \dots, m\}, k \in \{1, \dots, g\}} = \\
  %     &= \left( \mathbb{I}_{[\yih = k]} \right)_{i, k} \quad \in \R^{m \times g}
  %   \end{split}
  % \end{equation*}
  % $\bm{F} = (\mathbb{I}[\hh(\xi) = k])_{i \in \{1, \dots, m\}, 
  % k \in \{1, \dots, g\}} = (\mathbb{I}[\yih = k])_{i \in \{1, \dots, m\}, 
  % k \in \{1, \dots, g\}} \in \R^{m \times g}$
  \item Class probabilities: \\
  $$\bm{F} = \left(\hat{\pi}_k^{(i)} \right)_{i \in \{1, \dots, m\}, 
  k \in \{1, \dots, g\}}
  \quad \in [0, 1]^{m \times g}$$
  % \begin{equation*}
  %   \begin{split}
  %     \bm{F} &= \left(\pikxih \right)_{i \in \{1, \dots, m\}, 
  %     k \in \{1, \dots, g\}} = \\
  %     &= \left(\hat \pi_k^{(i)} \right)_{i, k} \quad \in \R^{m \times g}
  %   \end{split}
  % \end{equation*}
  % $\rightarrow \bm{F} = (\pikxih)_{i \in \{1, \dots, m\}, 
  % k \in \{1, \dots, g\}} = (\hat \pi_k^{(i)})_{i \in \{1, \dots, m\}, 
  % k \in \{1, \dots, g\}} \in \R^{m \times g}$
\end{enumerate}

\vfill

$\rightarrow$ These form the basis for evaluation.

\vfill

% Source https://docs.google.com/presentation/d/1eVGEZBS-MFIiweDz6FMioLAJJ25UXVOEykyoJIzUgeg/edit#slide=id.p
% \begin{center}
  \includegraphics[width = 0.4\textwidth]{figure_man/classif_thresholding.png}
% \end{center}


\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Labels: MCE \& ACC}

\begin{minipage}[t]{0.75\textwidth} 
  \small
  The \textbf{misclassification error rate (MCE)} counts the number of incorrect 
  predictions and presents them as a rate:
  $$ \rho_{MCE} = \frac{1}{m} \sum_{i = 1}^m [\yi \neq \yih] \in [0, 1]. $$
  % $$ MCE = \meanin [\yi \neq \yih] \in [0, 1]. $$
\end{minipage}%
\begin{minipage}[t]{0.25\textwidth}
  \centering
  % FIGURE SOURCE: 
  %https://docs.google.com/drawings/d/1AJrrXIBJhs0WH078boj3K0DKTJhYUOD_eiPL6J3qcok/edit?usp=sharing
  \begin{center}
    \textbf{MCE}
    \includegraphics[width=0.7\textwidth]
    {figure_man/eval-classif-loss-compare-mce.pdf}
  \end{center}
\end{minipage}

\lz

\begin{minipage}[t]{0.75\textwidth} 
  \small
  \textbf{Accuracy (ACC)} is defined in a similar fashion for correct 
  classifications:
  $$ \rho_{ACC} = \frac{1}{m} \sum_{i = 1}^m [\yi = \yih] \in [0, 1]. $$
  % $$ ACC = \meanin [\yi = \yih] \in [0, 1]. $$
\end{minipage}%
\begin{minipage}[t]{0.25\textwidth}
  \centering
  % FIGURE SOURCE: 
  %https://docs.google.com/drawings/d/1zx_hVua5EJbKUmOvAxDPNb9OEeX7fiF9w0Kykzd46jw/edit?usp=sharing
  \begin{center}
    \textbf{ACC}
    \includegraphics[width=0.7\textwidth]
    {figure_man/eval-classif-loss-compare.pdf}
  \end{center}
\end{minipage}

\lz

\begin{itemize}
  \small
  \item If the data set is small this can be brittle.
  \item MCE says nothing about how good/skewed predicted probabilities are.
  \item Errors on all classes are weighted equally, which is often 
  inappropriate.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Labels: Confusion Matrix}

Much better than reducing prediction errors to a simple number is 
tabulating them in a \textbf{confusion matrix}:
\begin{itemize}
 \item true classes in columns,
 \item predicted classes in rows.
\end{itemize}

We can nicely see class sizes (predicted/true) and where errors occur.


\begin{table}[]
\centering
\begin{tabular}{cc|cccc|c|}
& \multicolumn{6}{c}{True classes} \\ 
\hline
& &\textbf{setosa} & \textbf{versicolor} & \textbf{virginica} & \textbf{error} & \textbf{$n$} \\ 
\hline

\multirow{4}{*}{\rotatebox[origin=c]{90}{\parbox{1.5cm}{Predicted \\ classes}}}   

& \textbf{setosa}     & 50              & 0                   & 0                  & 0              & 50           \\
& \textbf{versicolor} & 0               & 46                  & 4                  & 4              & 50           \\
& \textbf{virginica}  & 0               & 4                   & 46                 & 4              & 50           \\
& \textbf{error}      & 0               & 4                   & 4                  & 8              &      -        \\
\hline
& \textbf{$n$}        & 50              & 50                  & 50                 &      -          & 150          \\ 

%& & & & & &\\

\hline
\end{tabular}
\end{table}
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Labels: Confusion Matrix}

\begin{itemize}
  \item In binary classification, we typically call one class "positive" and the 
  other "negative".
  \item The positive class is the more important, often smaller one.
\end{itemize}

% % FIGURE SOURCE: No source
% \includegraphics[width=0.7\textwidth]{figure_man/roc-confmatrix1.png}

\begin{center}
\small
\begin{tabular}{cc|>{\centering\arraybackslash}p{7em}>{\centering\arraybackslash}p{8em}}
    & & \multicolumn{2}{c}{\bfseries True Class $y$} \\
    & & $+$ & $-$ \\
    \hline
    \bfseries Pred.     & $+$ & True Positive (TP)  & False Positive (FP) \\
              $\yh$ & $-$ & False Negative (FN) & True Negative (TN) \\
\end{tabular}
\end{center}
% \\
e.g.,
\begin{itemize}
  \item \textbf{True Positive} (TP) means that an instance is classified as 
  positive that is really positive (correct prediction).
  % \item \textbf{False Positive}(FP) means that a instance is classified as positive which is actually negative (false prediction). 
  \item \textbf{False Negative} (FN) means that an instance is classified as 
  negative that is actually positive (incorrect prediction). 
  % \item \textbf{True Negative}(TN) means that a instance is classified as negative which is negative (true prediction).
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Labels: Costs}

\small

We can also assign different costs to different errors via a 
\textbf{cost matrix}.

$$
  Costs = \meanin C[\yi, \yih]
$$

\underline{Example}: % \textbf{\textcolor{blue}{@BB Elkan Paper! Confusion matrix discussion}}\\ 
% http://web.cs.iastate.edu/~honavar/elkan.pdf
Depending on certain features (age, income, profession, ...) a bank wants to 
decide whether to grant a 10,000 EUR loan.\\ 

\medskip

Predict if a person is solvent (yes / no).\\ 
Should the bank lend them the money?\\

\medskip
  \begin{tabular}{ll}
    % \begin{small}
    \textbf{Examplary costs:} & \\
    Loss in event of default: & 10,000 EUR\\
    Income through interest paid: & 100 EUR\\
    % \end{small}
  \end{tabular}
 
\begin{table}[ht]
  \small
  \begin{tabular}{cccc}
    \hline
    & &\multicolumn{2}{c}{True classes} \\ 
    & &\textbf{solvent} & \textbf{not solvent}  \\ 
    \hline
    \multirow{2}{*}{\parbox{1cm}{Predicted \\ classes}}& \textbf{solvent}     & 0                 & 10,000\\
    & \textbf{not solvent} & 100               & 0\\
    \hline
\end{tabular}
\end{table}
 
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Labels: Costs}

\begin{columns}
\begin{column}{0.5\textwidth} 
\footnotesize\textbf{Cost matrix}
 \begin{table}[ht]
 \tiny
\begin{center}
\begin{tabular}{cccc}
    \hline
    & &\multicolumn{2}{c}{True classes} \\ 
    & &\textbf{solvent} & \textbf{not solvent}  \\ 
 \hline
    \multirow{2}{*}{\parbox{0.5cm}{Predicted \\ classes}}& \textbf{solvent}     & 0                 & 10,000\\
    & \textbf{not solvent} & 100               & 0\\
    \hline
\end{tabular}
\end{center}
\end{table}
\end{column}

\begin{column}{0.5\textwidth}
\footnotesize\textbf{Confusion matrix}
 \begin{table}[ht]
 \tiny
\begin{center}
\begin{tabular}{cccc}
    \hline
    & &\multicolumn{2}{c}{True classes} \\ 
    & &\textbf{solvent} & \textbf{not solvent}  \\ 
 \hline
    \multirow{2}{*}{\parbox{0.5cm}{Predicted \\ classes}}& \textbf{solvent}     & 70                 & 3\\
    & \textbf{not solvent} & 7              & 20\\
    \hline
\end{tabular}
\end{center}
\end{table}
\end{column}
\end{columns}


  \begin{itemize}
      \item If the bank gives everyone a credit, who was predicted as \textit{solvent}, the costs are at:
      \begin{align*}
      Costs &= \meanin C[\yi, \yih] \\&= 
      \frac{1}{100} \left( 
      100 \cdot 7 + 
      0 \cdot 70 + 
      10.000 \cdot 3 +
      0 \cdot 20 
      \right) = 307
      \end{align*}
    \item If the bank gives everyone a credit, the costs are at:
      \begin{align*}
      Costs &= 
      \frac{1}{100} \left( 
      100 \cdot 0 + 
      0 \cdot 77 + 
      10.000 \cdot 23 +
      0 \cdot 0 
      \right) = 2.300
      \end{align*}
    
  
  \end{itemize}
 
 
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Probabilities: Brier Score}

Measures squared distances of probabilities from the true class labels:
$$\rho_{BS} = \frac{1}{m} \sum_{i = 1}^m 
\left( \hat \pi^{(i)} - \yi \right)^2$$
% \[
% BS_1 = \meanin \left( \pixih - \yi \right)^2
% \]

\begin{itemize}
  \item Fancy name for MSE on probabilities.
  \item Usual definition for binary case; $\yi$ must be encoded as 0 and 1.
\end{itemize}

\lz

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/eval_mclass_1} 

}

\end{knitrout}


\framebreak

$$\rho_{BS, MC} = \frac{1}{m} \sum_{i = 1}^m \sumkg
\left( \hat \pi_k^{(i)} - o_k^{(i)} \right)^2$$
% \[
% BS_2 = \meanin \sumkg \left( \pikxih - o_k^{(i)} \right)^2
% \]
\begin{itemize}
  \item Original by Brier, works also for multiple classes.
  \item $ o_k^{(i)} = [ \yi = k ] $ marks the one-hot-encoded class label.
  \item For the binary case, $\rho_{BS, MC}$ is twice as large 
  as $\rho_{BS}$: in $\rho_{BS, MC}$, we sum 
  the squared difference for each observation regarding both class 0 
  \textbf{and} class 1, not only the true class.
\end{itemize}

\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Probabilities: Log-Loss}
Logistic regression loss function, a.k.a. Bernoulli or binomial loss, $\yi$ 
encoded as 0 and 1.
$$\rho_{LL} = \frac{1}{m} \sum_{i = 1}^m \left( - \yi \log \left( 
\hat \pi^{(i)} \right) - \left( 1-\yi \right) \log \left( 1 - \hat \pi^{(i)} 
\right) \right).$$
% \[
% LL = \meanin \left( - \yi \log(\pixih) - (1-\yi) \log(1-\pixih) \right).
% \]
\begin{center}
\includegraphics[width=0.8\textwidth]{../supervised-classification/figure/log_loss.png}
\end{center}
\begin{itemize}
  \item Optimal value is 0, \enquote{confidently wrong} is penalized heavily.
  \item Multi-class version: $\rho_{LL, MC} = 
  - \frac{1}{m} \sum\limits_{i = 1}^m \sumkg o_k^{(i)} \log \left( 
  \hat \pi_k^{(i)} \right).$
\end{itemize}
\end{vbframe}

% ------------------------------------------------------------------------------

\endlecture
\end{document}
