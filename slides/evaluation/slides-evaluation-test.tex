\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}

\newcommand{\titlefigure}{figure/eval_test_3}
\newcommand{\learninggoals}{
\item Understand the definition of test error
\item Understand that test error is more reliable than train error    
\item Bias-Variance analysis of holdout splitting}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Evaluation: Test Error}
\lecture{Introduction to Machine Learning}
\sloppy

% ------------------------------------------------------------------------------

\begin{vbframe}{Test Error and Hold-Out Splitting}

% \begin{itemize}
% \item The fundamental idea behind test error estimation (and everything that will follow) is quite simple
% \item To measure performance, letâ€™s simulate how our model will be applied on new, unseen data
% \item So, to evaluate a given model do exactly that, predict only on data not used during training and measure performance there
% \item That implies that for a given set D , we have to preserve some data for testing that we cannot use for training
% \end{itemize}
  \small
\begin{itemize}
  \item Simulate prediction on unseen data, to avoid optimistic bias:
\small
$$\rho(\yv_{\mathrm{test}}, F_{\textrm{test}})\;\text{where}\; 
F_{\textrm{test}} = \begin{bmatrix} 
\fhDtrain(\xi[1]_{\textrm{test}}) \\ 
\dots \\
\fhDtrain(\xi[m]_{\textrm{test}})
\end{bmatrix}$$ 
% $$\rho(\yv_{\mathrm{test}}, F_{\textrm{test}}), \;\text{where}\; F_{\textrm{test}} = 
% \begin{bmatrix} \fhDtrain(\xi[1]_{\textrm{test}})^\top & \dots & \fhDtrain(\xi[m]_{\textrm{test}})^\top \end{bmatrix}^\top,$$ 
  % \item Evaluating a given model therefore means predicting only on the
  % test data and measuring the resulting performance.
  \item Partition data, e.g., 2/3 for train and 1/3 for test.
\end{itemize}

\begin{center}
  % FIGURE SOURCE: https://docs.google.com/drawings/d/1q7WN1_YKHedIPNySiZBEraLtTkHRX12Ej6M6ISbfMD0/edit?usp=sharing

  \includegraphics[width=0.6\textwidth]{figure_man/test_error.pdf}

\end{center}
A.k.a. holdout splitting.

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Test Error and Hold-Out Splitting}
% \begin{itemize}
%   \item Split data into 2 parts, e.g., 2/3 for training, 1/3 for testing
%   \item Evaluate on data not used for model building
% \end{itemize}
% 
% % FIGURE SOURCE: https://docs.google.com/drawings/d/1q7WN1_YKHedIPNySiZBEraLtTkHRX12Ej6M6ISbfMD0/edit?usp=sharing
% \includegraphics[width=\textwidth]{figure_man/test_error.pdf}
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Example: Polynomial Regression}

Previous example:
% $0.5 + 0.4 \cdot \sin (2 \pi x) + \epsilon$:

\vfill

\begin{center}
  \includegraphics[width=0.85\textwidth]{figure/eval_train_1}
\end{center}

% Again, we approximate the data with a $d^{th}$-degree polynomial:
\[ \fxt = \theta_0 + \theta_1 \xv + \cdots + \theta_d \xv^d = \sum_{j = 0}^{d}
\theta_j \xv^j\text. \]

\framebreak

Now with fresh test data:

\includegraphics[width=0.8\textwidth]{figure/eval_test_2} 

\begin{itemize}
  \footnotesize
  \item $d = 1$: MSE = 0.038: clearly underfitting
  \item $d = 3$: MSE = 0.002: pretty OK
  \item $d = 9$: MSE = \textcolor{blue}{0.046}: clearly overfitting
\end{itemize}

\vfill

While train error monotonically decreases in $d$, 
test error shows that high-d polynomials overfit.

\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Test Error}
% 
% Let's consider the following example:\\
% Sample data from sinusoidal function
% $0.5 + 0.4 \cdot \sin (2 \pi x) + \epsilon$\\
% \lz
% \begin{knitrout}\scriptsize
% \definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
% 
% {\centering \includegraphics[width=0.8\textwidth]{figure/eval_test_1} 
% 
% }
% 
% 
% 
% \end{knitrout}
% Try to approximate with a $d^{th}$-degree polynomial:
% \[ \fxt = \theta_0 + \theta_1 x + \cdots + \theta_d x^d = \sum_{j = 0}^{d} \theta_j x^j\text{.} \]
% \end{vbframe}
% 
% % ------------------------------------------------------------------------------

% \begin{vbframe}{Test Error}
% \begin{knitrout}\scriptsize
% \definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
% 
% {\centering \includegraphics[width=0.95\textwidth]{figure/eval_test_2}
% 
% }
% 
% 
% 
% \end{knitrout}
% 
% \begin{itemize}
% \item d=1: MSE = 0.038: Clear underfitting
% \item d=3: MSE = 0.002: Pretty OK
% \item d=9: MSE = 0.046: Clear overfitting
% \end{itemize}
% 
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Test Error}

Let's plot train and test MSE for all $d$:

\includegraphics[width=0.9\textwidth]{figure/eval_test_3} 

Increasing model complexity tends to cause

\begin{itemize}
  \item a decrease in training error, and\\
  \item a U-shape in test error\\ 
  (first underfit, then overfit, sweet-spot in the middle).
  \end{itemize}
  
\end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Training vs. test error}
\begin{small}
\begin{itemize}
\item Boston Housing data
    % set where the value of houses in the area around Boston is predicted based on $13$ features describing the region (e.g., crime rate, name of the town, etc. ).
\item Polynomial regression (without interactions)
    % $$ \yv_{medv} = \beta_0 + \sum_{j=1}^d \sumin \beta_{i, j} \left(\xv_i\right)^j$$
% with $n$ features and $d$ degrees polynomials.
% \item We check what happens with train and test error when we change the size of train and test set, and model complexity. 
\end{itemize}

% \textbf{The training error...}

% \begin{itemize}
  % \item is a biased estimator as performance is measured on the same data the 
  % model was trained on.
% \end{itemize}

% \framebreak

\textbf{The training error...}
\begin{itemize}
  \item decreases with smaller training set size as it becomes easier for the 
  model to learn all observed patterns perfectly.
\end{itemize}
\end{small}
\begin{center}
\includegraphics[width=0.6\textwidth]{figure/fig-train-vs-test-error-1}
\end{center}

\framebreak

\textbf{The training error...}
\begin{itemize}  
  \item decreases with increasing model complexity as the model gets better at
  learning more complex structures. 
\end{itemize}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure/fig-train-vs-test-error-4}
\end{center}

\framebreak

\textbf{The test error...}

\begin{itemize}
  \item will typically decrease with larger training set size as the model 
  generalizes better with more data to learn from.
  
\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{figure/fig-train-vs-test-error-2}
\end{center} 

\framebreak
\textbf{The test error...}

\begin{itemize}  
  
  \item will have higher variance with smaller test set size.

\end{itemize}
\begin{center}
\includegraphics[width=0.8\textwidth]{figure/fig-train-vs-test-error-3}
\end{center}  

\framebreak
\textbf{The test error...}

\begin{itemize}    
  
  \item will have higher variance with increasing model complexity.
\end{itemize}

\begin{center}
\includegraphics[width=0.8\textwidth]{figure/fig-train-vs-test-error-5}
\end{center} 

\end{vbframe}


% ------------------------------------------------------------------------------

\begin{vbframe}{Bias and Variance}

\begin{itemize}
  \item Test error is a good estimator of GE,
  given a) we have enough data b) test data is representative i.i.d.
  % indeed \textit{iid} samples from the same underlying distribution.
   \item Estimates for smaller test sets can fluctuate considerably -- this is why we use resampling in such situations.\\
   Repeated $\tfrac{2}{3}$ / $\tfrac{1}{3}$ holdout splits:\\
\texttt{iris} ($n=150$) and \texttt{sonar} ($n=208$).\\
% So we have about 50 (\texttt{iris}) and 70 (\texttt{sonar}) observations in our 
% respective test sets.\\

\vfill

\begin{center}
\includegraphics[width=0.8\textwidth]{figure/test-error-flucuation} 
\end{center}
  % \item \textbf{Sample size} plays a crucial role in deciding on a split 
  % strategy:
  % \begin{itemize}
  %   \item If the size of our initial, complete data set $\D$ is limited,
  %   single train-test splits can be problematic.
  %   \item Small-sample problems come in different shapes in ML -- 
  %   maybe overall set size is sufficient but one of the classes is very small.
  % \end{itemize}
  % \item It is generally advisable to try out different train-test splits and 
  % study the resulting error measurement fluctuation.
\end{itemize}


\end{vbframe}

% ------------------------------------------------------------------------------

% \begin{vbframe}{Training vs. test error}
%   \vspace{-0.25cm}
%   \begin{blocki}{The training error}
%   \vspace{-0.25cm}
%     \item is an over-optimistic (biased) estimator as the performance is measured on the same data the learned model was trained for
%     \item decreases with smaller training set size as it is easier for the model to learn the underlying structure in the training set perfectly
%     \item decreases with increasing model complexity as the model is able to learn more complex structures
%   \end{blocki}
%   \vspace{-0.25cm}
%   \begin{blocki}{The test error}
%   \vspace{-0.25cm}
%   \item will typically decrease when the training set increases as the model generalizes better with more data (more data to learn)
%   \item will have higher variance with decreasing test set size
%   \item will have higher variance with increasing model complexity
%   \end{blocki}
% \end{vbframe}

% ------------------------------------------------------------------------------

% Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

% \begin{vbframe}{Bias-Variance of Hold-Out}
% \begin{itemize}
% \item If the size of our initial, complete data set $\D$ is limited,
%   single train-test splits can be problematic.
% \item The smaller our single test set is, the higher the variance
%   of our estimated performance error (e.g., if we test on one observation, in the extreme case).
%   But note that by just making the test set smaller, we do not introduce any bias,
%   as we simply average losses on i.i.d. observations from $\Pxy$.
% \item The smaller our training set becomes, the more pessimistic bias we introduce into the model.
%   Note that if $|D| = n$, our aim is to estimate the performance of a model fitted
%   on $n$ observations (as this is what we will do in the end). If we fit on less data during
%   evaluation, our model will learn less, and perform worse. Very small training sets will also
%   increase variance a bit.
% \end{itemize}
% \end{vbframe}

% ------------------------------------------------------------------------------

\begin{vbframe}{Bias-Variance of Hold-Out -- Experiment}

    Hold-out sampling produces a trade-off between \textbf{bias} and
  \textbf{variance} that is controlled by split ratio.
  \begin{itemize}
    \item Smaller training set $\rightarrow$ poor fit, pessimistic bias in $\GEh$.
    \item Smaller test set $\rightarrow$ high variance.
  \end{itemize}   

\vfill

Experiment:
\begin{itemize}
  \item \texttt{spirals} data ($sd = 0.1$), with CART tree. 
  \item Goal: estimate real performance of a model with $|\Dtrain| = 500$.
  % \begin{itemize}
    \item Split rates $s \in \{0.05, 0.10, ..., 0.95\}$ 
    with $|\Dtrain| = s \cdot 500$.
    \item Estimate error on $\Dtest$ with $|\Dtest| = (1 - s) \cdot 500$.
    \item 50 repeats for each split rate.
    \item Get "true" performance by often sampling 500 points, 
      fit learner, then eval on $10^5$ fresh points.
    % obviously not feasible in practice.
  % \end{itemize}
\end{itemize}

\framebreak

%test-holdout-example

\includegraphics[width=\textwidth]{figure/test-holdout-example} 

\lz

\begin{itemize}
  \item Clear pessimistic bias for small training sets -- we learn
      a much worse model than with 500 observations. 
  \item But increase in variance when test sets become smaller.
\end{itemize}

\framebreak

\begin{itemize}
  \item Let's now plot the MSE of the holdout estimator.
  \item NB: Not MSE of model, but squared difference between estimated
      holdout values and true performance (horiz. line in 
  prev. plot).
  \item Best estimator is ca. train set ratio of 2/3.
  \item NB: This is a single experiment and not a scientific study, but this 
  rule-of-thumb has also been validated in larger studies.
\end{itemize}

% test-holdout-example
\begin{center}
  \includegraphics[width=0.9\textwidth]{figure/test-holdout-example-2} 
\end{center}

\end{vbframe}


\endlecture
\end{document}
