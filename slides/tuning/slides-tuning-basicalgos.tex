\input{../../style/preamble}
\newcommand{\titlefigure}{figure/cart_tuning_balgos_2}
\newcommand{\learninggoals}{
\item Understand the idea of grid search
\item Understand the idea of random search
\item Be able to discuss advantages and disadvantages of the two methods}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Hyperparameter Tuning - Basic Techniques}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Grid search}

\begin{itemize}
\item Simple technique which is still quite popular, tries all
HP combinations on a multi-dimensional discretized grid
\item For each hyperparameter a finite set of candidates is predefined
\item Then, we simply search all possible combinations in arbitrary order
\end{itemize}

\begin{footnotesize}
\begin{center}
Grid search over 10x10 points
\end{center}
\end{footnotesize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_tuning_balgos_1} 
}
% FIGURE SOURCE: fig-cart_tuning_balgos_1.R


\end{knitrout}


\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement
\item All parameter types possible
\item Parallelizing computation is trivial
\end{blocki}

\begin{blocki}{Disadvantages}
\item  Scales badly: combinatorial explosion
\item  Inefficient: searches large irrelevant areas
\item  Arbitrary: which values / discretization?
\end{blocki}
\end{vbframe}


\begin{vbframe}{Random search}

\begin{itemize}
\item Small variation of grid search
\item Uniformly sample from the region-of-interest
\end{itemize}


\begin{footnotesize}
\begin{center}
Random search over 100 points
\end{center}
\end{footnotesize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_tuning_balgos_2} 
}
% FIGURE SOURCE: fig-cart_tuning_balgos_1.R


\end{knitrout}

\framebreak

\begin{blocki}{Advantages}
\item Like grid search: very easy to implement, all parameter types possible, trivial parallelization
\item Anytime algorithm: can stop the search whenever our budget for computation is exhausted, or continue until we reach our performance goal.
\item No discretization: each individual parameter is tried with a different value every time
\end{blocki}
\vspace{-2ex}
\begin{blocki}{Disadvantages}
\item Inefficient: many evaluations in areas with low likelihood for improvement
\item Scales badly: high-dimensional hyperparameter spaces need \emph{lots} of samples to cover.
\end{blocki}
\end{vbframe}

\begin{vbframe}{Random Search vs. Grid Search}
	We consider a maximization problem $f(x1,x2)=g(x1)+h(x2)\approx g(x1)$, i.e. in order to maximize the target, $x1$ should be the parameter to focus on.
	\vspace{0.5cm}
	\begin{knitrout}\scriptsize
		\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
		
		{\centering \includegraphics[width=\textwidth]{figure/rs_gs_simulation_plot} 
		}
		% FIGURE SOURCE: make_rs_gs_simulation_plot.R
		
	\end{knitrout}
	$\Rightarrow$ In this setting, random search is more superior as we get a better coverage for the parameter $x1$ in comparison with grid search, where we only discover 5 distinct values for $x1$.
\end{vbframe}

\begin{vbframe}{Tuning Example - Grid Search}
	
	Tuning random forest with grid search and 5CV on the \texttt{sonar} data set for AUC:
	
	\begin{footnotesize}
		\begin{center}
			\begin{tabular}{|l|l|l|l}
				Hyperparameter          &  Type     & Min & Max \\
				\hline
				\texttt{num.trees}     & integer  & 3 & 500 \\
				\texttt{mtry}          & integer  & 5 & 50  \\
				\texttt{min.node.size} & integer  & 10 & 100\\
			\end{tabular}
		\end{center}
	\end{footnotesize}
	
	\begin{knitrout}\scriptsize
		\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
		
		{\centering \includegraphics[width=0.95\textwidth]{figure/tuning_gs_example}
		}
		%FIGURE SOURCE: fig-cart_tuning_balgos_2
		
		
	\end{knitrout}
	
\end{vbframe}

\begin{vbframe}{Tuning Example - Random Search}

Tuning random forest with random search and 5CV on the \texttt{sonar} data set for AUC:

\begin{footnotesize}
\begin{center}
\begin{tabular}{|l|l|l|l}
Hyperparameter          &  Type     & Min & Max \\
\hline
\texttt{num.trees}     & integer  & 3 & 500 \\
\texttt{mtry}          & integer  & 5 & 50  \\
\texttt{min.node.size} & integer  & 10 & 100\\
\end{tabular}
\end{center}
\end{footnotesize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/tuning_rs_example}
}
%FIGURE SOURCE: fig-cart_tuning_balgos_2


\end{knitrout}

\end{vbframe}

\endlecture
\end{document}
