\input{../../style/preamble}
\newcommand{\titlefigure}{figure/cart_tuning_balgos_2}
\newcommand{\learninggoals}{
\item Understand the idea of grid search
\item Understand the idea of random search
\item Be able to discuss advantages and disadvantages of the two methods}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Hyperparameter Tuning - Basic Techniques}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{vbframe}{Grid search}

\begin{itemize}
\item Simple technique which is still quite popular, tries all
HP combinations on a multi-dimensional discretized grid
\item For each hyperparameter a finite set of candidates is predefined
\item Then, we simply search all possible combinations in arbitrary order
\end{itemize}

\begin{footnotesize}
\begin{center}
Grid search over 10x10 points
\end{center}
\end{footnotesize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_tuning_balgos_1} 
}
% FIGURE SOURCE: fig-cart_tuning_balgos_1.R


\end{knitrout}


\framebreak

\begin{blocki}{Advantages}
\item Very easy to implement
\item All parameter types possible
\item Parallelizing computation is trivial
\end{blocki}

\begin{blocki}{Disadvantages}
\item  Scales badly: combinatorial explosion
\item  Inefficient: searches large irrelevant areas
\item  Arbitrary: which values / discretization?
\end{blocki}
\end{vbframe}


\begin{vbframe}{Random search}

\begin{itemize}
\item Small variation of grid search
\item Uniformly sample from the region-of-interest
\end{itemize}


\begin{footnotesize}
\begin{center}
Random search over 100 points
\end{center}
\end{footnotesize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_tuning_balgos_2} 
}
% FIGURE SOURCE: fig-cart_tuning_balgos_1.R


\end{knitrout}

\framebreak

\begin{blocki}{Advantages}
\item Like grid search: very easy to implement, all parameter types possible, trivial parallelization
\item Anytime algorithm: can stop the search whenever our budget for computation is exhausted, or continue until we reach our performance goal.
\item No discretization: each individual parameter is tried with a different value every time
\end{blocki}
\vspace{-2ex}
\begin{blocki}{Disadvantages}
\item Inefficient: many evaluations in areas with low likelihood for improvement
\item Scales badly: high-dimensional hyperparameter spaces need \emph{lots} of samples to cover.
\end{blocki}
\end{vbframe}

\begin{vbframe}{Tuning Example}

Tuning random forest with random search and 5CV on the \texttt{sonar} data set for AUC:

\begin{footnotesize}
\begin{center}
\begin{tabular}{|l|l|l|l}
Hyperparameter          &  Type     & Min & Max \\
\hline
\texttt{num.trees}     & integer  & 3 & 500 \\
\texttt{mtry}          & integer  & 5 & 50  \\
\texttt{min.node.size} & integer  & 10 & 100\\
\end{tabular}
\end{center}
\end{footnotesize}

\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_tuning_balgos_3}
}
%FIGURE SOURCE: fig-cart_tuning_balgos_2


\end{knitrout}

\end{vbframe}

\endlecture
\end{document}
