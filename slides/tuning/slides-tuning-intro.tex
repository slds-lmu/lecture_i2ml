\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}
\input{../../latex-math/ml-hpo.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Hyperparameter Tuning
  }{% Lecture title  
  Introduction
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/riskmin_bilevel3.png
}{% Learning goals, wrapped inside itemize environment
  \item Understand the difference between model parameters and hyperparameters
  \item Know different types of hyperparameters
  \item Be able to explain the goal of hyperparameter tuning
}

\sloppy

\begin{vbframe}{Motivating Example} 

\begin{itemize}
\item Given a data set, we want to train a classification tree. 
\item We feel that a maximum tree depth of $4$ has worked out well for us previously, so we decide to set this hyperparameter to $4$. 
\item The learner ("inducer") $\ind$ takes the input data, internally performs \textbf{empirical risk minimization}, and returns a fitted tree model $\fxh = f(\xv, \thetah)$ of at most depth $\lambda = 4$ that minimizes empirical risk.
\end{itemize}

% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/riskmin_bilevel1}
\end{figure}
\end{center}

\framebreak 

\begin{itemize}
\item We are \textbf{actually} interested in the \textbf{generalization performance} $\GEf$ of the estimated model on new, previously unseen data. 
\item We estimate the generalization performance by evaluating the model $\fh = \ind(\Dtrain, \lamv)$ on a test set $\Dtest$: $$
\GEh_{\Dtrain, \Dtest}(\ind,
  \lamv, \ntrain, \rho) =
  \rho \left( \yv_{\Dtest}, \F_{\Dtest, 
  \fh} \right)
$$
\end{itemize}
\vspace*{-0.6cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/riskmin_bilevel2.png}%figure_man/riskmin_bilevel3.png}
\end{figure}
\end{center}

\framebreak 

\begin{itemize}
\item But many ML algorithms are sensitive w.r.t. a good setting of their hyperparameters,
  and generalization performance might be bad if we have chosen a suboptimal configuration.
\item Consider a simulation example of 3 ML algorithms below, where we use the dataset $mlbench.spiral$ and 10,000 testing points. As can be seen, variating hyperparameters can lead to big difference in model's generalization performance.
  
  \begin{center}
\begin{figure}
\includegraphics[width=0.95\textwidth]{figure/tuning_importance.png}
\end{figure}
\end{center}

\framebreak
For our example this could mean:

\begin{itemize}
\item Data too complex to be modeled by a tree of depth $4$ 
\item Data much simpler than we thought, a tree of depth $4$ overfits
\end{itemize}
\item[$\implies$] Algorithmically try out different values for the tree depth. For each maximum depth $\lambda$, we have to train the model \textbf{to completion} and evaluate its performance on the test set. 
\item We choose the tree depth $\lambda$ that is \textbf{optimal} w.r.t. the generalization error of the model. 
\end{itemize}

%\vspace*{-0.6cm}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\begin{center}
\begin{figure}
\includegraphics[width=0.6\textwidth]{figure_man/riskmin_bilevel3.png}
\end{figure}
\end{center}

% $\to$ Finding the hyperparameter $\lambda$ that is optimal w.r.t. the generalization performance constitutes an optimization problem. 

\end{vbframe}

% \begin{vbframe}{The role of hyperparameters}

% \begin{itemize}
% \item Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
% \item But they can in principle influence any structural property of a model or computational part of the training process.
% \item If a model is not flexible enough, it cannot approximate the relationship between the features and the output well and will underfit.
% \item If a model is too flexible so that it simply \enquote{memorizes} the training data, we will face the dreaded problem of overfitting.
% \item[$\implies$] Hence, controlling the model's capacity, i.e., finding suitable hyperparameters,
% can prevent overfitting the model on the training set.
% \end{itemize}

% \end{vbframe}

\begin{vbframe}{Model Parameters vs. Hyperparameters}

It is critical to understand the difference between model parameters and hyperparameters. 

\lz

\textbf{Model parameters} $\thetav$ are optimized during training. %, typically via loss minimization. 
They are an \textbf{output} of the training. 

\lz

Examples:
\begin{itemize}
\item The splits and terminal node constants of a tree learner 
\item Coefficients $\thetav$ of a linear model $\fx = \thx$
\end{itemize}

\framebreak

In contrast, \textbf{hyperparameters} (HPs) $\lambda$ are not optimized during training. They must be specified in advance, are an \textbf{input} of the training. 
Hyperparameters often control the complexity of a model, i.e., how flexible the model is.
They can in principle influence any structural property of a model or computational part of the training process.

\lz

\vspace{-0.2em}

The process of finding the best hyperparameters is called \textbf{tuning}.

\lz

\vspace{-0.2em}

Examples:

\begin{itemize}
\item Maximum depth of a tree 
\item $k$ and which distance measure to use for $k$-NN
\item Number and maximal order of interactions to be included in a linear regression model
\item Number of optimization steps if the empirical risk minimization
is done via gradient descent
\end{itemize}

\end{vbframe}


\begin{vbframe}{Types of hyperparameters}
We summarize all hyperparameters we want to tune in a vector $\lamv \in \Lam$ of (possibly) mixed type. HPs can have different types: 

\begin{itemize}
\item Real-valued parameters, e.g.:
\begin{itemize}
\item Minimal error improvement in a tree to accept a split
\item Bandwidths of the kernel density estimates for Naive Bayes 
\end{itemize}
\item Integer parameters, e.g.:
\begin{itemize}
\item Neighborhood size $k$ for $k$-NN
\item $mtry$ in a random forest
\end{itemize}
\item Categorical parameters, e.g.:
\begin{itemize}
\item Which split criterion for classification trees?
\item Which distance measure for $k$-NN?
\end{itemize}
\end{itemize}

Hyperparameters are often \textbf{hierarchically dependent} on each other, e.g., \emph{if} we use 
a kernel-density estimate for Naive Bayes, what is its width? 
  % with polynomials of the features up to a certain maximal degree $d$, then 
% we must specify whether to also include polynomial interaction terms like e.g. $x_j^{d-d'}x_m^{d'}$ or not 
% and up to which degree $d' \leq d$.
\end{vbframe}


\endlecture

\end{document}
