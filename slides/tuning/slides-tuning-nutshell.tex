\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-hpo.tex}

\title{Introduction to Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Hyperparameter Tuning
  }{% Lecture title  
  In a Nutshell
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/pexels-karolina-grabowska-4472108.jpg
  % stock free image from pexels.com
}{% Learning goals, wrapped inside itemize environment
  \item Understand the main idea behind tuning,
  \item fulfilling the untouched-test set principle via nested resampling,
  \item and pipelines
}

\begin{vbframe}{What is Tuning?}
\begin{itemize}
\item \small Tuning is the process of selecting the best hyperparameters, denoted as $\lamv$, for a machine learning model.
\item \small Hyperparameters are the parameters of the learner (versus model parameters $\thetav$).
\item \small Consider a guitar analogy: Hyperparameters are akin to the tuning pegs. Learning the best parameters \bm{$\thetavh$} -- playing the guitar -- is a separate process that depends on tuning.
\end{itemize}

\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
\includegraphics[width = 0.75\textwidth]{figure_man/riskmin_bilevel3.png}
\end{center}

\end{vbframe}



\begin{vbframe}{Why Tuning Matters}
\begin{itemize}
\item \small Just like a guitar won't perform well when out-of-tune, properly tuning a learner can drastically improve the resulting model performance.
\item \small Tuning helps find a balance between underfitting and overfitting.
\end{itemize}

\begin{center}
\vspace{2em}
\includegraphics[width = 0.9\textwidth]{figure/tuning_importance.png}
\end{center}
\vspace{1em}
\begin{center}
\scriptsize Comparing AUCs of different values for hyperparameters \textit{maxdepth, k, gamma, and C}
\end{center}

\end{vbframe}



\begin{vbframe}{How hard could it be?}
\begin{itemize}
\item \small Very difficult: There are lots of different configurations to choose from, known as the hyperparameter space, denoted by $\Lam$ (analogous to $\Theta$).
\item \small Black box: If one opts for a configuration $\lamv \in \Lam$, how can its performance be measured (and compared)?
\end{itemize}

$\Rightarrow$ Well-thought-out \textbf{Black-Box Optimization Techniques} are needed.

\begin{center}
\vspace{2em}
\includegraphics[width=200pt]{figure/cart_tuning_balgos_1.pdf}
\end{center}

\begin{center}
\scriptsize Exponential growth of $\Lam$: For two discrete hyperparameters with each 10 possible values, $10 \cdot 10 = 100$ configurations can be evaluated
\end{center}

\end{vbframe}



\begin{vbframe}{Na√Øve Approaches}

Goal: Find a best configuration $\lams \in \argmin \limits_{\boldsymbol{\lambda \in \Lam}} \widehat{\mathrm{GE}}(\ind, \rho, \lamv) $

$\Rightarrow$ Tuners $\tau$, e.g., \textbf{Grid Search} and \textbf{Random Search}, output a $\lams$

\vspace{2em}

\begin{minipage}{0.51\textwidth}
\begin{center}
\textbf{Grid Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_1.pdf}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\begin{center}
\textbf{Random Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_2.pdf}
\end{minipage}

\vspace{4em}
\small Sophisticated techniques, based on assumptions about the objective function, search for optimal solutions more efficiently.
\end{vbframe}



\begin{vbframe}{Untouched-Test-Set Principle}
We've found a $\lams \in \Lam$. How well does it perform?
\begin{itemize}
\item \textbf{Careful:} We cannot use the same data for both tuning and performance estimation, as this would lead to (optimistically) biased performance estimates!
\item To obtain an unbiased $\widehat{\mathrm{GE}}$, we need an \textbf{untouched} test set:
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=0.7\textwidth]{../nested-resampling/figure_man/train_valid_test.pdf}
\end{center}

\end{vbframe}



\begin{vbframe}{Nested Resampling}
To decrease variance of the $\widehat{\mathrm{GE}}$, \textbf{Nested Resampling} is used:

\begin{itemize}
\item \small Just as we generalized holdout splitting to resampling, we generalize the three-way split to nested resampling (as we first have to find $\lams$):
%\item \small The key idea is to repeatedly perform a three-way split with an additional layer (as we first have to find a $\lams$):
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width = 0.6\textwidth]{../nested-resampling/figure_man/Nested_Resampling.png}
\end{center}

\end{vbframe}



\begin{vbframe}{Pipelines in Machine Learning}
Pipelines are like the assembly lines in machine learning. They automate the sequence of data processing and model building tasks.
\vspace{1em}

\textbf{Why Pipelines Matter:}
\begin{itemize}
\item \small \textbf{Streamlined Workflow:} Automates the flow from data preprocessing to model training.
\item \small \textbf{Reproducibility:} Ensures that results can be reproduced consistently.
\item \small \textbf{Error Reduction:} Minimizes the chance of human errors in the model building process.
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width = 0.8\textwidth]{figure_man/linear_pipeline-no_circle.png}
\end{center}

\end{vbframe}

\endlecture
\end{document}
