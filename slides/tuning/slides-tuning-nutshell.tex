\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-hpo.tex}
\usepackage{tikz}

\newcommand{\titlefigure}{figure_man/pexels-karolina-grabowska-4472108.jpg}
% stock free image from pexels.com

\newcommand{\learninggoals}{
\item Understand the main idea behind tuning,
\item the untouched-test set principle,
\item nested resampling,
\item and pipelines
}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
% \institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-% lmu.github.io/lecture\_i2ml}}
\date{}
\begin{document}

\lecturechapter{Tuning: In a Nutshell}
\lecture{Introduction to Machine Learning}
\sloppy



\begin{vbframe}{What is Tuning?}
\begin{itemize}
\item \small Tuning is the process of selecting the best hyperparameters, denoted as $\lamv$, for a machine learning model
\item \small Hyperparameters are the parameters of the learner (versus model parameters $\theta$)
\item \small Consider a guitar analogy: Hyperparameters are akin to the tuning pegs. Learning the best parameters \bm{$\thetabh$} - playing the guitar - is a separate process that depends on tuning!
\end{itemize}

\begin{center}
\includegraphics[width = 0.75\textwidth]{figure_man/riskmin_bilevel3.png}
\end{center}

\end{vbframe}



\begin{vbframe}{Why Tuning Matters}
\begin{itemize}
\item \small Just like a guitar won't perform well when out-of-tune, properly tuning a learner can drastically improve the resulting model performance
\item \small Tuning helps find a balance between underfitting and overfitting
\end{itemize}

\begin{center}
\vspace{2em}
\includegraphics[width = 0.9\textwidth]{figure/tuning_importance.png}
\end{center}
\vspace{1em}
\begin{center}
\scriptsize comparing AUCs of different values for hyperparameters \textit{maxdepth, k, gamma}
\end{center}

\end{vbframe}



\begin{vbframe}{How hard could it be?}
\begin{itemize}
\item \small Very difficult: There are lots of different configurations to choose from, known as the hyperparameter space, denoted by $\Lam$ (analogous to $\Theta$)
\item \small Black box: If one opts for a configuration $\lamv \in \Lam$, how can its performance be measured (and compared)?
\end{itemize}

\begin{tikzpicture}
  \draw[->, line width=2pt] (0,0) -- (0.7,0);
  \node[anchor=west] at (0.8,-0.05) {Well-thought-out \textbf{Black-Box Optimization Techniques} are needed!};
\end{tikzpicture}


\begin{center}
\vspace{2em}
\includegraphics[width=200pt]{figure/cart_tuning_balgos_1.pdf}
\end{center}

\end{vbframe}



\begin{vbframe}{Na√Øve Approaches}

Goal: Find a best configuration $\lams \in \argmin \limits_{\boldsymbol{\lambda \in \LamS}} \widehat{\mathrm{GE}}(\ind, \mathcal{J}, \rho, \lamv) $

\textbf{Grid Search} and \textbf{Random Search}:

\vspace{2em}
\begin{minipage}{0.51\textwidth}
\begin{center}
\textbf{Grid Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_1.pdf}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\begin{center}
\textbf{Random Search}
\end{center}

\includegraphics[width=190pt]{figure/cart_tuning_balgos_2.pdf}
\end{minipage}

\vspace{5em}
\small Sophisticated techniques, based on assumptions about the objective function, search for optimal solutions more efficiently.
\end{vbframe}



\begin{vbframe}{untouched-test-set principle}
We've found a $\lamh \approx \lams \in \LamS$. How well does it perform?
\begin{itemize}
\item \textbf{Careful!} We cannot use the same data for both tuning and performance estimation, as this would lead to (optimistically) biased performance estimates!
\item To obtain an unbiased $\widehat{\mathrm{GE}}$, we need an \textbf{untouched} test set:
\end{itemize}
\begin{center}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1kBxfxdUzyUP2-_Y5kWWJ1TnWsm-C4AjHQeTaYPeFo24/edit?usp=sharing
\includegraphics[width=0.7\textwidth]{../nested-resampling/figure_man/train_valid_test.pdf}
\end{center}

\end{vbframe}



\begin{vbframe}{Nested Resampling}
To decrease variance of the $\widehat{\mathrm{GE}}$, \textbf{Nested Resampling} is used:

\begin{itemize}
\item \small It involves two layers of resampling: the outer layer for performance estimation and the inner layer for hyperparameter tuning
\item \small The key idea is to repeatedly perform a three-way split with an additional layer:
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width = 0.6\textwidth]{../nested-resampling/figure_man/Nested_Resampling.png}
\end{center}

\end{vbframe}



\begin{vbframe}{Pipelines in Machine Learning}
Pipelines are like the assembly lines in machine learning. They automate the sequence of data processing and model building tasks.
\vspace{1em}

\textbf{Why Pipelines Matter:}
\begin{itemize}
\item \small \textbf{Streamlined Workflow:} Automates the flow from data preprocessing to model training
\item \small \textbf{Reproducibility:} Ensures that results can be reproduced consistently
\item \small \textbf{Error Reduction:} Minimizes the chance of human errors in the model building process
\end{itemize}

\begin{center}
% FIGURE SOURCE: No source
\includegraphics[width = 0.8\textwidth]{figure_man/linear_pipeline-no_circle.png}
\end{center}

\end{vbframe}



\endlecture
\end{document}