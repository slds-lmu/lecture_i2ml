\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-eval.tex}
\input{../../latex-math/ml-hpo.tex}

\title{Introduction to Machine Learning}

\begin{document}
%! includes: tuning-intro

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Hyperparameter Tuning
  }{% Lecture title  
  Problem Definition
}{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/riskmin_bilevel3.png
}{% Learning goals, wrapped inside itemize environment
  \item Definition of HPO objective and components
  \item Understand its properties
  \item What makes tuning challenging
}

\sloppy


\begin{vbframe}{Hyperparameter Optimization}

\vskip 3em
\textbf{Hyperparameters (HP)} $\lamv$ are parameters that are \emph{inputs} to learner $\ind$ which performs ERM on training data set to find optimal \textbf{model parameters} $\thetav$.
HPs can influence the generalization performance in a non-trivial and subtle way.

\vskip 2em

\textbf{Hyperparameter optimization (HPO) / Tuning} is the process of finding a well-performing hyperparameter configuration (HPC) $\lamv \in \LamS$ for an learner $\Ilam$.
\end{vbframe}

% \begin{vbframe}{Hyperparameter Tuning}
% \begin{itemize}
% \item Optimize hyperparameters for learner w.r.t. prediction error
% Tuner proposes configuration, eval by resampling, tuner receives performance, iterate
% \end{itemize}
% \begin{columns}[c, onlytextwidth]
% \column{0.45\textwidth}
% FIGURE SOURCE: No source
  % \includegraphics[trim={0cm 0cm 0cm 0cm}, clip, width=1.2\textwidth]{figure_man/chain.jpg}
% \column{0.45\textwidth}
  % FIGURE SOURCE: https://drive.google.com/open?id=1wY3aUZxIMZPje3vR0t2yWiDMx_osXRCi
% \includegraphics[trim={1cm 0cm 1cm 0cm}, clip, width=1.2\textwidth]{figure_man/tuning_process.jpg}
% \end{columns}

% \end{vbframe}

% \framebreak
\begin{vbframe}{Objective and Search Space}
% \begin{itemize}
% \item HPO  algorithms  automatically identify  a  well-performing  hyperparameter configuration (HPC) $\lamv \in \LamS$ for an learner $\Ilam$.
Search space $\LamS \subset \Lam$ with all optimized HPs and ranges:
$$\LamS = \LamS_1 \times \LamS_2 \times \dots \times \LamS_l$$
 where $\LamS_i$ is a bounded subset of the domain of the i-th HP $\Lam_i$, and can be either continuous, discrete, or categorical.
% \end{itemize}

% \framebreak
% \begin{footnotesize}

\vspace{1cm}

The general HPO problem is defined as:
\begin{eqnarray*}
    \lams \in \argmin_{\lamv \in \LamS} \clam = \argmin_{\lamv \in \LamS}
    %\GEhlamsubIJrho
    \GEhresa
\end{eqnarray*}
with $\lams$ as theoretical optimum, and $\clam$ is short for estim. gen. error
when $\ind$, resampling splits $\JJ$, performance measure $\rho$ are fixed.
\end{vbframe}

% \begin{vbframe}{Objective and Search Space}
% \begin{itemize}
% \item The data set $\D$
% \item A learner $\Ilam$ (possibly: several competing learners?) to be tuned %(e.g. a decision tree classifier)
% \item The learner's hyperparameters and their respective regions-of-interest $\LamS$ over which we optimize % (e.g. $\texttt{tree depth} \in \{1, 2, ..., 20\}$)
% \item The performance measure $\rho$, as determined by the application.\\ Not necessarily identical to the loss function that defines the risk minimization problem for the learner!\\
% % We could even be interested in multiple measures simultaneously, e.g., accuracy and computation time of our model, TPR and PPV, etc.
% \item A (resampling) procedure for estimating the predictive performance which gives rise to the splits $\JJ$
%    % The expected performance on unseen data can be estimated by holdout (i.e., a single train-test-split) or more advanced techniques like cross-validation.
% % More on this later.
% \end{itemize}
% \end{vbframe}

\begin{vbframe}{Objective and Search Space}
\begin{eqnarray*}
    \lams \in \argmin_{\lamv \in \LamS} \clam = \argmin_{\lamv \in \LamS}
    %\GEhlamsubIJrho
    \GEhresa
\end{eqnarray*}
\begin{figure}[h]
    \centering
    \includegraphics[width = 0.65\textwidth]{figure/hpo_loop_1.eps}
    %{figures/riskmin_bilevel3.png}
\end{figure}

\begin{itemize}
    \item Evals are stored in \textbf{archive} $\archive = ((\lamv^{(1)}, c(\lamv^{(1)})), (\lamv^{(2)}, c(\lamv^{(2)})), \dots)$, with $\archivet[t+1] = \archivet[t] \cup (\lamp, \clamp)$.
% \end{footnotesize}

% \medskip

 \item We can define tuner as function $\tau: (\D, \ind, \LamS, \JJ, \rho)\mapsto \lamh$
     % that proposes its estimate $\lamh$ of the true optimal configuration $\lams$ given $\D, \Ilam$ with corresponding search space $\LamS$ to optimize, and a target measure $\rho$.
\end{itemize}
% \framebreak

% \begin{itemize}
% \item This means we estimate and optimize the generalization error
% \begin{equation*}
% \small
% \begin{split}
% \clam = \GEhresa = \agr\Big(
%  &\rho\Big(\yv_{\Jtesti[1]}, \FJtestftraini[1]\Big), \\ &\large{\vdots} \\
% & \rho\Big(\yv_{\Jtesti[1]}, \FJtestftraini[B]\Big)
%     \Big),
% \end{split}
% \end{equation*}
% of a learner $\Ilam$, w.r.t.\ an HPC $\lamv$
% \item  $\clam$ is a black-box, as it usually has no closed-form representation, and hence no analytic gradients are available.
% \item The evaluation of $\clam$ can take a significant amount of time.
% \item Hence, \textbf{expensive black-box} optimization problem.
% \end{itemize}


\end{vbframe}


% \begin{vbframe}{Tuner}

% \vspace{0.2cm}
%
% We face a \textbf{bi-level} optimization problem: The well-known risk minimization problem to find $\fh$ is \textbf{nested} within the outer hyperparameter optimization (also called second-level problem):
%
% \begin{center}
% \begin{figure}
% % FIGURE SOURCE: https://docs.google.com/presentation/d/14xwcs5zncTjFL4hIHAprjZMmyGIPqk5vs8DS32vEAvQ/edit?usp=sharing
% \includegraphics[width=0.8\textwidth]{figure_man/riskmin_bilevel3.png}
% \end{figure}
% \end{center}
%
% \framebreak
%   \footnotesize
%   \begin{itemize}
%     \item For a learning algorithm $\inducer$ (also inducer) with $d$ hyperparameters, the hyperparameter \textbf{configuration space} is:
%       $$\Lam=\Lam_{1} \times \Lam_{2} \times \ldots \times \Lam_{d},$$
%       where $\Lam_{i}$ is the domain of the $i$-th hyperparameter.
%     \item The domains can be continuous, discrete or categorical.
%     \item For practical reasons, the domain of a continuous or integer-valued hyperparameter is typically bounded.
%     \item A vector in this configuration space is denoted as $\bm{\lambda} \in \Lam$.
%     \item A learning algorithm $\inducer$ takes a (training) dataset $\D \in \allDatasets$ and a hyperparameter configuration $\lamv \in \Lam$ and returns a trained model (through risk minimization)
%
%     \vspace*{-0.2cm}
%   \begin{eqnarray*}
%     \inducer: \preimageInducer &\to& \Hspace \\
%     (\D, \lamv) &\mapsto& \inducer(\D, \lamv) = \hat f_{\D, \lamv}
%   \end{eqnarray*}
%     % \item Additionally, some hyperparameters may only need to be specified if another hyperparameter (or combination of hyperparameters) takes on a certain value.
%   \end{itemize}
%
%   % \lz
%
%   % Note that
%
%   %   In contrast to the first-level (empirical) risk minimization problem, hyperparameter optimization is also referred to as \textbf{second-level} optimization. The first-level problem can be seen as a subroutine called by the second-level problem: Each evaluation of $\lamv$ requires to solve the first-level optimization problem.
%
%
% %   \framebreak
%
% %   \begin{itemize}
% %     \item search for the \textbf{inducer} hyperparameter $\lamv$
% %     \item that minimizes the \textbf{generalization error}
% %       $$
% %         \min_{\lamv} \E_{\D_n \sim \Pxy, (\xv, y) \sim \Pxy} \left(V\left(y, \hat f_{\D, \lamv}(\xv)\right)\right).
% %       $$
% %   \end{itemize}
%
% % We compare: In empirical risk minimization, we
%
% %   \begin{itemize}
% %     \item search for the \textbf{model} parameter $\thetav$
% %     \item that minimizes the \textbf{empirical risk}
% %       $$
% %         \min_{\thetav} \sum_{(\xi, \yi) \in \Dtrain} L\left(\yi, \fxi\right).
% %       $$
% %   \end{itemize}
%
% %   In hyperparameter optimization, we
%
% %   \begin{itemize}
% %     \item search for the \textbf{inducer} hyperparameter $\lamv$
% %     \item that minimizes the \textbf{test error}
% %       $$
% %         \min_{\lamv \in \Lam} \sum_{(\xi, \yi) \in \Dtest} V\left(\inducer(\Dtrain, \lamv)(\xi), \yi\right).
% %       $$
% %   \end{itemize}
%
% %   \framebreak
%
%     % \framebreak
%
%   % The hyperparameter optimization problem is difficult in many ways:
%   \end{vbframe}
%
%
% \begin{vbframe}{Tuning: A bi-level optimization problem}
% We formally state the nested hyperparameter tuning problem as:
%
% $$
% \min_{\lamv \in \LamS} c(\lamv) \text{ where } c(\lamv) := \GEh_{\Dtrain, \Dtest}(\inducer,
%   \lamv, \ntrain, \rho)
% $$
%
% \begin{itemize}
% \item The learner $\inducer(\Dtrain, \lamv)$ takes a training data set as well as hyperparameter settings $\lamv$ (e.g., the maximal depth of a classification tree) as an input.
% \item The search space $\LamS$  is a bounded subset of $\Lam$.
% \item $\inducer(\Dtrain, \lamv)$ performs empirical risk minimization on the training data and returns the optimal model $\fh$ for the given hyperparameters.
% \item Note that for the estimation of the generalization error, more sophisticated resampling strategies like cross-validation can be used.
% \end{itemize}
%
% \framebreak

% The components of a tuning problem are:

% \medskip
% We can thus define the HP tuner $\tau: (\D,\ind,\LamS,\rho)\mapsto \lamh$ that proposes its estimate $\lamh$ of the true optimal configuration $\lams$ given $\D, \Ilam$ with corresponding search space $\LamS$ to optimize, and a target measure $\rho$.

% \framebreak

% \begin{center}
% \begin{figure}
% FIGURE SOURCE: https://docs.google.com/presentation/d/1JUtguuVBgidcqD0IdFFIiKH9zqYzM6YRjCqC53V90dA/edit?usp=sharing
% \includegraphics[width=1.2\textwidth]{figure_man/autotune_in_model_fit.pdf}
% \end{figure}
% \end{center}

% \end{vbframe}




% \framebreak

% Possible scenarios for finding default hyperparameters:

% \begin{itemize}
%   \item If the learner's performance is fairly insensitive to changes of a hyperparameter, we don't really have to worry as long as we remain within the range of reasonable values.
%   \item Constant default: we can benchmark the learner across a broad range of data sets and scenarios and try to find hyperparameter values that work well in many different situations. Quite optimistic?
%   \item Dynamic (heuristic) default: We can benchmark the learner across a broad range of data sets and scenarios and try to find an easily computable function that sets the hyperparameter in a data dependent way,
%   e.g. using \texttt{mtry}$ = p/3$ for RF.\\
%     How to construct or learn that heuristic function, though...?
%   \item In some cases, can try to set hyperparameters optimally by extracting more info from the fitted model. E.g. \texttt{ntrees} for a random forest (does OOB error increase or decrease if you remove trees from the ensemble?).
% \end{itemize}
% \end{vbframe}


\begin{vbframe}{Why is tuning so hard?}
  \begin{itemize}
\item Tuning is usually \textbf{black box}: No derivatives of the objective are availabe.
    We can only eval the performance for a given HPC via a computer program (CV of learner on data).
\item Every evaluation can require multiple train and predict steps, hence it's \textbf{expensive}.
\item Even worse: the answer we get from that evaluation is \textbf{not exact, but stochastic} in most settings, as we use resampling.
\item \textbf{Categorical and dependent hyperparameters} aggravate our difficulties: the space of hyperparameters we optimize over can have non-metric, complicated structure.
\item Many standard optimization algorithms cannot handle these properties.
\end{itemize}

\end{vbframe}

\endlecture

\end{document}
