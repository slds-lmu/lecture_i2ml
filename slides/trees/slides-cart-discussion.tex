\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-ml.tex}

\newcommand{\titlefigure}{figure/cart_dis_1}
\newcommand{\learninggoals}{
\item Understand the advantages and disadvantages of CART
\item Know when and where CART are applied}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}


\begin{document}
\lecturechapter{CART: Advantages \& Disadvantages}
\lecture{Introduction to Machine Learning}

\sloppy

\begin{vbframe}{Advantages}
  \begin{itemize}
    \item Fairly easy to understand, interpret and visualize.
    \item Not much preprocessing required:
    \begin{itemize}
      \item Automatic handling of non-numerical features
      \item Automatic handling of missing values via surrogate splits
      \item No problems with outliers in features
      \item Monotone transformations do not affect the model fit: scaling becomes irrelevant
    \end{itemize}
    \item Interaction effects between features are easily possible
    \item Can model discontinuities and non-linearities
    \item Performs automatic feature selection
    \item Relatively fast, scales well with larger data
    \item Flexibility through the definition of custom split criteria or leaf-node prediction rules: clustering trees, semi-supervised trees, density estimation, etc.
    \end{itemize}
\end{vbframe}

\begin{vbframe}{Disadvantage: Linear Dependencies}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_dis_1} 

}


Linear dependencies must be modeled over several splits. 
Logistic regression would model this easily with fewer parameters.
\end{vbframe}

\begin{vbframe}{Disadvantages: Smooth functions and extrapolation}


{\centering \includegraphics[width=0.85\textwidth]{figure/not-smooth-extrapolation.pdf} 

}

Prediction functions of trees are never smooth as they are always step functions and do not extrapolate well beyond the training observations as the model a piecewise-constant function.

In this case, the tree does not recover the smoothness of $y$ and does not extrapolate well beyond the training domain ($[0, 10]$).
\end{vbframe}

 \begin{vbframe}{Disadvantage: Instability}
 \begin{itemize}
 \item High instability (variance) of the trees:
 \item Small changes in the data may lead to very different splits/trees
 \item This leads to a) less trust in interpretability b) is a reason why the prediction error of trees is usually comparably high
 \end{itemize}

The high instability of trees will be demonstrated using the Wisconsin Breast Cancer data set.
It has 699 observations on 9 features and one target class with values \enquote{benign} and \enquote{malignant}. We fit two trees: One with the full data set and one where we eliminated a single observation. The table shows that this resulted in a label flip for 17 (!) observations of the training data:

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & benign & malignant \\ 
  \hline
benign & 445 &   6 \\ 
  malignant &  11 & 236 \\ 
   \hline
\end{tabular}
\end{table}

The rows indicate the predictions of the learner with the complete data set, and the columns the reduced one.

% \begin{table}
% \begin{tabular}{ll}
% Feature name & Explanation\\
% \hline
% \code{Cl.thickness} & Clump Thickness\\
% \code{Cell.size} & Uniformity of Cell Size\\
% \code{Cell.shape} & Uniformity of Cell Shape\\
% \code{Marg.adhesion} & Marginal Adhesion\\
% \code{Epith.c.size} & Single Epithelial Cell Size\\
% \code{Bare.nuclei} & Bare Nuclei\\
% \code{Bl.cromatin} & Bland Chromatin\\
% \code{Normal.nucleoli} & Normal Nucleoli\\
% \code{Mitoses} & Mitoses\\
% \end{tabular}
% \end{table}

% \framebreak

% Tree fitted on complete Wisconsin Breast Cancer data
% <<results='hide', fig.height=5>>=
% # BB: i am using the BC dataset directly from mlbench
% # and convert features, see issue in i2ml nr 277
% library(mlbench)
% data(BreastCancer)
% d = BreastCancer
% d$Id = NULL
% for (i in 1:9)
%   d[,i] = as.integer(d[,i])
% print(str(d))
% lrn = makeLearner("classif.rpart")
% task1 = makeClassifTask(data = d, target = "Class")
% mod1 = train(lrn, task1)
% d2 = d[-13, ]
% task2 = makeClassifTask(data = d2, target = "Class")
% mod2 = train(lrn, task2)
% fancyRpartPlot(mod1$learner.model, sub = "")
% @
% \framebreak

% Tree fitted on Wisconsin Breast Cancer data without observation 13
% <<results='hide', fig.height=5>>=
% fancyRpartPlot(mod2$learner.model, sub = "")
% @
\end{vbframe}

\begin{vbframe}{Disadvantage: Instability}

However, even more strikingly, the resulting decision trees look very different from one another:

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.99\textwidth]{figure/instability_full.pdf} 
\end{figure}
\end{column}
\begin{column}{0.5\textwidth}
\begin{figure}
\includegraphics[width=0.99\textwidth]{figure/instability_reduced.pdf} 
\end{figure}
\end{column}
\end{columns}

\end{vbframe}

\begin{vbframe}{CART in practice}

\begin{itemize}
\item Compared to other learners CART has suboptimal predictive performance, mainly because of the problems previously shown.
\item Thus, it is rarely used in predictive tasks.
\item However, most disadvantages can be overcome when trained in ensembles: bagging or random forests.
\item Furthermore, trees are attractive tools if an interpretable model is desired or legally required.
\end{itemize}



\end{vbframe}

\begin{vbframe}{Further tree methodologies}

This lecture is mainly focused on Classification and Regression Trees (CART) \citebutton{Breiman, 1984}{https://www.taylorfrancis.com/books/mono/10.1201/9781315139470/classification-regression-trees-leo-breiman} \\
However, there are noteworthy other tree-based approaches as well:

\begin{itemize}
\item Automatic Interaction Detection (AID) \citebutton{Sonquist and Morgan, 1964}{https://scholar.googleusercontent.com/scholar.bib?q=info:VohIjpOwsj4J:scholar.google.com/&output=citation&scisdr=Cm0J5dajEP3hpuOm9tE:AGlGAw8AAAAAZGag7tFRhfgI-PwTnQdXQ3Pv4RA&scisig=AGlGAw8AAAAAZGag7i-yoiF8w79_qwEhGcqETy4&scisf=4&ct=citation&cd=-1&hl=de} and Chi-squared Automatic Interaction Detection (CHAID) \citebutton{Kass, 1980}{https://www.jstor.org/stable/2986296}: Creates all possible cross-tabulations for each categorical predictor until the best outcome is achieved and no further splitting can be performed
\item C4.5 \citebutton{Quinlan, 1993}{https://link.springer.com/article/10.1007/BF00993309}: Not limited to binary splitting
\item Unbiased Recursive Partitioning \citebutton{Hothorn et al., 2006}{https://www.zeileis.org/papers/Hothorn+Hornik+Zeileis-2006.pdf}: Improves the variable selection algorithm
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
