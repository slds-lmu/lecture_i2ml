\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}

\newcommand{\titlefigure}{figure/splitcrit_optimal-constant-sub1.pdf}
\newcommand{\learninggoals}{
\item Understand how to define split criteria via ERM
\item Understand how to find splits in regression with $L_2$ loss}

\title{Introduction to Machine Learning}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}
\lecturechapter{CART: Splitting Criteria for Regression}
\lecture{Introduction to Machine Learning}
\sloppy


\begin{frame}{Splitting criteria}

 \begin{figure}
    \centering
      \includegraphics[height = 5.0cm, keepaspectratio]{figure/tree-regr-depth3.pdf}
    \end{figure}

How to find good splitting rules when growing a tree?
\lz

$\implies$ \textbf{Empirical Risk Minimization}

\end{frame}

\begin{vbframe}{Optimal constants in leaves}
Idea: A split is good if each child's point predictor reflects its data well. We pretend the children are leaves.
\vspace{0.2cm}

For each child $\Np$ we pick the optimal constant for a given loss, e.g., the mean $c_{\Np} = \frac{1}{|\Np|}\sum\limits_{(\xv, y) \in \Np} y$ for the L2 loss $\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} (y - c_{\Np})^2$

\begin{figure}
\includegraphics[width=0.4\textwidth]{figure/splitcrit_optimal-constant.pdf} 
\end{figure}



\end{vbframe}



\begin{vbframe}{Optimal constants in leaves}

Which of these two splits is better?

\begin{columns}
\begin{column}{0.5\textwidth}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/splitcrit_optimal-constant-sub1.pdf} 
\end{figure}

 
\end{column}
\begin{column}{0.5\textwidth}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/splitcrit_optimal-constant-sub2.pdf} 
\end{figure}

\end{column}
\end{columns}

\end{vbframe}

\begin{vbframe}{Risk of a split}

\begin{columns}
\begin{column}{0.5\textwidth}

%\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/splitcrit_optimal-constant-sub1.pdf} 
\end{figure}

$\risk(\Np_1) = 23.4$, $\risk(\Np_2) = 72.4$ 
 
\end{column}
\begin{column}{0.5\textwidth}

\begin{figure}
\includegraphics[width=0.7\textwidth]{figure/splitcrit_optimal-constant-sub2.pdf} 
\end{figure}

$\risk(\Np_1) = 78.1$, $\risk(\Np_2) = 46.1$

\end{column}
\end{columns}
\vspace{0.1in}

The total sum of individual risks, or sum of squared errors (SSE), is:
\vspace{0.1in}

\begin{columns}
\begin{column}{0.5\textwidth}
$23.4 + 72.4 = 95.8$
\end{column}

\begin{column}{0.5\textwidth}
$78.0 + 46.1 = 124.1$ 
\end{column}
\end{columns}

\vspace{0.1in}
Based on the SSE, we prefer the first split.

\end{vbframe}

\begin{vbframe}{Searching the best split}

Let's find the best split for this feature by tabulating results.

\begin{figure}
\includegraphics[width=0.75\textwidth]{figure/splitcrit_optimal-constant-grid.pdf} 
\end{figure}


\end{vbframe}

\begin{vbframe}{Searching the best split}

Let's iterate -- quantile-wise or over all points.
\begin{figure}
\includegraphics[width=0.68\textwidth]{figure/splitcrit_optimal-constant-grid2.pdf} 
\end{figure}

We have reduced the problem to a simple loop.

\end{vbframe}

\begin{vbframe}{Formalization}

\begin{itemize}
\item $\Np \subseteq \D$ is the data contained in this node
\item Let $c_{\Np}$ be the predicted constant for $\Np$ (pretend it is a leaf)
\item The risk $\risk(\Np)$ for a node is:
  $$\risk(\Np) = \sum\limits_{(\xv, y) \in \Np} L(y, c_{\Np})$$
\item The optimal constant is $c_{\Np} = \argminlim_c \sum\limits_{(\xv, y) \in \Np} L(y, c)$ 
\item We often know what that is from theoretical considerations -- or we can perform a simple univariate optimization
\end{itemize}

\framebreak

\begin{itemize}
\item A split w.r.t. \textbf{feature $x_j$ at split point $t$} divides a parent node $\Np$ into 
  \begin{align*}
    \Nl &= \{ (\xv, y) \in \Np: x_j < t \} \text{ and } \Nr = \{ (\xv, y) \in \Np: x_j \geq t \}.
  \end{align*}
\item   
  To evaluate its quality, we compute the risk of our new, finer model
  
     \begin{align*}
      \risk(\Np, j, t) &=  \risk(\Nl) +  \risk(\Nr) \\
                  &= \left(\sum\limits_{(\xv,y) \in \Nl} L(y, c_{\Nl}) + \sum\limits_{(\xv,y) \in \Nr} L(y, c_{\Nr})\right)
      \end{align*}
  \item Finding the best way to split $\Np$ into $\Nl, \Nr$ means solving
  $$\argmin_{j, t} \risk(\Np, j, t)$$
\end{itemize}

\framebreak
\begin{itemize}

\item $\risk(\Np, j, t) &=  \risk(\Nl) +  \risk(\Nr)$, makes sense if $\risk$ is a simple sum
\item If we use averages, we have to reweight the terms to obtain a global average w.r.t. $\Np$ as the children have different sizes
     \begin{align*}
      \bar{\risk}(\Np, j, t) &= \frac{|\Nl|}{|\Np|} \bar{\risk}(\Nl) + \frac{|\Nr|}{|\Np|} \bar{\risk}(\Nr) \\
                  % &= \frac{1}{|\Np|}\left(\sum\limits_{(\xv,y) \in \Nl} L(y, c_{\Nl}) + \sum\limits_{(\xv,y) \in \Nr} L(y, c_{\Nr})\right)
      \end{align*}
\item We mention this for clarity, as quite a few texts contain only the (more complicated) weighted formula without clear explanation

\end{itemize}
\end{vbframe}



\endlecture
\end{document}
