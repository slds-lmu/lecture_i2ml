\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}
\usepackage{subfig}


% Defines macros and environments
\input{../../style/common.tex}
% \input{common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}



\begin{document}
% Set style/preamble.Rnw as parent.


% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...










% Defines macros and environments
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-trees.tex}
%! includes: cart-intro, cart-splitcriteria

\lecturechapter{CART: Growing a Tree}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Tree Growing}

We start with an empty tree, a root node that contains all the data.\\
Trees are then grown by recursively applying \emph{greedy} optimization to each node $\Np$.\\
\lz

Greedy means we do an \textbf{exhaustive search}: All possible splits of $\Np$ on all possible points $t$ for all features $x_j$ are compared in terms of their empirical risk $\risk(\Np, j, t)$. 

The training data is then distributed to child nodes according to the optimal split and the procedure is repeated in the child nodes.

\framebreak

Start with a root node of all data, then search for a feature and split-point that minimizes the empirical risk in the child nodes. 
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_1} 

}




{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_12} 

}



\end{knitrout}
Nodes display their current label distribution here for illustration.


\framebreak

We then proceed recursively for each child node:
Iterate over all features, and for each feature over all possible split points. Select the best split and divide data in parent node into left and right child nodes:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_2} 

}




{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_22} 

}



\end{knitrout}

\framebreak

We then proceed recursively for each child node:
Iterate over all features, and for each feature over all split points. Select the \enquote{best} split and divide data in parent node into left and right child nodes:
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_3}

}




{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_32} 

}



\end{knitrout}

\end{vbframe}

\begin{vbframe}{Split placement}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=0.95\textwidth]{figure/cart_treegrow_4}

}



\end{knitrout}
\lz
Splits are usually placed at the mid-point of the observations they split: the large margin to the next closest observations makes better generalization on new, unseen data more likely.
\end{vbframe}

% \begin{vbframe}{Regression Example}
% \begin{columns}[T,onlytextwidth]
% \column{0.2\textwidth}
% <<out.width='\\textwidth'>>=
% modForrester = makeSingleObjectiveFunction(
%   name = "Modification Forrester et. all function",
%   fn = function(x) (sin(4*x - 4)) * ((2*x - 2)^2) * (sin(20*x - 4)),
%   par.set = makeNumericParamSet(lower = 0, upper = 1, len = 1L),
%   noisy = TRUE
% )
% design = generateDesign(7L, getParamSet(modForrester), fun = lhs::maximinLHS)
% design$y = modForrester(design)
% ordered.design = design[order(design$x),]
% rownames(ordered.design) = NULL
% kable(ordered.design, digits = 3)
% @
% 
% \hspace{0.5cm}
% \column{0.7\textwidth}
% % FIGURE SOURCE: No source
% \includegraphics[height = 0.55\textheight]{figure_man/regression_tree}
% \end{columns}
% \vspace{0.5cm}
% Data points (red) were generated from the underlying function (black):
% 
% $ sin(4x - 4) * (2x - 2)^2 * sin(20x -4) $
% 
% % \framebreak
% 
% % BB: doesnt seem too useful to show this, nothing really new in here
% % <<fig.height=5>>=
% % regr.task = makeRegrTask(data = design, target = "y")
% % regr.rpart = makeLearner("regr.rpart", par.vals = list(minsplit=1, minbucket = 1))
% % regr.model = train(regr.rpart, regr.task)
% % fancyRpartPlot(regr.model$learner.model, sub="")
% % @
% \end{vbframe}






\endlecture
\end{document}
