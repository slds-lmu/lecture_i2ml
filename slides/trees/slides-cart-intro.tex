\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}


\newcommand{\titlefigure}{figure/cart_intro_annotated-tree.pdf}
\newcommand{\learninggoals}{
\item Understand the basic structure of a tree model
\item Understand that the basic idea of a tree model is the same for classification and regression 
\item Know how the label of a new observation is predicted via CART
\item Know hypothesis space of CART}


\title{Introduction to Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\begin{document}

\lecturechapter{Classification and Regression Trees (CART): Basics}
\lecture{Introduction to Machine Learning}
\sloppy

\begin{vbframe}{Binary Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_binary-tree_1.pdf}
    \end{figure}
  \begin{itemize}
    \item Binary trees are a common data structure
    \item They repesent a top-down hierarcy facilitated by binary splits
    \item They can be used in many application, including machine learning
    \item A tree is split into different nodes: a) the root node, b) internal nodes and c) terminal nodes (also leaves).
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Binary Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_binary-tree_2.pdf}
    \end{figure}
  \begin{itemize}
    \item Nodes have relative relationships, they are either:
    \begin{itemize}
    \item Parent nodes
    \item Children nodes
    \end{itemize}
    \item Root nodes do not have parents and terminal nodes do not have children
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Classification Trees}
    \begin{figure}
    \centering
      % FIGURE SOURCE: No source
      \includegraphics[height = 4.5cm, keepaspectratio]{figure/cart_intro_annotated-tree.pdf}
    \end{figure}
  \begin{itemize}
    \item Classification Trees use the data structure of a binary tree
    \item Binary splits are constructed top-down in an \emph{optimal} way
    \item A splitting rule decides what is optimal and coincides with the lowest possible loss
  \end{itemize}
\end{vbframe}



\begin{vbframe}{Classification Tree Model and Prediction}
  \begin{itemize}
    \item For predictions, observations are passed down the tree, according to the splitting rules in each node
    \item An observation will end up in exactly one leaf node
    \item All observations in a leaf node are assigned the same prediction for the target
  \end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.8\textwidth, keepaspectratio]{figure/tree-classif-depth3.pdf}

}

\end{vbframe}

\begin{vbframe}{Regression Tree Model and Prediction}
  \begin{itemize}
    \item For predictions, observations are passed down the tree, according to the splitting rules
      in each node
    \item An observation will end up in exactly one terminal node
    \item All observations in a leaf node are assigned the same prediction for the target
  \end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.8\textwidth, keepaspectratio]{figure/tree-regr-depth3.pdf}

}

\end{vbframe}



\begin{vbframe}{Predicting new data}
\begin{itemize}
\item When predicting new data we use the learnt split points and pass an observation through the tree
\item Each observation is assigned to exactly one leaf
\item Classification trees can make hard-label predictions (here: "setosa") or predict scores / probabilities (here: 0.98 setosa, 0.02 versicolor, 0.00 virginica)
\item Regression trees always predict numeric outcomes
\end{itemize}

\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
{\centering \includegraphics[width=0.73\textwidth, keepaspectratio]{figure/no-points-classif-depth3.pdf}

}
\end{vbframe}


\begin{vbframe}{Tree as an additive model}
Trees divide the feature space $\Xspace$ into \textbf{rectangular regions}: 
  \begin{align*}
    \fx = \sum_{m=1}^M c_m \I(\xv \in Q_m),
  \end{align*}
  where a tree with $M$ leaf nodes defines $M$ \enquote{rectangles} $Q_m$.\\
  $c_m$ is the predicted numerical response, class label or class
  distribution in the respective leaf node.
  \begin{figure}
\includegraphics[width=0.7\textwidth, keepaspectratio]{figure/cart_intro_classification_tree_wide.pdf}
\end{figure}

\end{vbframe}



\begin{vbframe}{Tree as an additive model}

The hypothesis space of a CART is the set of all step functions over rectangular partitions of $\Xspace$:
\begin{align*}
    \fx = \sum_{m=1}^M c_m \I(\xv \in Q_m)
\end{align*}

The steps are learnt sequentially:

\begin{figure} 
\includegraphics[width=0.75\textwidth, keepaspectratio]{figure/cart_intro_regression_tree_wide.pdf}
\end{figure}

\end{vbframe}


\begin{vbframe}{Tree as an additive model}

With two features, prediction areas can be visualized in 3D:

\begin{figure} 
\includegraphics[width=0.75\textwidth, keepaspectratio]{figure_man/tree-contin-surface3d.png}
\end{figure}

\end{vbframe}





% % BB: as we are not really talking too much about impurity anymore, I took this out
% % <<splitcriteria-plot, results='hide', fig.height=5>>=
% % Colors = pal_3
% % par(mar = c(5.1, 4.1, 0.1, 0.1))
% % p = seq(1e-6, 1-1e-6, length.out = 200)
% % entropy = function(p) (p * log(p) + (1 - p) * log(1 - p))/(2 * log(0.5))
% % gini = function(p) 2 * p * (1 - p)
% % missclassification = function(p) (1 - max(p, 1 - p))
% % plot(p, entropy(p), type = "l", col = Colors[1], lwd = 1.5, ylab = "",
% %   ylim = c(0, 0.6), xlab = expression(hat(pi)[Nk]))
% % lines(p, gini(p), col = Colors[2], lwd = 1.5)
% % lines(p, sapply(p, missclassification), col = Colors[3], lwd = 1.5)
% % legend("topright", c("Gini Index", "Entropy", "Misclassification Error"),
% %        col = Colors[1:3], lty = 1)
% % @





\endlecture
\end{document}
