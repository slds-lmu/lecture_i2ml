---
title: "Exercise 10 -- Neural Networks"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

## Exercise 1: Logistic regression or deep learning?

::: {.callout-note title="Learning goals" icon=false}
1) Understand how stacking single neurons can solve a non-linear classification problem
2) Translate between functional & graph description of NNs
:::

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
make_plot = function(dt) {
  ggplot(dt, aes(x, y, col = class, shape = class)) +
  geom_point() +
  geom_text(aes(label = label), nudge_y = 0.25, show.legend = FALSE) +
  labs(x = expression(x[1]), y = expression(x[2])) +
  theme_bw()
}

make_plot_sol = function(dt, nudge_x = 0) {
  ggplot(dt, aes(x, y, col = class, shape = class)) +
  geom_point() +
  geom_text(
    aes(label = label), nudge_y = 0.25, nudge_x = nudge_x, show.legend = FALSE
  ) +
  labs(x = expression(x[1]), y = expression(x[2])) +
  theme_bw()
}
```

Suppose you have a set of five training points from two classes. Consider a logistic regression model $\fx = \sigma\left(\bm{\alpha}^\top \xv \right) = \sigma(\alpha_0 + \alpha_1 x_1 + \alpha_2 x_2)$, with $\sigma(\cdot)$ the logistic/sigmoid function, $\sigma(c) = \frac{1}{1 + \exp(-c)}$.

***
Which values for $\bm{\alpha} = (\alpha_0, \alpha_1, \alpha_2)^\top$ would result in correct classification for the problem in @fig-figure1 (assuming a threshold of 0.5 for the positive class)? 

Don't use any statistical estimation to answer this question -- think in geometrical terms: you need a linear hyperplane that represents a suitable decision boundary.

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="left", fig.height=2, fig.width=3}
#| label: fig-figure1
#| fig-cap: "Classification problem I"

x = c(0, 1, 0, -1, 0)
y = c(0, 0, -1, 0, 1)
data_1 = data.frame(
  x = x,
  y = y,
  class = as.factor(c(rep(0, 3), rep(1, 2))),
  label = sprintf("x%i", 1:5)
)
make_plot(data_1)
```

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

We need to find a linear hyperplane -- i.e.,  a line in 2D -- that separates our classes. There are infinitely many such lines, but the sketch suggests that the line with intercept 0.5 and slope 1 has maximal "safety margin": it has the broadest corridor in which we could move the line without incurring a misclassification error (indicated by dotted lines). Intuitively, this leads to better generalization than any line with a smaller such margin (the mathematical reasoning is treated in support vector machines $\rightsquigarrow$ supervised learning lecture).

Now, we need to translate this into logistic regression coefficients. Recall that the linear hyperplane for binary logistic regression is defined via 

$$
\alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 = - \log(\tfrac{1}{c} - 1) ~
\Longleftrightarrow  ~
\alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 = 0 ~~ \text{ for } c = 0.5,
$$ {#eq-hyperplane}

where $c \in [0, 1]$ is the classification threshold. 

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="left", fig.height=2, fig.width=3}
x = c(0, 1, 0, -1, 0)
y = c(0, 0, -1, 0, 1)
data_1 = data.frame(
  x = x,
  y = y,
  class = as.factor(c(rep(0, 3), rep(1, 2))),
  label = sprintf("x%i", 1:5)
)
make_plot(data_1) +
  geom_abline(intercept = 0.5, slope = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = 3) +
  geom_abline(intercept = 1, slope = 1, linetype = 3)
```

::: {.panel-tabset}
### Quick solution
Logistic regression is not the focus of this exercise, so TL;DR: pick values (not unique) for which points on the line fulfill @eq-hyperplane. For example: $\bm{\alpha}^\prime = (-0.5, -1, 1)^\top$.

### Detailed solution
@eq-hyperplane has 3 unknown quantities -- using a system of linear equations, derived from points we know our line must cross, 3 equations should suffice if there is a unique solution. It turns out that the solution for the linear hyperplane in logistic regression is *not* necessarily unique: multiplying the coefficients in @eq-hyperplane with a constant leaves the equality unchanged.

In ERM, we have additional constraints that usually allow us to find an optimal parameter vector: larger $\bm{\alpha}$ driving the loss down for correctly classified observations is offset by driving it up for misclassified ones.

However, in this special case of linear separability, where a linear classifier produces no misclassification, there is no such optimum (or rather, it is loss-optimal to push the parameter vector to infinity). We will therefore, in this highly unrealistic and simplistic example, use two points the line crosses (which is enough to define any line) and just choose some value for $\bm{\alpha}$ that fulfills @eq-hyperplane.

Let's pick the points $(0, 0.5)^\top$ and $(-0.5, 0)^\top$ (0's are always convenient):

$$
\alpha_0 + \alpha_1 \cdot 0 + \alpha_2 \cdot 0.5 = 0
$$ {#eq-two}

$$
\alpha_0 - \alpha_1 \cdot 0.5 + \alpha_2 \cdot 0 = 0
$$ {#eq-three}

@eq-two - @eq-three yields

$$
\alpha_1 \cdot 0.5 + \alpha_2 \cdot 0.5 = 0 ~ \Leftrightarrow ~ 
\alpha_1 = - \alpha_2,
$$ {#eq-four}

so set, e.g., $\alpha_1^\prime = -1$ and $\alpha_2^\prime = 1$. Plugging this back into either @eq-two or @eq-three yields $\alpha_0^\prime = -0.5$, such that $\bm{\alpha}^\prime = (-0.5, -1, 1)^\top$, and we get the following 
probabilistic scores:

| $i$ | $\yi$ | $\sigma(\alpha_0^\prime + \alpha_1^\prime x_1^{(i)} + \alpha_2^\prime x_2^{(i)})$ |
| --- | ----- | --------------------------------------------------------------------------------- |
| 1   | 0     | 0.38                                                                              |
| 2   | 0     | 0.18                                                                              |
| 3   | 0     | 0.18                                                                              |
| 4   | 1     | 0.62                                                                              |
| 5   | 1     | 0.62                                                                              |

(You can easily verify that a multiple $a \cdot \bm{\alpha}^\prime$ of $\bm{\alpha}^\prime$ leads to the same class assignments, with probabilities closer to 0/1 if $a > 1$.)

:::
</details> 
:::

***
Apply the same principle to find the parameters $\bm{\beta} = (\beta_0, \beta_1, \beta_2)^\top$ for the modified problem in @fig-figure2.

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="left", fig.height=2, fig.width=3}
#| label: fig-figure2
#| fig-cap: "Classification problem II"
data_2 = data_1
data_2$class = as.factor(c(0, rep(1, 2), rep(0, 2)))
make_plot(data_2)
```

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

By the same principle as above, we can derive a possible solution $\bm{\beta}^\prime = (-0.5, 1, -1)^\top$.
  
```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="center", fig.height=5, fig.width=5}
data_2 = data_1
data_2$class = as.factor(c(0, rep(1, 2), rep(0, 2)))
make_plot(data_2) +
  geom_abline(intercept = -0.5, slope = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = 3) +
  geom_abline(intercept = -1, slope = 1, linetype = 3)
```

| $i$ | $\yi$ | $\sigma(\beta_0^\prime + \beta_1^\prime x_1^{(i)} + \beta_2^\prime x_2^{(i)})$ |
| --- | ----- | ------------------------------------------------------------------------------ |
| 1   | 0     | 0.38                                                                           |
| 2   | 1     | 0.62                                                                           |
| 3   | 1     | 0.62                                                                           |
| 4   | 0     | 0.18                                                                           |
| 5   | 0     | 0.18                                                                           |

</details> 
:::

***
Now consider the problem in @fig-figure3, which is not linearly separable anymore, so logistic regression will not help us any longer. Suppose we had alternative coordinates $\left(z_1, z_2\right)^\top$ for our data points:

| $i$ | $z_1^{(i)}$ | $z_2^{(i)}$ | $\yi$ |
| --- | ----------- | ----------- | ----- |
| 1   | 0           | 0           | 1     |
| 2   | 0           | 1           | 0     |
| 3   | 0           | 1           | 0     |
| 4   | 1           | 0           | 0     |
| 5   | 1           | 0           | 0     |

```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="left", fig.height=2, fig.width=3}
#| label: fig-figure3
#| fig-cap: "Classification problem III"
data_3 = data_1
data_3$class = as.factor(c(1, rep(0, 4)))
make_plot(data_3)
```

Explain how we can use $z_1$ and $z_2$ to classify the dataset in @fig-figure3. The question is now, of course, how we can get these $z_1$ and $z_2$ that provide a solution to our previously unsolved problem -- naturally, from the data.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

Intuitively, we can see that a condition like $z_1 + z_2 > 0$ suffices to separate $\xi[1]$ from the other observations, which should remind you of the equation for a linear hyperplane.

Graphically, we can visualize our points in the new $z_1, z_2$ coordinates and see that this transformed dataset is indeed linearly separable:
  
```{r, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="center", fig.height=2, fig.width=3}
data_3 = data.frame(
  x = c(0, 0, 1),
  y = c(0, 1, 0),
  class = as.factor(c(1, 0, 0)),
  label = c("x1", "x2 = x3", "x4 = x5")
)
make_plot(data_3) +
  xlim(c(-0.5, 1.5)) +
  ylim(c(-0.5, 1.5)) +
  geom_abline(intercept = 0.5, slope = -1) +
  labs(x = expression(z[1]), y = expression(z[2]))
```

</details> 
:::

***
The question is now, of course, how we can get these z1 and z2 that provide a solution to our previously unsolved problem – naturally, from the data.

Perform logistic regression to predict $z_1$ and $z_2$ (separately), treating them as target labels to the original observations with coordinates $\left(x_1, x_2\right)^\top$. Find the respective parameter vectors $\bm{\gamma}, \bm{\phi} \in \R^3$.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

Looking at $z_1$, we find that it serves to separate observations $\{1, 2, 3\}$ from $\{4, 5\}$. Likewise, $z_2$ separates $\{1, 4, 5\}$ from $\{2, 3\}$. Predicting $z_1$ thus corresponds to classification problem I, and $z_2$ to classification problem II, so we can use our previous solutions $\bm{\gamma} = \bm{\alpha}^\prime = (-0.5, -1, 1)^\top$ and $\bm{\phi} = \bm{\beta}^\prime = (-0.5, 1, -1)^\top$.
  
</details> 
:::

***
Lastly, put together your previous results to formulate a model that predicts the original target $y$ from the original features $\left(x_1, x_2\right)^\top$.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

We can simply stack the logistic regression components as follows:

\begin{align*}
  \yh &= \sigma\left( \theta_0 + \theta_1 z_1 + \theta_2 z_2\right) \\
  &= \sigma\left( \theta_0 + \theta_1 \cdot \sigma\left(\alpha_0 + \alpha_1 
  x_1 + \alpha_2 x_2\right) + \theta_2 \cdot \sigma\left(\beta_0 + \beta_1 x_1 
  + \beta_2 x_2\right)
  \right)
\end{align*}

Choosing parameters as before yields, e.g., $\thetav^\prime = (0.5, -1, -1)^\top$. In practice, we will of course use ERM to estimate, rather than hand-pick, $\bm{\alpha}, \bm{\beta}, \bm{\theta}$.
</details> 
:::

***
Sketch the neural network you just created (perhaps without realizing it).

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

We created a feedforward neural network with two input neurons, two hidden neurons (each performing one intermediate logistic regression), and one output neuron. The biases are depicted as 1-neurons:

![Feedforward neural network](figure/nn.png){width=40%}
</details> 
:::

***
Explain briefly how the chain rule is applied to the computational graph such a neural network represents. Can you think of a use we can put the resulting gradients to?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

The chain rule enables us to compute (first) derivatives of the empirical risk w.r.t. arbitrary parts of the computational graph by chaining known derivatives of elementary functions, thus providing a simple and scalable solution to the problem of differentiating complicated functions.

For example, we can easily derive $\frac{\partial}{\partial \alpha_1} \riske$, which should remind you of the update term in gradient descent. Indeed, (variants of) gradient descent is exactly what we use to train neural networks, updating all parameters according to their contribution to $\riske$ during the so-called \emph{backpropagation}.
</details> 
:::
