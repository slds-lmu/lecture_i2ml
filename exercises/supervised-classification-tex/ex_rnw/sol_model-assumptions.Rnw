
\begin{enumerate}
\item \textbf{Assumptions of Each Method:}
    \begin{itemize}
        \item \textbf{Quadratic Discriminant Analysis (QDA):}
        \begin{itemize}
            \item Assumes that each class has its own class-specific mean vector \( \boldsymbol{\mu}_k \) and covariance matrix \( \Sigma_k \).
            \item Does not assume feature independence.
            \item Decision boundaries are quadratic as covariance matrices can differ between classes.
        \end{itemize}
        
        \item \textbf{Linear Discriminant Analysis (LDA):}
        \begin{itemize}
            \item A special case of QDA where the covariance matrices are the same for all classes:
            \[
            \Sigma_k = \Sigma \quad \forall k
            \]
            \item Assumes shared covariance structure but different mean vectors for each class.
            \item Results in linear decision boundaries due to the shared covariance matrix.
        \end{itemize}
        
        \item \textbf{Naive Bayes (NB):}
        \begin{itemize}
            \item Another special case of QDA, assuming conditional independence of features given the class label:
            \[
            p(\mathbf{x} | y = k) = \prod_{j=1}^p p(x_j | y = k)
            \]
            \item This results in diagonal covariance matrices where only the variances of the features are considered, and all covariances are zero.
            \item Simplifies computation and parameter estimation.
        \end{itemize}
    \end{itemize}
  \item \textbf{Assignments to Regions:} A is QDA (as it makes the most general assumptions with class-specific covariance matrices), B and C are either LDA or NB.
\begin{enumerate}
\item[1)] LDA can be seen as a special case of QDA if the covariance matrix is equal for all classes: $\Sigma_k = \Sigma \; \; \forall k$
\item[2)] Gaussian NB can be seen as a special case of QDA if the features are conditionally independent given class $k$:
\begin{align}
\pdfxyk = p((x_1, x_2, ..., x_p)|y = k)=\prodjp p(x_j|y = k),
\end{align}
which results in diagonal covariance matrices.
\item[3)] Gaussian NB and LDA have an intersection if the covariance matrix is equal for all classes: $\Sigma_k = \Sigma \; \; \forall k$ \textbf{and} features are conditionally independent given class $k$, leaving each class with the same diagonal covariance matrix $\Sigma$.
\end{enumerate}

\item \textbf{Specific Assumption:}
The Venn diagram is valid under the assumption that the class-conditional distributions are Gaussian. This allows LDA, QDA, and NB to be represented as overlapping regions. In cases without Gaussian assumptions, the relationships may change, and NB may exceed the region of QDA (ellipse of NB ends outside of that of QDA to account for different distributions).

\end{enumerate}