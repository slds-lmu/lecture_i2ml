\begin{enumerate}
\item[1)] In the case of the linear model (LM), empirical risk minimization (ERM) does not necessarily result in a trained model that always satisfies $\thetabh^T \xv \in \unitint$, thus leading to predictions that cannot be sensibly interpreted as probabilities. Therefore, the hypothesis space must be restricted to a function that ensures above condition, which holds for the logistic function $s$:
\begin{align}
\Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pix = s(\thx) \right\}
\end{align}
\item[2)] If one plugs in the Bernoulli loss function $\Lpixy$ into the empirical risk function $\riskef$, lets probabilities $\pix$ be modeled by the logistic function $\pixt = s(\thx)$, and specifies the risk surface to be minimized with regards to the parameter vector $\thetab \in \Theta$, the following explicit ERM problem emerges:
\begin{align}
\thetabh = \argmin_{\thetab \in \Theta} \sumin -\;\yi\ln\left(s\left( \thetab^T\xi \right)\right)-\Bigl(1-\yi\Bigr)\ln\left(1-s\left( \thetab^T\xi \right)\right)
\end{align}

\item[4)] Deriving the log-likelihood function $\logl$ of a single Bernoulli distributed random variable $Y$, one gets

\begin{align}
\LL & = \P(Y=y) = \pi^y (1-\pi)^{1-y} \\
\logl & = ln(\LL) \\
& = y\ln{(\pi)} + (1-y)\ln{(1-\pi)},
\end{align}
which is equivalent to the Bernoulli loss function if one multiplies by $(-1)$. This demonstrates the correspondence of \textit{maximum} likelihood estimation and empirical risk \textit{minimization} in the context of a logistic regression model. Both approaches lead to identical parameter estimates.
\end{enumerate}


  
