% !Rnw weave = knitr

<<setup-child, include = FALSE>>=
knitr::set_parent("../../style/preamble_ueb.Rnw")
options(warn = -1)
@


\kopfic{3}{Supervised Classification}

\loesung{}{
<<child="ex_rnw/sol_research-group-logreg.Rnw">>=
@
}

\newpage

\loesung{}{
% Maybe add some stuff from below to the inclass exercise? Otherwise just keep this to provide further details during the session.
Consider binary classification with labels $Y=\{0,1\}$ and feature vectors $x\in\mathbb R^p$.
We use the padded representation $x\leftarrow (1,x_1,\dots,x_p)^\top$ so that $\theta\in\mathbb R^{p+1}$ and $f(x\mid\theta)=\theta^\top x$.
The probabilistic classifier is
\[
\pi(x\mid\theta) = s\bigl(\theta^\top x\bigr),\qquad s(z)=\frac{\exp(z)}{1+\exp(z)}=\frac{1}{1+\exp(-z)}.
\]
Given iid data $D=((x^{(1)},y^{(1)}),\ldots,(x^{(n)},y^{(n)}))\in (\mathcal X\times\mathcal Y)^n$ with $y^{(i)}\in\{0,1\}$:
\begin{enumerate}
\item Explain the role of the logistic function $s(\cdot)$ in mapping linear scores to valid probabilities.

\begin{itemize}
\item The score $f(x\mid\theta)=\theta^\top x\in\mathbb R$ is unrestricted; applying $s$ yields $\pi(x\mid\theta)\in(0,1)$, interpretable as $P(y=1\mid x)$.
\item The inverse link is the logit: $s^{-1}(\pi)=\log\!\left(\tfrac{\pi}{1-\pi}\right)=\theta^\top x$, so features act additively and linearly on log-odds.
\item Intercept $\theta_0$ shifts the curve horizontally; scaling the score rescales slope.
\end{itemize}

\item Derive the likelihood $L(\theta)$ and log-likelihood $\ell(\theta)$ under the Bernoulli model.

For a single observation, under the Bernoulli model with success prob. $\pi(x\mid\theta)$,
\[
P\bigl(y\mid x,\theta\bigr)=\pi(x\mid\theta)^{\,y}\,\bigl(1-\pi(x\mid\theta)\bigr)^{\,1-y}.
\]
Assuming iid observations, the (joint) likelihood and log-likelihood are
\[
L(\theta)=\prod_{i=1}^n \pi(x^{(i)}\mid\theta)^{\,y^{(i)}}\,\bigl(1-\pi(x^{(i)}\mid\theta)\bigr)^{\,1-y^{(i)}},\qquad
\ell(\theta)=\sum_{i=1}^n \Bigl[ y^{(i)}\log\pi(x^{(i)}\mid\theta)+(1-y^{(i)})\log\bigl(1-\pi(x^{(i)}\mid\theta)\bigr)\Bigr].
\]

\item Show that maximizing $\ell(\theta)$ equals minimizing the empirical risk with Bernoulli (log) loss
\[
R_{\text{emp}}(\theta)\;=\;\sum_{i=1}^n\Bigl[-y^{(i)}\log\pi(x^{(i)}\mid\theta)\;-\;(1-y^{(i)})\log\bigl(1-\pi(x^{(i)}\mid\theta)\bigr)\Bigr].
\]
(Note: dividing by $n$ does not change the minimizer.)

Maximizing $\ell(\theta)$ is equivalent to minimizing the negative log-likelihood,
\[
-\ell(\theta)=\sum_{i=1}^n\Bigl[-y^{(i)}\log\pi(x^{(i)}\mid\theta)\;-\;(1-y^{(i)})\log\bigl(1-\pi(x^{(i)}\mid\theta)\bigr)\Bigr] \,=\, R_{\text{emp}}(\theta).
\]
Equivalently, in terms of the linear score $f(x\mid\theta)=\theta^\top x$ (and using $\pi=s\circ f$), the pointwise Bernoulli loss can be written as
\[
L\bigl(y, f(x)\bigr)\;=\;-y\,f(x)+\log\!\bigl(1+\exp(f(x))\bigr),
\]
which yields the same empirical risk $R_{\text{emp}}(\theta)=\sum_{i=1}^n L\bigl(y^{(i)},\theta^\top x^{(i)}\bigr)$ and is convex in $\theta$.
\end{enumerate}
}