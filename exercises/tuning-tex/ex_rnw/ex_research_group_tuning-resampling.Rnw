Having mastered the intricacies of random forests, our medical research team has set eyes on yet another (and indeed very central) topic in machine learning: \textbf{tuning}.
Researcher Laetitia, who has vigorously studied the I2ML lecture materials, is asked to explain tuning in simple terms to her colleagues:
\textit{"Hyperparameter tuning, often abbreviated as tuning, can be broken down to one simple formula:}
\begin{align}
\min_{\lamv \in \LamS} \GEhresa.
\end{align}
Researchers Lisa and Holger seem to have not studied the materials as engaged as Laetitia, since they cannot make sense of the expression above.
\begin{enumerate}\bfseries
  \item[1)] Explain hyperparameter tuning in your own words, using the formula above.
  What do data scientists mean when they call it a bi-level optimization problem?
\end{enumerate}

A tuning problem consists of several elements: 
\begin{itemize}
  \item A data set $\D$,
  \item A learner $\ind$,
  \item $d$ hyperparameters of the learner and their configuration space $\LamS = \LamS_1 \times \LamS_{2} \times \ldots \times \LamS_d$,
  \item A performance measure $\rho$ to estimate the generalization error, as determined by the application
  \item A (resampling) procedure $\JJ$ for estimating the predictive performance
\end{itemize}

Lisa finds herself wondering how this would translate to their specific research problem.
As before, they try to predict whether a patient admitted to the hospital will require intensive care, a binary classification task with target space $\Yspace = \setzo$.
The feature space is the same as before:
$\Xspace = (\R_{0}^{+})^3$, with $\xi = (x_{age},\; x_{blood\;pressure},\; x_{weight})^{(i)} \in \Xspace$ for $i = 1, 2, \dots, n$ observations.

\begin{enumerate}\bfseries
  \item[2)] Given a data set $\D$, a random forest learner $\ind$, and our research group's problem, write down an example for a specific tuning problem using the list above.
\end{enumerate}

The research group wants to apply the tuning procedure and get as best an estimate of the generalization error as they can.
Holger tunes a learner with a simple 5-fold cross-validation (CV), training 5 models for each of the 100 hyperparameter configurations $\bm{\lambda}_i, \; i = 1, \dots, 100$ generated by random search.
The unseen CV splits are used to estimate the performance, and the best-performing hyperparameter configuration $\bm{\lambda}^{*}$, which yielded an average accuracy of $\rho_{ACC} = 0.92$, is chosen to train the final model on the entire data set.

\begin{enumerate}\bfseries
  \item[3)] Does the final model result in an expected accuracy $\rho_{ACC}$ wrt new, unseen observations equal to, lower or higher than $0.92$?
\end{enumerate}