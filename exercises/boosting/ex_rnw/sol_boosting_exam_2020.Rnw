In the following, you assume that your outcome follows a $\log_2$-normal distribution with density function
$$p(y|f) = \frac{1}{y \sigma \sqrt{2\pi}}\exp\left( - \frac{(\log_2(y)-f)^2}{2\sigma^2} \right).$$ In other words, $\log_2(Y)$ follows a normal distribution. You observe $n = 3$ data points $\bm{y}$ and want to model $f$ using features $\bm{X} \in \mathbb{R}^{n \times p}$. You choose to use a gradient boosting tree algorithm. 
\begin{enumerate}
\item 
\begin{itemize}
\item $\ell(f) = const - (\log_2(y) - f)^2 / 2\sigma^2$ (1P)
\item $\partial \ell / \partial f = \sigma^{-2} (\log_2(y) - f) \propto (\log_2(y) - f)$ (1P)
\end{itemize}
\item Use $\tilde{y} = \log_2(y) = (0,1,2)$ (1P)
\begin{itemize}
\item[(i)] $\hat{f}^{[0]}(\bm{x}) = \bar{\tilde{y}} = 1$ as this is the optimal constant model for squared error. (1P)
\item[(ii)] $\bm{r}^{[1]} = \sigma^{-2} (-1,0,1)$ (1P)
\item[(iii)] $R_t^{[1]}, t=1,2$ will split using $\bm{x}_1$, as $\bm{x}_2$ carries no information. Since $x^{(1)}_{1}=x^{(2)}_{1}$, 
$$R_1 = -0.5 I(x_1 \geq 0.5)$$ and $$R_2 = 1 I(x_1 \leq 0.5).$$ (2P) 
\item[(iv)] $\hat{f}^{[1]}(\bm{x}) = (0.5,0.5,2)$ (1P)
\item[(v)] $\bm{r}^{[2]} = \sigma^{-2} (-0.5,0.5,0)$ (1P)
\end{itemize}
\item Nothing, because there is no information that can be used to further improve the model. (1P)
\item 
\begin{itemize}
\item[(i)] $M$ grows: The capacity will increase and the algorithm may eventually overfit (1P)
\item[(ii)] $n$ grows: The capacity will stay the same and the algorithm may underfit (1P)
\end{itemize}
\end{enumerate}


