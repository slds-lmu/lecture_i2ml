In the following, you assume that your outcome follows a $\log_2$-normal distribution with density function
$$p(y|f) = \frac{1}{y \sigma \sqrt{2\pi}}\exp\left( - \frac{(\log_2(y)-f)^2}{2\sigma^2} \right).$$ In other words, $\log_2(Y)$ follows a normal distribution. You observe $n = 3$ data points $\bm{y}$ and want to model $f$ using features $\bm{X} \in \mathbb{R}^{n \times p}$. You choose to use a gradient boosting tree algorithm. 
\begin{enumerate}
\item Derive the pseudo-residuals based on the negative log-likelihood for the given distribution assumption up to all constants of proportionalities.
\item Given only the 3 samples $\bm{y} = (1,2,4)^\top$ and two features
$$\bm{X} = (\bm{x}_1, \bm{x}_2) = \begin{pmatrix}
1 & 0 \\
1 & 0 \\
0 & 0 \\
\end{pmatrix}
$$ explicitly derive or state with explanation
\begin{itemize}
\item[(i)] the loss-optimal initial boosting model $\hat{f}^{[0]}(\bm{x})$,
\item[(ii)] the pseudo-residuals $\bm{r}^{[1]}$ (use $L2$ loss if you haven't been able to solve (a)),
\item[(iii)] the regression stump $R_t^{[1]}, t=1,2$,
\item[(iv)] the boosting model $\hat{f}^{[1]}(\bm{x})$ as well as
\item[(v)] the pseudo-residuals $\bm{r}^{[2]}$
\end{itemize}
for tree base learners with depth $d=1$ (stumps) and a step length $\nu = 1$. You are allowed to use results from the lecture. If you have not managed to derive the pseudo-residuals for the $\log_2$-normal distribution, use an $L2$ loss.
\item What would happen in the second iteration of the previous boosting algorithm?
\item If you are given more data points, but still the two binary feature vectors $\bm{x}_1$ and $\bm{x}_2$, what will happen as 
\begin{itemize}
\item[(i)] $M$ grows
\item[(ii)] $n$ grows
\end{itemize}
in terms of model capacity (if $d$ is kept fixed)?
\end{enumerate}


