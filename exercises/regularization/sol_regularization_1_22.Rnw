% 
%
%
\begin{itemize}
%
\item  We can show
% 
$$\min_{\thetab} \riskrt  = 	\min_{\thetab} \sum_{i=1}^p	- \thetah_{i}\theta_i + \frac{\theta_i^2}{2} + \lambda \mathds{1}_{|\theta_i| \neq 0}$$
%
as follows:
%
\begin{align*}
%	
	\min_{\thetab} \riskrt  
	%
	&= \min_{\thetab} \frac12 \sum_{i=1}^n \left( \yi - \thetab^\top \xi \right)^2 + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \\
%	
	&= \min_{\thetab} \frac12 \| \yv - \Xmat \thetab \|_2^2 + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \\
%	
	&= \min_{\thetab} \frac12  (\yv - \Xmat \thetab)^\top(\yv - \Xmat \thetab) + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \\
%	
	&= \min_{\thetab} \frac12  \yv^\top \yv -\yv^\top \Xmat \thetab +  \frac12 \thetab^\top \Xmat^\top \Xmat \thetab  + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \\
%	
	&= \min_{\thetab}  -\yv^\top \Xmat \thetab +  \frac12 \thetab^\top \Xmat^\top \Xmat \thetab  + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \tag{ $ \yv^\top \yv$ does not depend on $\thetab$  }\\
%	
	&= \min_{\thetab}  -\yv^\top \Xmat \thetab +  \frac12 \thetab^\top\thetab  + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \tag{By assumption  $  \Xmat^\top \Xmat = \id$  }\\
	%	
	&= \min_{\thetab}  -\thetabh^\top \thetab +  \frac12 \thetab^\top\thetab  + \lambda \sum_{i=1}^p \mathds{1}_{|\theta_i| \neq 0} \tag{By assumption  $  \Xmat^\top \Xmat = \id$ so that $ \thetabh = ({\Xmat}^T \Xmat)^{-1} \Xmat^T\yv = \Xmat^T\yv$ }\\	
	%	
	&= \min_{\thetab} \sum_{i=1}^p	- \thetah_{i}\theta_i + \frac{\theta_i^2}{2} + \lambda \mathds{1}_{|\theta_i| \neq 0} \tag{Writing out the inner products}\\
%	
\end{align*}
%
\item Note that the minimization problem on the right-hand side of the previous math display can be written as $\sum_{i=1}^p g_i(\theta_i),$ where 
$$ g_i(\theta) =   - \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda \mathds{1}_{|\theta| \neq 0}. $$ 
%
The advantage of this representation, if we are interested in finding the $\thetab$ with entries $\theta_1,\ldots,\theta_p$ minimizing $\riskrt,$ is that we can minimize each $g_i$ separately to obtain the optimal entries.
%
\item Consider first the case that $|\thetah_i|>\sqrt{\lambda}.$ 
%
Note that 
%
\begin{align*}
%	
	g_i(\thetah_i) 
%	
	&= - \thetah_i^2 +  \frac{\thetah_i^2 }{2} + \lambda \tag{$|\thetah_i|>\sqrt{\lambda}>0$} \\
%	
	&= - \frac{\thetah_i^2 }{2} + \lambda \\
%	
	&< -\frac{\lambda}{2}. \tag{$|\thetah_i|>\sqrt{\lambda}$ $\Rightarrow$ $- \thetah_i^2 < -\lambda$}
%	
\end{align*}
%
Further, note that $	g_i(0) = 0$ and consequently $g_i(\thetah_i)<g_i(0).$
%
This means that $0$ cannot be the minimizer of $g_i$ in this case.
%
Next, it holds that for any $\theta \neq 0$ with $sgn(\theta)=sgn(\thetah_i),$ i.e., $\thetah_{i}\theta>0$, that
%
$$  g_i(\theta) =   \underbrace{-\thetah_{i}\theta}_{<0} + \underbrace{\frac{\theta^2}{2}}_{=\frac{(-\theta)^2}{2} } + \lambda \mathds{1}_{|\theta| \neq 0} <   \underbrace{\thetah_{i}\theta}_{>0} + \frac{(-\theta)^2}{2} + \lambda \mathds{1}_{|-\theta| \neq 0} = g_i(-\theta), $$
%
so that for the minimizer of $g_i$ it must hold that it has the same sign as $\thetah_i$ in this case.
%
Now differentiating  $g_i$ (for $\theta>0$) and setting it to zero implies
%
\begin{align*}
%	
	&\frac{\partial g_i(\theta)}{\partial \theta} = - \thetah_{i} + \theta \stackrel{!}{=} 0 \\
%	
	\Leftrightarrow \quad & \theta = \thetah_{i}.
%	
\end{align*}
%
For sake of completeness, the second derivative is $\frac{\partial^2 g_i(\theta)}{\partial^2 \theta} = 1 >0$ so that we have indeed a minimum.
%
Thus, for the minimizer $\theta^*_i$ of $g_i$ it must hold that $\theta^*_i=\thetah_i.$
%
\item By taking the constraint of the case into account, we can write $\theta^*_i=\thetah_i \mathds{1}_{|\thetah_{i}| > \sqrt{\lambda}}.$
%
\item Consider the complementary case, i.e., $|\thetah_i|\leq \sqrt{\lambda}.$ 
%
As seen above it holds that $g_i(0) = 0.$ 
%
Consider the smooth extension of $g_i:$
%
\begin{align*}
%	
	\tilde g_i(\theta) =  
		- \thetah_{i}\theta + \frac{\theta^2}{2} + \lambda 
%	
\end{align*} 
%
and note that $\tilde g_i$ and $g_i$ are the same for any $\theta \neq 0,$ while  $\tilde g_i$ is smooth on $\R$ and $g_i$ is discontinuous at $0.$
%
In particular, it holds that $g_i(\theta) \leq \tilde g_i(\theta),$ with equality for any $\theta\neq 0$ and strict inequality for $\theta=0.$  
%
So the minimizer of $g_i$ is either the same as the one for $\tilde g_i$ or it is zero.
%
Similarly as above, it holds for any $\theta \neq 0$ with $sgn(\theta)=sgn(\thetah_i),$ i.e., $\thetah_{i}\theta>0$, that
%
$$  \tilde g_i(\theta) =   \underbrace{-\thetah_{i}\theta}_{<0} + \underbrace{\frac{\theta^2}{2}}_{=\frac{(-\theta)^2}{2} } + \lambda <   \underbrace{\thetah_{i}\theta}_{>0} + \frac{(-\theta)^2}{2} + \lambda  = \tilde g_i(-\theta), $$
%
so that for the minimizer of $\tilde g_i$ it must hold that it has the same sign as $\thetah_i$ in this case.
%
Now differentiating  $\tilde g_i$ and setting it to zero implies
%
\begin{align*}
	%	
	&\frac{\partial \tilde g_i(\theta)}{\partial \theta} = - \thetah_{i} + \theta \stackrel{!}{=} 0 \\
	%	
	\Leftrightarrow \quad & \theta = \thetah_{i}.
	%	
\end{align*}
%
However, if $\thetah_i \neq 0$ then
%
\begin{align*}
	%	
	g_i(\thetah_i) 
	%	
	&= - \thetah_i^2 +  \frac{\thetah_i^2 }{2} + \lambda \tag{If $\thetah_i \neq 0$} \\
	%	
	&= - \frac{\thetah_i^2 }{2} + \lambda \\
	%	
	& \geq \frac{\lambda}{2}, \tag{$|\thetah_i|\leq \sqrt{\lambda}$ $\Rightarrow$ $- \thetah_i^2 \geq -\lambda$}
	%	
\end{align*}
%
so that $g_i(\thetah_i)  > 0 = g_i(0).$
%
Hence, the minimizer $\theta^*_i$ of $g_i$ is $\theta^*_i=0,$ which can be written, by taking the constraint of the case into account, as $\theta^*_i=\thetah_i \mathds{1}_{|\thetah_{i}| > \sqrt{\lambda}}.$
%
\item In summary, we have shown that the minimizer $\theta^*_i$ of $g_i$ is $\theta^*_i=\thetah_i \mathds{1}_{|\thetah_{i}| > \sqrt{\lambda}}$ for any $i=1,\ldots,p.$
%
Since 
%
$$\min_{\thetab} \riskrt  = 	\min_{\thetab} \sum_{i=1}^p	- \thetah_{i}\theta_i + \frac{\theta_i^2}{2} + \lambda \mathds{1}_{|\theta_i| \neq 0} = \min_{\thetab} \sum_{i=1}^p g_i(\theta_i),$$
%
we conclude that 
%
$\thetah_{\text{L0}} = (\thetah_{\text{L0},1},\ldots,\thetah_{\text{L0},p})^\top$ given by
%
$$		\thetah_{\text{L0},i} = \thetah_{i} \mathds{1}_{|\thetah_{i}| > \sqrt{\lambda}}, \quad i=1,\ldots,p,	$$
%
is the minimizer of the $L_0$-regularized empirical risk over the linear models.
%
\end{itemize}