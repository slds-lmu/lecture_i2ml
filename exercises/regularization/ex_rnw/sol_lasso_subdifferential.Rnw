\begin{enumerate}
  \item The Taylor approximation of first order of a function $f(x)$ at point $x_0$ is $$f(x) \approx f(x_0) + f^\prime(x_0)(x-x_0).$$ On the other hand, a differentiable function $f$ is said to be convex on an interval $\mathcal{I}$ if and only if $$f(x) \geq f(x_0) + f^\prime(x_0)(x-x_0)$$ for all points $x,x_0 \in \mathcal{I}$.
  \begin{enumerate}
  \item  If we approximate a convex function with a Taylor approximation of first order, we will always get a lower bound at the given point as the second equation states. 
  \item Visualization of such an approximation for $2x^2$ on $\mathcal{I} = [-2,2]$ (we will only later see how to calculate a derivative(-like) measure for the non-differentiable functions). The approximation in this case is $f(x) \approx 2 x_0^2 + 4 x_0 (x-x_0) = -2x_0^2 + 4 x_0 x$ and for $x=0$ exactly the function $2x^2$ mirrored at the $f(x) = 0$ axis. We can plot this for several values of x:
  \end{enumerate}
<<echo=TRUE, fig.height=5, fig.width=7>>=
xx <- seq(-2, 2, by = 0.01)
yy <- 2*xx^2
# this will give us the approximation function for x=0
# and what happens if we vary x (its slope)
# for given x0
approx_fun <- function(x0) c(-2*x0^2, 4*x0)

plot(xx, yy, type = "l", xlab = "x", ylab = "f(x)", ylim=c(-4,10), col ="red", lwd=2.5)
for(x0 in seq(-2,2,by=0.5))
  abline(approx_fun(x0), col = rgb(0,0,0,0.5))
@
  \item A subdifferential of $f$ is a set of values $\breve{\nabla}_x f$ defined as $$\breve{\nabla}_x f = \{ g: f(x) \geq f(x_0) + g \cdot (x-x_0) \, \forall x \in \mathcal{I} \}.$$ Every scalar value $g \in \breve{\nabla}_x$ is said to be a subgradient of $f$.  A subdifferential thus generalizes the idea of a lower approximation from before by replacing $f^\prime(x_0)$ with any constant $g$ for which the approximation is still strictly below the objective function $f$.
  \item We can make use of subdifferentials for convex but non-differentiable loss functions like the one induced by the Lasso, because we are now not restricted to cases where we can compute $f^\prime(x_0)$. It holds that:\\
  \begin{center}
  A point $x_0$ is the global minimum of a convex function $f$ $\Leftrightarrow$ $0$ is contained in the subdifferential $\breve{\nabla}_x f$.\\
  \end{center}
  We can define a subdifferential at point $x_0$ also as a non-empty interval $[x_l,x_u]$ where the lower and upper limit is defined by $$x_l = \lim_{x \to x_0^{-}} \frac{f(x)-f(x_0)}{x-x_0}, \quad x_u = \lim_{x \to x_0^{+}} \frac{f(x)-f(x_0)}{x-x_0}.$$ These resemble the limits of the derivative $\partial f / \partial x$ evaluated at a point very close to $x_0$ when coming from the left or right side, respectively. 
  \begin{enumerate}
  \item In the case for $f(x) = |x|$, $\lim_{x\to 0^{\pm}} |x|/x = \pm 1$ and thus $\breve{\nabla}_x f = [-1,1]$ at $x_0 = 0$.
  \item $x_0$ is a global minimum as $0 \in \breve{\nabla}_x f$
  \item The $L1$ penalty has no derivative at $\theta_k = 0$ for all $\theta_k$ with $k\in \{1,\ldots,p\}$. Thus we are particularly interested in the subdifferential at this point, which is  $$\breve{\nabla}_{\theta_k} \lambda \sum_{j=1}^p |\theta_j| = \sum_{j=1}^p \breve{\nabla}_{\theta_k} \lambda |\theta_j| = \breve{\nabla}_{\theta_k} \lambda |\theta_k| = [-\lambda, \lambda],$$ where in the second equation we use that the subdifferential of a constant function is zero. For a (sub-) gradient at any other differentiable point, we get the conventional gradient using the given hint, which is $-\lambda$ for $\theta_k < 0$ and $\lambda$ for $\theta_k > 0$.
  \end{enumerate}
  \item The subdifferential for the Lasso w.r.t. $\theta_2$ is then simply the combination of the standard gradient for the unregularized risk $\nabla_{emp} := n^{-1} \sum_{i=1}^n -2 x^{(i)}_{2} (y^{(i)} - x^{(i)}_{1}\theta_1 - x^{(i)}_{2}\theta_2)$ plus the subdifferential for the penalty: $$\breve{\nabla}_{\theta_2} \mathcal{R}_{reg} =  
  \begin{cases} 
  \nabla_{emp} - \lambda & \text{if } \theta_2 < 0\\
  [\nabla_{emp} - \lambda, \nabla_{emp}  + \lambda] & \text{if } \theta_2 = 0\\
  \nabla_{emp} + \lambda & \text{if } \theta_2 > 0.
  \end{cases}
  $$
  \end{enumerate}
  % \textbf{Extra}: So far we only have derived the subdifferential for a given $\theta_k$ (or here $\theta_2$) but no update routine yet. Being able to derive a closed form solution in each step, we use this solution to update $\theta_k$ in each step. Here, we set $\breve{\nabla}_{\theta_2} \mathcal{R}_{reg}$ to zero and rewritte the results in terms of $\theta_2$ to get a corresponding update policy. First, note that $$\nabla_{emp} = 2n^{-1} (-c + \sum_i (x_2^{(i)})^2 \theta_2),$$ where $c$ is constant term w.r.t. $\theta_2$ and we know that $\sum_i (x_2^{(i)})^2 > 0$. For the first and the third case from the subdifferential above $\breve{\nabla}_{\theta_2} \mathcal{R}_{reg} = 0$ implies that our updated parameter 
  % $$\theta_2=\begin{cases} 
  % (c + 0.5n\lambda)/ \sum_i (x_2^{(i)})^2 & \text{for } c < - 0.5n\lambda  \\
  % (c - 0.5n\lambda)/ \sum_i (x_2^{(i)})^2 & \text{for } c > 0.5n\lambda.
  % \end{cases}
  % $$
  % For the special second case, setting the interval to zero implies we use $\theta_2 = 0$ if $- 0.5n\lambda \leq c \leq 0.5n\lambda$ holds.



