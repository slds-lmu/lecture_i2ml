A medical research group wants to train a supervised regression model to predict how long a patient stays in the hospital after admission (measured in days). The three researchers Holger, Laetitia, and Lisa have gathered data on each patientâ€˜s age (in years), systolic blood pressure (in mmHg, a continuous scale), and weight (in kg).
Formally, this gives rise to the target space $\Yspace = \R$ (where we technically allow for negative values as a prerequisite of the linear model) and the feature space $\Xspace = (\R_{0}^{+})^3$, with $\xi = (x_{age},\;x_{blood\;pressure},\;x_{weight})^{(i)} \in \Xspace$ 
  for $i = 1, 2, \dots, n$ observations. 
At first, they believe all of these three features should be modeled to have a linear effect on the target variable. To construct their supervised regression model, they remember how most of such models can be described:
\medbreak
\textbf{Learning = Hypothesis Space + Risk + Optimization.}
\medbreak
With the first of the three components of learning in his mind, Holger comes up with the following hypothesis space:
\begin{align}
\Hspace = \{f: \fx = \theta_0 + \theta_1 \cdot x_{age} + \theta_2 \cdot x_{blood\;pressure} + \theta_3 \cdot x_{weight} \enspace | \enspace \thetav \in \N^{3}\}
\end{align}

\begin{enumerate}\bfseries
  \item[1)] Is the hypothesis space formulated above correct? If not, find the correct one.
\end{enumerate}

Lisa recalls the second component of learning: risk. She remembers two slightly different functions for calculating the empirical risk, but is not sure what difference they will make in optimization:
\begin{align}
\riskef = & \sumin \Lxyi \\
\riskeb(f) = & \frac{1}{n}\sumin \Lxyi
\end{align}

\begin{enumerate}\bfseries
  \item[2)] What difference may the two functions make for optimization?
\end{enumerate}

As a next step, the research group debates the influence the loss function (L1 or L2 loss) they choose has on the three components of learning.

\begin{enumerate}\bfseries
  \item[3)] Which of the three components of learning are impacted by the choice of the loss function in the case of the linear regression model?
\end{enumerate}

Laetitia suggests to use L2 loss for their linear regression model, and notes this would require no numerical optimization to find the parametrization with lowest empirical risk.

\begin{enumerate}\bfseries
  \item[4)] Is this true or false? Explain your answer.
\end{enumerate}

After training the linear regression model, Holger proposes to add more flexibility by training a polynomial regression model with degree $d = 1$ for age and systolic blood pressure, and $d = 2$ for weight. Once again, he formulates a hypothesis space:
\begin{align}
\Hspace = \{f: \fx = \theta_0 + \theta_1 \cdot x_{age} + \theta_2 \cdot x_{blood\;pressure} + \theta_3 \cdot (x_{weight}+x_{weight}^2) \enspace | \enspace \thetav \in \R^{4}\}
\end{align}

\begin{enumerate}\bfseries
  \item[5)] Is the hypothesis space formulated above correct? If not, find the correct one.
\end{enumerate}

The research group decides to train the polynomial regression model using L2 loss. Again, Laetitia is worried about the optimization component of learning. She is not entirely sure how to find the parametrization with lowest empirical risk.

\begin{enumerate}\bfseries
  \item[6)] Explain how the model with the lowest empirical risk can be found in the case of the polynomial regression model with L2 loss.
\end{enumerate}