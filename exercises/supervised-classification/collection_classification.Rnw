% !Rnw weave = knitr

<<setup-child, include = FALSE, echo=FALSE>>=
library('knitr')
knitr::set_parent("../../style/preamble_ueb_coll.Rnw")
@

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{Supervised Classification}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{foo}{
<<child="ex_rnw/ex_logreg_dec_boundaries.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_logreg_dec_boundaries.Rnw">>=
@
}

\aufgabe{foo}{
<<child="ex_rnw/ex_softmax.Rnw">>=
@
}
\dlz
\loesung{
<<child="ex_rnw/sol_softmax.Rnw">>=
@
}

\aufgabe{foo}{
<<child="ex_rnw/ex_mlr_dec_boundaries.Rnw">>=
@
}
\dlz
\loesung{

See \href{https://github.com/compstat-lmu/lecture_i2ml/blob/master/exercises/supervised-classification/sol_mlr_dec_boundaries.R}{R code}
}
\dlz

\aufgabe{foo}{
<<child="ex_rnw/ex_knn_manh.Rnw">>=
@
}
\dlz
\loesung{
<<child="ex_rnw/sol_knn_manh.Rnw">>=
@
}
\newpage

\aufgabe{foo}{
<<child="ex_rnw/ex_naive_bayes.Rnw">>=
@
}
\dlz
\loesung{
<<child="ex_rnw/sol_naive_bayes.Rnw">>=
@
}

\newpage

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\dlz
\exexams
\lz

\aufgabeexam{WS2020/21}{main}{1}{

<<echo=FALSE, fig.align="center", fig.width=5, fig.height=3>>=
library(ggplot2)
set.seed(31415)


x1 <- runif(49, 0.1, 1.9)
x2 <- runif(98, 2.1, 5.9)
x3 <- runif(49, 6.1, 7.9)
x4 <- c(1,3,5,7)
x <- c(x1, x2, x3, x4)

y1 <- 2 + rnorm(49, 0, 0.02)
y2 <- 4 + rnorm(98, 0, 0.02)
y3 <- 3 + rnorm(49, 0, 0.02)
y4 <- c(4, 2, 3, 4)
y <- c(y1, y2, y3, y4)

d = data.frame(x, y)

ggplot(data = d, aes(x = x, y = y)) +
  geom_point(alpha = 0.25)

@

The above plot shows $\D = \Dset $, a data set with %$n$ observations
%data $\xv = (x^{(1)}, \dots, x^{(n)})^\top$ and $\ydat = \yvec$ for 
$n$ = 200 observations of a continuous target variable $y$ and a continuous, 
1-dimensional feature variable $\xv$. In the following, we aim at predicting 
$y$ with a machine learning model that takes $\xv$ as input.

\begin{enumerate}
  \item Since the data seem to fall in 3 quite well-separable classes, we now 
  want to apply a classification model instead of the regression model in a). 
  To prepare the data for classification, we categorize the target variable $y$ 
  in 3 classes and call the transformed target variable $z$, as follows:
  \[
    z^{(i)} = \left\{\begin{array}{lr}
        1, \text{ if }&  -\infty < \yi \leq 2.5 \\
        2, \text{ if }&  2.5\ < \yi \leq 3.5 \\
        3, \text{ if }&  3.5 < \yi < \infty
        \end{array} \right\}
  \]
  Now we can apply quadratic discriminant analysis (QDA):
  \begin{enumerate}
    \item[(i)] Estimate the class means $\mu_k = \E(\xv|z = k)$ for each of the 
    three classes $k \in \{1, 2, 3\}$ visually from the plot. Do not 
    overcomplicate this, a rough estimate is sufficient here.
    \item[(ii)] Make a hand-drawn plot that visualizes the different estimated 
    densities per class.
    \item[(iii)] How would your drawing from (ii) change if we used linear 
    discriminant analysis (LDA) instead of QDA? Explain your answer.
    \item[(iv)] Why is QDA for this data preferable over LDA?
  \end{enumerate}  
  \item Given are two new observations $\xv_{*1} = -10$ and $\xv_{*2} = 7$. 
  State the prediction for each of the two models 
  \begin{enumerate}
    \item[(i)] regression tree (from a))
    \item[(ii)] QDA (from b))
  \end{enumerate}
  and explain how you derived the predictions.
  \item Discuss in 1-2 sentences which of the 2 models (regression tree, QDA) 
  you would prefer for modeling the data and explain your decision.
\end{enumerate}

\newpage

\dlz
\loesung{

\begin{enumerate}
  \item \phantom{foo}
  \begin{enumerate}
    \item[(i)] $\mu_1 = 1$, $\mu_2 = 7$, $\mu_3 = 4$
    \item[(ii)] 
    <<echo=FALSE, message=FALSE, warning = FALSE, fig.height=4, fig.width=5>>=
    library(MASS)
    z <- as.factor(ifelse(y<2.5, 1,
                        ifelse(y<3.5, 2, 3)))
  
    d1 <- data.frame(z, x)
    qd <- qda(z ~ x, d1)
    
    mean_vec <- c(mean(x[z==1]),
                  mean(x[z==2]),
                  mean(x[z==3]))
    sd_vec <- c(sd(x[z==1]),
                  sd(x[z==2]),
                  sd(x[z==3]))
    x_grid <- seq(0,8, by=0.01)
    plot(x_grid, dnorm(x_grid, mean = mean_vec[1], sd = sd_vec[1]),type="l", ylab = "density")
    lines(x_grid, dnorm(x_grid, mean = mean_vec[2], sd = sd_vec[2]), col='blue')
    lines(x_grid, dnorm(x_grid, mean = mean_vec[3], sd = sd_vec[3]), col = 'orange')
    legend('top', legend = c("z = 1","z = 2","z = 3"), col=c('black', 'blue', 'orange'), lty=1)
    @
    \item[(iii)] Variances would be all equal. Assumption of LDA is equal 
    variances, i.e., estimatated models will always have equal variances, no 
    matter if this fits the data or not
    \item[(iv)] Variances seem not to be equal, this is only captured in QDA
  \end{enumerate}  
  \item Given are two new observations $\xv_{*1} = -10$ and $\xv_{*2} = 7$. 
  State the prediction for each of the two models 
  \begin{enumerate}
    \item[(i)] $\hat{y}_{*1} = 2$, $\hat{y}_{*2} = 3$, 
    \item[(ii)] $\hat{z}_{*1} = 3$, since the variance of class 3 is higher, 
    the density will overshoot the density of class 1. 
    $\hat{z}_{*2} = 2$, obviously highest posterior here.
  \end{enumerate}
  and explain how you derived the predictions.
  \item E.g., 
  \begin{itemize}
    \item CART better than LM because I do not have to specify those indicator 
    functions manually and estimate the split points manually, CART does this 
    data driven
    \item For QDA we have to throw away information of y, this favors CART
    \item QDA predicts the middle class (3) for very extreme observations, this 
    does not seem right. However, we do not know how data behave outside the 
    bounds of x.
    \item QDA assumes gaussian distributions which is clearly not the case.
  \end{itemize}
\end{enumerate}

}

% ------------------------------------------------------------------------------

\newpage
\aufgabeexam{WS2020/21}{main}{3}{

Consider a binary classification algorithm that yielded the following results 
on 8 observations. The table shows  true classes and  predicted probabilities
for class 1:

\begin{tabular}{ | c | c | c | }
  \hline
  ID & True Class & Prediction  \\ \hline
  1 & 1 & 0.50  \\
  2 & 0 & 0.01  \\
  3 & 0 & 0.90  \\
  4 & 0 & 0.55  \\
  5 & 1 & 0.10  \\
  6 & 1 & 0.72  \\
  7 & 1 & 0.70  \\
  8 & 1 & 0.99  \\
  \hline
\end{tabular}

\begin{enumerate}
  \item Draw the ROC curve of the classifier manually. Explain every step 
  thoroughly, and make sure to annotate the axes of your plot.
  \item Calculate the AUC of the classifier. Explain every step of your 
  computation thoroughly.
  \item Calculate the partial AUC for FPR $\leq 1/3$. Explain every step of your 
  computation thoroughly.
  \item Create a confusion matrix assuming a threshold of 0.75. Point out which 
  values correspond to true positives (TP), true negatives (TN), false positives 
  (FP), and false negatives (FN).
  \item Compute sensitivity, specificity, negative predictive value, positive 
  predictive value, accuracy and F1-score. State the respective formulas first.
  \item In the following plot, you see the ROC curves of two different 
  classifiers with similar AUC's. Describe a practical situation where you would 
  prefer classifier 1 over classifier 2 and explain why. (Note: The data in this 
  question is not related to the data of the above questions.)
  <<include=TRUE, results="asis", echo=FALSE, fig.width=5, fig.height=3.5>>=
  library(reshape)
  roc_data1 <- data.frame(TPR = c(0, 0.2, 0.4, 0.6, 0.6, 0.6, 0.6, 0.8, 0.8, 1),
                          FPR = c(0, 0.0, 0.1, 0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 1))
  roc_data2 <- data.frame(TPR = c(0, 0.4, 0.5, 0.65, 0.90, 0.95, 1),
                          FPR = c(0, 0.4, 0.4, 0.4, 0.5, 0.6, 1))
  melted <- melt(list(
    classifier1 = roc_data1,
    classifier2 = roc_data2),
    id.vars = c("TPR","FPR"))
  names(melted)[3] <- "Classifier"
  cols = c("blue", "orange")
  ggplot(melted, aes(x = FPR, y = TPR, colour = Classifier, group=Classifier)) +
    geom_line(aes(linetype = Classifier)) +
    scale_colour_manual(values = cols) +
    geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
  @
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}

  \item Scores: \\
   <<echo=FALSE, message=FALSE, warning = FALSE>>=
  library(pROC)  
  set.seed(1414)
  labels <- sample(c(0,1), 8, replace = TRUE)
  preds <- c(0.5, .01, .9, .55, .1, .72, .7, .99)
  cdata <- data.frame(
    true_labels = labels,
    scores = preds)
  cdata <- cdata[order(cdata$scores, decreasing = T),]
  knitr::kable(cdata)
  @
  Here we see that $\frac{1}{n_+} = \frac{1}{5} = 0.2$ and $\frac{1}{n_-} = 
  \frac{1}{3}$. Now we follow the algorithm as described in the lecture slides:
  \begin{itemize}
    \item Set  $\tau = 1$, so we start in $(0,0)$; we predict everything as 1.
    \item Set  $\tau = 0.95$ yields TPR $0 + \frac{1}{n_+} = 0.2$ and FPR  0. 
    (Obs. 8 is '1')
    \item Set  $\tau = 0.8$ yields TPR $0.2$ and FPR  $0 + \frac{1}{n_-} = 1/3$. 
    (Obs. 3 is '0')
    \item Set  $\tau = 0.71$ yields TPR $0.2 + \frac{1}{n_+} = 0.4$ and FPR  
    $1/3$. (Obs. 6 is '1')
    \item Set  $\tau = 0.6$ yields TPR $0.4 + \frac{1}{n_+} = 0.6$ and FPR  
    $1/3$. (Obs. 7 is '1')
    \item Set  $\tau = 0.52$ yields TPR $0.6$ and FPR  $1/3 + \frac{1}{n_-} = 
    2/3$. (Obs. 4 is '0')
    \item Set  $\tau = 0.3$ yields TPR $0.6+ \frac{1}{n_+} = 0.8$ and FPR  
    $2/3$. (Obs. 1 is '1')
    \item Set  $\tau = 0.05$ yields TPR $0.8 + \frac{1}{n_+} = 1$ and FPR  
    $2/3$. (Obs. 5 is '1')
    \item Set  $\tau = 0$ yields TPR $1$ and FPR  $2/3 + \frac{1}{n_-} = 1$. 
    (Obs. 2 is '0')
  \end{itemize}
  Therefore we get the polygonal path consisting of the ordered list of vertices 
  \[(0,0), (0,0.2), (1/3,0.2), (1/3,0.4), (1/3,0.6), (2/3,0.6), (2/3,0.8), 
  (2/3, 1), (1, 1).\]
  <<echo=FALSE, message=FALSE, warning = FALSE, fig.height=3, fig.width=3>>=
  library(pROC)  
  set.seed(1414)
  labels <- sample(c(0,1), 8, replace = TRUE)
  preds <- c(0.5, .01, .9, .55, .1, .72, .7, .99)
  ROC <- roc(labels, preds)
  plot(ROC, legacy.axes=TRUE)
  @   
  
  \item The AUC is the sum of three rectangles: $0.2 \cdot 1/3 + 0.6 \cdot 1/3 + 
  1\cdot 1/3 = 0.6$
  
  \item The partial AUC is the area under the curve that is left from 
  FPR $= 1/3$, so it is just the first rectangle: $0.2 \cdot 1/3 = 1/15$
  
  \item \phantom{foo}
  \begin{tabular}{ | c | c | c | } \hline
    & Actual Class - 0 & Actual Class - 1  \\
    Prediction - 0 & 2 & 4  \\
    Prediction - 1 & 1 & 1  \\ \hline
  \end{tabular}
  so we get
  \begin{tabular}{ | c | c | c | c | }
    \hline
    FN & FP & TN & TP   \\ \hline
    4 & 1 & 2 & 1 \\ \hline
  \end{tabular}
    
  \item $$\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} =
  \frac{1}{5} $$
  $$\text{Specificity}  = \frac{\text{TN}}{\text{TN} + \text{FP}} =\frac{2}{3}$$
  $$\text{Negative Predictive Value} = \frac{\text{TN}}{\text{TN} + \text{FN}} =
  \frac{1}{3} $$
  $$\text{Positive Predictive Value} = \frac{\text{TP}}{\text{TP} + \text{FP}} =
  \frac{1}{2} $$
  $$\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + 
  \text{FP} + \text{FN}} =\frac{3}{8} $$
  $$\text{F1-score} = \frac{2\cdot\text{PPV}\cdot\text{Sensitivity}}{\text{PPV}
  +\text{Sensitivity}} = (0.2) / (0.7) = 2/7  $$
  
  \item Important point is to understand that classifier 1 does better in the
  'high scores'. Or: For some threshold below 0.5 the precision is far better
  for classifier 1 than for classifier 2. For example if you want to output a
  certain number of 'most promising customers' this could be a good idea.

\end{enumerate}
}

% ------------------------------------------------------------------------------

\dlz
\aufgabeexam{WS2020/21}{main}{4}{

The table below shows $\D = \Dset $, a data set with 
$n$ = 8 observations of a binary target variable $y$ containing the information 
if the object is a \texttt{Banana} or not and a 4-dimensional feature vector 
$\xv$, containing \texttt{Color}, \texttt{Form}, \texttt{Origin} (categorical 
features) and \texttt{Length} (continuous feature) of the object. In the 
following, we aim at predicting \texttt{Banana} with a machine learning model 
that takes $\xv$ as input.

\begin{tabular}{ | c | c | c | c | c | c |}
\hline
ID  &  Color   &  Form    &  Origin    & Length [cm]  &  Banana  \\  \hline
1   &  yellow  &  oblong  &  imported  & 15           &  yes      \\
2   &  yellow  &  round   &  domestic  & 5            &  no       \\
3   &  yellow  &  oblong  &  imported  & 10           &  no       \\
4   &  brown   &  oblong  &  imported  & 17           &  yes      \\
5   &  brown   &  round   &  domestic  & 16           &  no        \\
6   &  green   &  round   &  imported  & 13           &  yes       \\
7   &  green   &  oblong  &  domestic  & 25           &  no       \\
8   &  red     &  round   &  imported  & 7            &  no        \\
\hline
\end{tabular}

We want to use a naive Bayes classifier to predict the label of a new 
observation. 

\begin{enumerate}
  \item Calculate the posterior probability $\pi(\xv_*) = \P(y = \text{yes} | 
  \xv_*)$ for a new observation \ $\xv_* = (\text{green}, \text{oblong}, 
  \text{imported}, 14)^\top$. Explain every step thoroughly. (Hint: At some 
  point you will have to compute values of Gaussian densities - use R for this
  step.)
  \item How would you classify the new observation? Explain your answer.
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}

  \item The features $\xv := (x_\text{Color},x_\text{Form},x_\text{Origin},
  x_\text{Length})$ given the category $y \in \{\text{yes},\text{no}\}$ are
  assumed to be conditionally independent of each other (since we are using
  Naive Bayes), s.t.

  $$p(\xv|y = k) = p(x_\text{Color}|y = k)\cdot p(x_\text{Form}|y = k) \cdot
  p(x_\text{Origin}|y = k) \cdot p(x_\text{Length}|y = k).$$

  For the posterior probabilities $\pi_k(\xv) = \P(y = k | \xv)$ it holds with
  Bayes' Theorem:
  \begin{align*}
  \pi_k(\xv)  \propto & \; \underbrace{\pi_k \cdot p(\xv|y = k)
  }_{=: \alpha_k(\xv)} \\
  \iff & \exists c \in \mathbb{R}: \pi_k(\xv) = c \cdot \alpha_k(\xv),
  \end{align*}

  where $\pi_k = \P(y = k)$ is the prior probability of class $k$.

  From this and since the posterior probabilities need to sum up to 1, it holds
  that
  \begin{align*}1 =& \; c \cdot \alpha_\text{yes}(\xv) +  c \cdot
  \alpha_\text{no}(\xv) \\
  \iff & c = \frac{1}{\alpha_\text{yes}(\xv) + \alpha_\text{no}(\xv)}.
  \end{align*}

  This means, the scores $\alpha_\text{yes}(x)$ and $\alpha_\text{no}(x)$ are
  needed to compute  $$\pi_\text{yes}(\xv) = \frac{\alpha_\text{yes}(\xv)}{
  \alpha_\text{yes}(\xv) + \alpha_\text{no}(\xv)}. $$

  For the new observation $\xv_*$ we have to estimate the respective
  probabilities and densities. For the categorical features, we can simply
  compute relative frequencies. For the continuous features \texttt{Length} we
  have to estimate the densities per class and evaluate those at the value $14$.
  Doing this with R we end up with:

   <<echo=FALSE, message=FALSE, warning = FALSE>>=
  x_yes <- c(15,17,13)
  x_no <- c(5,10,16,25,7)
  m_yes = mean(x_yes)
  s_yes = sd(x_yes)

  m_no = mean(x_no)
  s_no = sd(x_no)

  dnorm(14, m_yes, s_yes)
  dnorm(14, m_no, s_no)
  @

  \begin{center}
  \begin{tabular}{ | c | c | c | c | c | c |}
  \hline
  k     &  $\hat{\pi}_k$   &  $\hat{p}(\text{green}|y = k)$    &  
  $\hat{p}(\text{oblong}|y = k)$    & $\hat{p}(\text{imported}|y = k)$    & 
  $\hat{p}(\text{14}|y = k)$   \\  \hline
  yes   &  3/8  &  1/3    &  2/3  & 3/3     &  0.176 \\
  no    &  5/8  &  1/5    &  2/5  & 2/5     &  0.049 \\
  \hline
  \end{tabular}
  \end{center}

  \begin{align*}
  \hat{\alpha}_\text{yes}(x) = & \;  \hat{\pi}_{yes} \cdot \hat{p}(
  \text{green}|y = \text{yes})\cdot \hat{p}(\text{oblong}|y = \text{yes}) \cdot 
  \hat{p}(\text{imported}|y = \text{yes}) \cdot \hat{p}(14|y = \text{yes}) \\
  = & \; \frac{3}{8} \cdot \frac{1}{3} \cdot \frac{2}{3} \cdot 1 \cdot 0.176  
  \approx 0.0147, \\
  \hat{\alpha}_\text{no}(x) = & \;  
  \hat{\pi}_{no} \cdot \hat{p}(\text{green}|y = \text{no})\cdot 
  \hat{p}(\text{oblong}|y = \text{no}) \cdot \hat{p}(\text{imported}|y = 
  \text{no}) \cdot \hat{p}(14|y = \text{no}) \\
  = & \; \frac{5}{8} \cdot \frac{1}{5} \cdot \frac{2}{5} \cdot \frac{2}{5} 
  \cdot 0.049  \approx 0.00098.
  \end{align*}
  
  With this we can calculate the posterior probability
  $$\hat{\pi}_\text{yes}(x) = \frac{\hat{\alpha}_\text{yes}(x)}{
  \hat{\alpha}_\text{yes}(x) + \hat{\alpha}_\text{no}(x)} \approx 0.937.$$
  
  \item Classification as yes = \texttt{banana}. We have to define a threshold, 
  observations with a posterior probability equal or above the threshold are 
  hard labeled as yes, others as no. The optimal threshold has to be chosen, 
  e.g., inspecting ROC measures. With default 0.5 we end up with the above 
  classification.

\end{enumerate}
}

% ------------------------------------------------------------------------------

\newpage
\aufgabeexam{WS2020/21}{retry}{3}{

Consider a binary classification algorithm that yielded the following results on 
8 observations. The table shows  true classes and  predicted probabilities for 
class 1:

\begin{tabular}{ | c | c | c | }
  \hline
  ID & True Class & Prediction  \\ \hline
  1 & 1 & 0.30  \\
  2 & 0 & 0.91  \\
  3 & 0 & 0.03  \\
  4 & 0 & 0.55  \\
  5 & 0 & 0.45  \\
  6 & 0 & 0.65  \\
  7 & 1 & 0.71  \\
  8 & 1 & 0.98  \\
  \hline
\end{tabular}

\begin{enumerate}
  \item Draw the ROC curve of the classifier manually. Compute all relevant 
  numbers explicitly, state the respective formulas first, and make sure to 
  annotate the axes of your plot.
  \item Calculate the AUC of the classifier. Explain every step of your 
  computation thoroughly.
  \item Calculate the partial AUC for TPR $\geq 2/3$. Explain every step of your 
  computation thoroughly.
  \item Create a confusion matrix assuming a threshold of 0.7. Point out which 
  values correspond to true positives (TP), true negatives (TN), false positives 
  (FP), and false negatives (FN).
  \item Compute sensitivity, specificity, negative predictive value, positive 
  predictive value, accuracy and F1-score. State the respective formulas first.  
  \item Now look at the following plot. Explain thoroughly why the solid line 
  cannot be the ROC curve of a classifier.
  <<include=TRUE, results="asis", echo=FALSE, fig.width=3, fig.height=2.5>>=
  library(reshape)
  roc_data1 <- data.frame(TPR = c(0, 0.2, 0.4, 0.6, 0.45, 0.4, 0.7, 0.8, 0.8, 1),
                          FPR = c(0, 0.0, 0.1, 0.2, 0.4, 0.6, 0.6, 0.8, 0.8, 1))
  ggplot(roc_data1, aes(x = FPR, y = TPR)) + 
    geom_line(aes(x = FPR, y = TPR)) + 
    geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
  @   
  \item Imagine you are working in the marketing department of a large company. 
  Your colleagues are developing a marketing campaign where they will call 
  selected customers by phone in order to advertise a new product. Your company 
  has 1,000,000 customers in the database and the budget of the marketing 
  campaign allows to call 1,000 of these customers. Now it is your job to select 
  the most promising 1,000 customers, i.e., those who will most likely buy the 
  new product after having received the advertising phone call. Luckily, you 
  have a large amount of similar data from older marketing campaigns that are 
  representative for this campaign and can be used to train a supervised 
  classification model with the target variable indicating if the customer did 
  buy the product (1) or not (0). You train a random forest on the 1,000,000 
  observations and get the following cross-validated ROC curve. Assuming a 
  balanced target variable: Do you think this model is fairly good for your 
  purpose? Explain why and describe, how you would proceed from here to provide 
  your colleagues with the 1,000 most promising customers.
  <<include=TRUE, results="asis", echo=FALSE, fig.width=3, fig.height=2.5>>=
    library(reshape)
    roc_data1 <- data.frame(TPR = c(0, 0.2, 0.4, 0.4, .4, .6, 0.8, 0.9, 0.95, 1),
                            FPR = c(0, 0.0, 0.05, 0.2, .4, .6, 0.7, 0.8, 0.85, 1))
    ggplot(roc_data1, aes(x = FPR, y = TPR)) + 
      geom_line(aes(x = FPR, y = TPR)) + 
      geom_abline(slope = 1, intercept = 0, linetype = 'dashed')
  @   
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}

  \item First we sort the results by score: \\
   <<echo=FALSE, message=FALSE, warning = FALSE>>=
  library(pROC)  
  set.seed(1414)
  labels <- c(1, 0, 0, 0, 0, 0, 1, 1)
  preds <- c(0.3, .91, .03, .55, .45, .65, .71, .98)
  cdata <- data.frame(true_labels = labels, scores = preds)
  cdata <- cdata[order(cdata$scores, decreasing = T),]
  knitr::kable(cdata)
  @
  Here we see that $\frac{1}{n_-} = \frac{1}{5} = 0.2$ and $\frac{1}{n_+} = 
  \frac{1}{3}$. Now we follow the algorithm as described in the lecture slides:
  \begin{itemize}
    \item Set  $\tau = 1$, so we start in $(0,0)$; we predict everything as 1.
    \item Set  $\tau = 0.95$ yields TPR $0 + \frac{1}{n_+} = 1/3$ and FPR  0. 
    (Obs. 8 is '1')
    \item Set  $\tau = 0.9$ yields TPR $1/3$ and FPR  $0 + \frac{1}{n_-} = 0.2$. 
    (Obs. 2 is '0')
    \item Set  $\tau = 0.70$ yields TPR $1/3 + \frac{1}{n_+} = 2/3$ and FPR  
    $0.2$. (Obs. 7 is '1')
    \item Set  $\tau = 0.6$ yields TPR $2/3$ and FPR  $0.2 + \frac{1}{n_-} = 
    0.4$. (Obs. 6 is '0')
    \item Set  $\tau = 0.5$ yiÂ´elds TPR $2/3$ and FPR  $0.4 + \frac{1}{n_-} = 
    0.6$. (Obs. 4 is '0')
    \item Set  $\tau = 0.4$ yields TPR $2/3$ and FPR  $0.6 + \frac{1}{n_-} = 
    0.8$. (Obs. 5 is '0')
    \item Set  $\tau = 0.1$ yields TPR $2/3 + \frac{1}{n_+} = 1$ and FPR  $0.8$. 
    (Obs. 1 is '1')
    \item Set  $\tau = 0$ yields TPR $1$ and FPR  $0.8 + \frac{1}{n_-} = 1$. 
    (Obs. 3 is '0')
  \end{itemize}
    <<echo=FALSE, message=FALSE, warning = FALSE, fig.height=3, fig.width=3>>=
  library(pROC)  
  set.seed(1414)
  labels <- c(1, 0, 0, 0, 0, 0, 1, 1)
  preds <- c(0.3, .91, .03, .55, .45, .65, .71, .98)
  ROC <- roc(labels, preds)
  plot(ROC, legacy.axes=TRUE)
  @   

  \item The AUC is the sum of three rectangles: 
  $0.2 \cdot 1/3 + 0.6 \cdot 2/3 + 1\cdot 0.2 = 2/3$
  
  \item The partial AUC is the area under the curve that is above TPR $= 2/3$, 
  so it is just the small rectangle in the upper right corner: 
  $0.2 \cdot 1/3 = 1/15$ 
  
  \item   
  \begin{tabular}{ | c | c | c | } \hline
     & Actual Class - 0 & Actual Class - 1  \\
    Prediction - 0 & 4 & 1  \\
    Prediction - 1 & 1 & 2  \\ \hline
  \end{tabular}
  so we get
  \begin{tabular}{ | c | c | c | c | } \hline
    FN & FP & TN & TP   \\ \hline
    1 & 1 & 4 & 2 \\ \hline
  \end{tabular}
  
  \item   
  $$\text{Sensitivity} = \frac{\text{TP}}{\text{TP} + \text{FN}} =\frac{2}{3}$$
  $$\text{Specificity}  = \frac{\text{TN}}{\text{TN} + \text{FP}} =\frac{4}{5}$$
  $$\text{Negative Predictive Value} = \frac{\text{TN}}{\text{TN} + \text{FN}} 
  =\frac{4}{5} $$
  $$\text{Positive Predictive Value} = \frac{\text{TP}}{\text{TP} + \text{FP}} 
  =\frac{2}{3} $$
  $$\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + 
  \text{FP} + \text{FN}} =\frac{6}{8} $$
  $$\text{F1-score} = \frac{2\cdot\text{PPV}\cdot\text{Sensitivity}}{\text{PPV}
  +\text{Sensitivity}} = (8/ 9) / (4/3) = 2/3  $$
  
  \item TPR and FPR are both metrics that increase monotonically as the 
  threshold $c$ traverses from 1 to 0. The plot shows two instances of 
  diminishing TPR, which would mean that, at the corresponding threshold, an 
  observation that had previously been correctly classified as positive was not 
  detected anymore. This is not possible with a binarization threshold.
  
  \item Yes. I would order the customers wrt the scores, then roughly the first 
  200,000 customers would be true positives (since the ROC is based on 1,000,000 
  customers and true class is balanced) and I would just select the top 1,000 
  customers. It doesn't matter that the classifiers gets bad later.
  
\end{enumerate}

}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo