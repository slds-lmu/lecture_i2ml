One popular classification model is \textbf{logistic regression}. The medical research group from last week is eager to find out if it can be used to predict whether a patient admitted to the hospital will require intensive care. This is a binary classification task with target space $\Yspace = \setzo$, with $y=1$ if the patient requires intensive care and $y=0$ if not. The feature space is the same as before:
$\Xspace = (\R_{0}^{+})^3$, with $\xi = (x_{age},\;x_{blood\;pressure},\;x_{weight})^{(i)} \in \Xspace$ for $i = 1, 2, \dots, n$ observations. 

\medbreak
Before the group trains a logistic regression model, researcher Holger remarks they could just as well fit a linear model (LM), as in the case of a binary classification task, both models would make identical predictions. Therefore, he comes up with the following hypothesis space:
\begin{align}
\Hspace = \left\{\pi: \Xspace \to [0,1] ~|~\pix = \thx \right\}
\end{align}

\begin{enumerate}\bfseries
  \item[1)] Are predictions and hypothesis space of a logistic regression model and an LM identical for a binary classification tasks? If not, explain why they could differ and write down the correct hypothesis space.
\end{enumerate}

Researcher Lisa knows that logistic regression follows a discriminant approach, meaning the discriminant functions are optimized directly via empirical risk minimization (ERM). She remembers the general form of ERM:
\begin{align}
\fh = \argmin\limits_{f \in \Hspace} \riskef = \argmin_{f \in \Hspace} \sumin \Lxyi
\end{align}
Additionally, she recalls the Bernoulli loss function of the logistic regression model in statistics:
\begin{align}
\Lpixy = -\;y\ln(\pix)-(1-y)\ln(1-\pix)
\end{align}
Lastly, she recollects how logistic regression models the posterior probabilities $\pixt$ of the labels $\text{--}$ the estimated linear scores are "squashed" through the logistic function $s$:
\begin{align}
\pixt = \frac{\exp\left( \thx \right)}{1+\exp\left(\thx \right)} = \frac{1}{1+\exp\left( -\thx \right)} = s\left( \thx \right)
\end{align}
Given (2)$\;-\;$(4), she figures one could formulate the explicit ERM problem, but leaves the task to you.

\begin{enumerate}\bfseries
  \item[2)] Write down the explicit form of the ERM problem.
\end{enumerate}

Later, the research group trains the logistic regression model and receives a corresponding parameter estimate $\thetabh = (\thetah_0,\; \thetah_{age},\; \thetah_{blood\;pressure},\; \thetah_{weight})$. Researcher Son, who has worked all night on the research problem, finds a function scribbled on his personal notes. He remembers it was useful in the context of a logistic regression model, but does not recall how:
\begin{align}
h\left( \xv^{(i)} ~|~ \thetabh, \; \alpha \right) = \scalebox{1.2}{$\I_{[\alpha, 1]}$} \left( \frac{1}{1+\exp (- \thetabh^T \xi ) } \right), ~~~ \alpha \in \; (0,1)
\end{align}

\begin{enumerate}\bfseries
  \item[3)] What purpose does the function serve in the case of a trained logistic regression model with estimated parameters $\thetabh$? Explain the role of the parameter $\alpha$.
\end{enumerate}

Researcher Son is curious about why the loss function of the logistic regression model in $(3)$ is called \textit{Bernoulli loss}. He seems certain that he can connect it to the Bernoulli distribution, which has the following probability mass function:
\begin{align}
\P(Y=y) = \pi^y (1-\pi)^{1-y}, ~~~ y \in \setzo
\end{align}

\begin{enumerate}\bfseries
  \item[4)] Derive the log-likelihood function $\logl$ of a single Bernoulli distributed random variable $Y$. How is it related to the loss function used for ERM in $(3)$?
\end{enumerate}

