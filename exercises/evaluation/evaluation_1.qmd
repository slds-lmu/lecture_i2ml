---
title: "Exercise 5 -- Evaluation I"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:
  - notebook: ex_eval_1_R.ipynb
    title: "Exercise sheet for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/ex_eval_1_R.ipynb"
  - notebook: ex_eval_1_py.ipynb
    title: "Exercise sheet for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/ex_eval_1_py.ipynb"
  - notebook: sol_eval_1_R.ipynb
    title: "Solutions for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/sol_eval_1_R.ipynb"
  - notebook: sol_eval_1_py.ipynb
    title: "Solutions for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/sol_eval_1_py.ipynb"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

<details> 
<summary>*Hint: useful `Python` libraries*</summary>

{{< embed ex_eval_1_py.ipynb#import echo=true >}}
</details>

## Exercise 1: Evaluating regression learners

::: {.callout-note title="Learning goals" icon=false}
1. Understand importance of using test error for evaluation
2. Discuss choice of performance metric in given situation
:::

Imagine you work for a data science start-up and sell turn-key statistical models. Based on a set of training data, you develop a regression model to predict a customer's legal expenses from the average monthly number of indictments brought against their firm. 

***
Due to the financial sensitivity of the situation, you opt for a very flexible learner that fits the customer's data ($n_{train} = 50$ observations) well, and end up with a degree-21 polynomial (blue, solid). Your colleague is skeptical and argues for a much simpler linear learner (gray, dashed). Which of the models will have a lower empirical risk if standard $L2$ loss is used?

```{r, echo=FALSE, fig.height=3, fig.width=5}
set.seed(123)
x_train <- seq(10, 15, length.out = 50)
y_train <- 10 + 3 * sin(0.15 * pi * x_train) + rnorm(length(x_train), sd = 0.5)
data_train <- data.frame(x = x_train, y = y_train)
ggplot2::ggplot(data_train, ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() +
  ggplot2::geom_smooth(method = lm, formula = y ~ poly(x, 21), se = FALSE) + 
  ggplot2::geom_smooth(
    method = lm, formula = y ~ x, se = FALSE, col = "darkgray", linetype = 2L) + 
  ggplot2::labs(
    x = "average number of indictments per month", 
    y = "legal expenses in million EUR") +
  ggplot2::theme_classic()
```

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

Since the polynomial learner clearly achieves a better fit for the training data and some observations lie rather far from the regression line, which is strongly penalized by $L2$ loss, it will have lower empirical risk than the linear learner.

</details> 
:::

***
Why might evaluation based on training error not be a good idea here?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

First and foremost, evaluation on the training data is almost never a good idea. Under certain conditions the training error does tell us something about generalization ability, but for flexible learners and/or small training data it is deceptive due to optimistic bias. In this particular situation, we have few training observations and quite some points that look a little extreme. A low training error might be achieved by a learner that fits every quirk in the training data but generalizes poorly to unseen points with only slightly different distribution. Evaluation on separate test data is therefore non-negotiable.

</details> 
:::

***
Evaluate both learners using 

i. mean squared error (MSE), and

ii. mean absolute error (MAE).

State your performance assessment and explain potential differences.
Use the code below to create the training and test points.

::: {.panel-tabset}
### R
{{< embed ex_eval_1_R.ipynb#legal-data echo=true >}}
### Python
{{< embed ex_eval_1_py.ipynb#legal-data echo=true >}}
:::

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

::: {.panel-tabset}
### R
Data
{{< embed sol_eval_1_R.ipynb#legal-data echo=true >}}
Train and evaluate
{{< embed sol_eval_1_R.ipynb#legal-learn echo=true >}}
### Python
Data
{{< embed sol_eval_1_py.ipynb#legal-data echo=true >}}
Train and evaluate
{{< embed sol_eval_1_py.ipynb#legal-learn echo=true >}}
:::

The picture is inconclusive: based on MSE, we should prefer the complex polynomial model, while MAE tells us to pick the linear one. It is important to understand that the choices of inner and outer loss functions encode our requirements and may have substantial impact on the result. Our learners differ strongly in their complexity: we have an extremely flexible polynomial and a very robust (though perhaps underfitting) linear one. If, for example, our test data contains an extreme point far from the remaining observations, the polynomial model might be able to fit it fairly well, while the LM incurs a large MSE because the distance to this point enters quadratically. The MAE, on the other hand, is more concerned with small residuals, and there, our LM fares better. Here, following the MAE assessment would signify preference for a robust model.

However, we must keep in mind that our performance evaluation is based on a single holdout split, which is not advisable in general and particularly deceptive with so little data. For different test data we quickly get in situations where the polynomial has both worse MSE and MAE -- after all, slapping a learner with 21 + 1 learnable parameters on a 50-points data set should strike you as a rather bad idea.

Take-home message: the choice of our performance metric matters, and making decisions based on a single train-test split is risky in many data settings.

</details> 
:::


## Exercise 2: Importance of train-test split

::: {.callout-note title="Learning goals" icon=false}
1) Understand how strongly performance estimates can depend on data samples
2) Explain bias-variance trade-off of GE estimator for repeated sampling across different train/test splits
:::

We consider the `CaliforniaHousing` data for which we would like to predict the median house value (`MedHouseVal`) from
the median income in the neighborhood (`MedInc`).

::: {.panel-tabset}
### R
Get data
{{< embed ex_eval_1_R.ipynb#cali-data echo=true >}}
Inspect
{{< embed ex_eval_1_R.ipynb#cali-scatterplot-r echo=true >}}
### Python
Get data
{{< embed ex_eval_1_py.ipynb#cali-data echo=true >}}
Inspect
{{< embed ex_eval_1_py.ipynb#cali-scatterplot echo=true >}}
:::

***
Use the first 100 observations as training data to compute a linear model and evaluate the performance of your learner on the remaining data using MSE.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
Get the data, define a task and corresponding train-test split, and predict with trained model:

::: {.panel-tabset}
### R
Get data
{{< embed sol_eval_1_R.ipynb#cali-data echo=true >}}
Perform split
{{< embed sol_eval_1_R.ipynb#cali-split echo=true >}}
Train and evaluate
{{< embed sol_eval_1_R.ipynb#cali-lm echo=true >}}
### Python
Get data
{{< embed sol_eval_1_py.ipynb#cali-data echo=true >}}
Perform split
{{< embed sol_eval_1_py.ipynb#cali-split echo=true >}}
Train and evaluate
{{< embed sol_eval_1_py.ipynb#cali-lm echo=true >}}
:::

</details> 
:::

***
What might be disadvantageous about the previous train-test split?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
We have chosen the first few observations from a data set that is ordered by feature value, which is not a good idea because this is not a random sample and covers only a particular area of the feature space. Consequently, we obtain a pretty high test MSE (relatively speaking -- we will see in the next exercise which error values we can usually expect for this task). Looking at the data, Looking at the data, this gives us a regression line that does not reflect the overall data situation. Also, the training set is pretty small and will likely lead to poor generalization.
</details> 
:::

***
Now, sample your training observations from the data set at random. Use a share of 0.1 through 0.9, in 0.1 steps, of observations for training and repeat this procedure ten times. Afterwards, plot the resulting test errors (in terms of MSE) in a suitable manner.

<details> 
<summary>*Hint*</summary>

::: {.panel-tabset}
### R
`rsmp` is a convenient function for splitting data -- you will want to choose the "holdout" strategy. Afterwards, \texttt{resample} can be used to repeatedly fit the learner.

### Python
`from sklearn.model_selection import train_test_split` is a convenient function for splitting data. It has an optional parameter `random_state`, which can be used to split the data randomly in each iteration.

:::
</details>

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
We repeat the above procedure for different train-test splits like so:

::: {.panel-tabset}
### R
Train and predict for different splits
{{< embed sol_eval_1_R.ipynb#cali-resample echo=true >}}
Compute errors
{{< embed sol_eval_1_R.ipynb#cali-error echo=true >}}
Visualize
{{< embed sol_eval_1_R.ipynb#cali-boxplot-r echo=true >}}

### Python
Train, predict and compute errors for different splits
{{< embed sol_eval_1_py.ipynb#cali-resample echo=true >}}
Visualize
{{< embed sol_eval_1_py.ipynb#cali-boxplot echo=true >}}
:::

</details> 
:::

***
Interpret the findings from the previous question.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
We can derive two conclusions:

i. A smaller training set tends to produce higher estimated generalization errors.

ii. A larger training set, at the expense of test set size, will cause high variance in the individual generalization error estimates.

</details> 
:::
