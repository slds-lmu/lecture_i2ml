---
title: "Exercise 6 -- Evaluation II"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:
  - notebook: ex_eval_2_R.ipynb
    title: "Exercise sheet for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/ex_eval_2_R.ipynb"
  - notebook: ex_eval_2_py.ipynb
    title: "Exercise sheet for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/ex_eval_2_py.ipynb"
  - notebook: sol_eval_2_R.ipynb
    title: "Solutions for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/sol_eval_2_R.ipynb"
  - notebook: sol_eval_2_py.ipynb
    title: "Solutions for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/evaluation/sol_eval_2_py.ipynb"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

<details> 
<summary>*Hint: Useful libraries*</summary>

::: {.panel-tabset}
### R
{{< embed ex_eval_2_R.ipynb#import echo=true >}}
### Python
{{< embed ex_eval_2_py.ipynb#import echo=true >}}
:::
</details>

## Exercise 1: Overfitting & underfitting

::: {.callout-note title="Learning goals" icon=false}
Discuss for given situations if they might lead to overfitting / underfitting
:::

Assume a polynomial regression model with a continuous target variable $y$, a continuous, $p$-dimensional feature vector $\xv$ and polynomials of degree $d$, i.e., 

$$
\fxi = \sum_{j=1}^p \sum_{k=0}^d \theta_{j,k} (\xi_j)^k.
$$

<!-- and $\yi = \fxi + \epsi$ where the $\epsi$ are iid with $\var(\epsi)=\sigma^2 \ \forall i\in \{1, \dots, n\}$.  -->

***
For each of the following situations, indicate whether we would generally expect the performance of a flexible polynomial learner (high $d$) to be better or worse than an inflexible one (low $d$). Justify your answer.

::: {.callout-warning icon=false title="NB"}
We can only state tendencies here; performance strongly depends on the specific data situation.
:::

***
i. The sample size $n$ is extremely large, and the number of features $p$ is small.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
We expect the flexible learner to perform better because it covers a large number of hypotheses and we have enough data to tell them apart, so there is little risk of overfitting, while the inflexible learner might underfit.
</details> 
:::

***
ii. The number of features $p$ is extremely large, and the number of observations $n$ is small.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
With a flexible learner we might quickly run into overfitting here. The data situation, which we frequently encounter in biostatistics (e.g., genomics data), creates a high-dimensional and sparsely populated input space with few training points (relatively speaking) that are easy to overfit. Therefore, a low-degree polynomial should fare better (but note that it is not immune to overfitting either in this challenging setting). Some sort of feature selection could be advisable first.
</details> 
:::

***
iii. The true relationship between the features and the response is highly non-linear.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
The inflexible learner will likely underfit here, so the flexible variant can be expected to perform better.
</details> 
:::

***
iv. The data could only be observed with a high level of noise.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
The flexible learner might interpolate between the noisy training samples, creating a wiggly curve that generalizes poorly, so we expect better performance from a less complex learner.
</details> 
:::

***
Are overfitting and underfitting properties of a learner or of a fixed model? Explain your answer.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
Overfitting and underfitting are always connected to a particular fixed *model*, even though attributes of the underlying hypothesis space typically influence the tendency toward one or the other behavior, as we have seen in the previous question. In order to understand this, think of a classification problem with linearly separable data. Applying a QDA learner, which is able to learn more complex decision boundaries, poses a risk of overfitting with the chosen model, but the degree of overfitting depends on the model itself. In theory, the QDA learner is free to set equal covariances for the Gaussian class densities, amounting to LDA and *not* overfitting the data. Under- and overfitting are therefore properties of a specific model and not of an entire learner.

A common strategy is to choose a rather flexible model class and encourage simplicity in the actual model by *regularization* (e.g., take a higher-degree polynomial but drive as many coefficients a possible toward zero, which you might know as LASSO regression).
</details> 
:::

***
Should we aim to completely avoid both overfitting and underfitting?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
That will be hardly possible. Recall how we defined the two variants of data fit:

$$
UF(\fh, L) = GE(\fh, L) - GE(f^\ast, L)
$$

$$
OF(\fh, L) = GE(\fh, L) - \riske(\fh, L)
$$

In order to avoid underfitting completely we would need to always find the universally loss-optimal model across arbitrary hypothesis spaces (the so-called *Bayes-optimal* model), which is obviously not something we can hope to achieve in general. Zero overfitting would mean to exactly balance theoretical generalization error and empirical risk, but the way empirical risk minimization is designed, our model will likely fare a bit worse on unseen test data.

In practice we will always experience these phenomena to some degree and finding a model that trades them off well is the holy grail in machine learning.
</details> 
:::


## Exercise 2: Resampling strategies

::: {.callout-note title="Learning goals" icon=false}
1. Implement resampling procedures in R/Python
2. Understand how the choice of resampling strategy affects the quality of the GE estimator
:::

***
Why would we apply resampling rather than a single holdout split?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
The two main advantages of resampling are:
  
- We are able to use larger training sets (at the expense of test set size) because the high variance this incurs for the resulting estimator is smoothed out by averaging across repetitions.

- Repeated sampling reduces the risk of getting lucky (or not so lucky) with a particular data split, which is especially relevant with few observations.
</details> 
:::

***
Classify the `german_credit` data into solvent and insolvent debtors using logistic regression. Compute the training error w.r.t. MCE.

<details> 
<summary>*Python Hint*</summary>

Read the already preprocessed file [german_credit_for_py.csv](https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/data/german_credit_for_py.csv)

</details>

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
::: {.panel-tabset}
### R
{{< embed sol_eval_2_R.ipynb#2-b-1 echo=true >}}
### Python
Read data
{{< embed sol_eval_2_py.ipynb#2-b-1 echo=true >}}
Encode data and train classifier
{{< embed sol_eval_2_py.ipynb#2-b-2 echo=true >}}
:::
</details> 
:::

***
In order to evaluate your learner, compare the test MCE using

> 1. three times ten-fold cross validation (3x10-CV)
> 2. 10x3-CV
> 3. 3x10-CV with stratification for the feature \texttt{foreign\_worker} to ensure equal representation in all folds
> 4. a single holdout split with 90\% training data

<details> 
<summary>*Hint*</summary>

::: {.panel-tabset}
### R
You will need `rsmp`, `resample` and `aggregate`.

### Python
You will need `RepeatedKFold`, `RepeatedStratifiedKFold` and `train_test_split`.

:::
</details>

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
::: {.panel-tabset}
### R
{{< embed sol_eval_2_R.ipynb#2-c-1 echo=true >}}
foo
{{< embed sol_eval_2_R.ipynb#2-c-2 echo=true >}}
### Python
{{< embed sol_eval_2_py.ipynb#2-c-1 echo=true >}}
foo
{{< embed sol_eval_2_py.ipynb#2-c-2 echo=true >}}
foo
{{< embed sol_eval_2_py.ipynb#2-c-3 echo=true >}}
foo
{{< embed sol_eval_2_py.ipynb#2-c-4 echo=true >}}
:::
</details> 
:::

***
Discuss and compare your findings and compare them to the training error computed previously.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
Generalization error estimates are pretty stable across the different resampling strategies because we have a fairly large number (1000) of observations. Still, the pessimistic bias of small training sets is visible: 10x3-CV, using roughly 67% of data for training in each split, estimates a higher generalization error than 3x10-CV with roughly 90% training data. Stratification by `foreign_worker` does not seem to have much effect on the estimate. However, we see a glaring difference when we use a single 90%-10% split, where the estimated GE is quite different from 3x10-CV, meaning we got a different error just because of an (un)lucky split.

Comparing the results (except for the unreliable one produced by a single split) with the training error from before indicates no serious overfitting. 
</details> 
:::

***
Would you consider LOO-CV to be a good alternative?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>
LOO is probably not a very good idea here -- with 1000 observations this would take a long time. Also, LOO has high variance by nature. Repeated CV with a sufficient number of folds should give us a pretty good idea about the expected GE of our learner.
</details> 
:::
