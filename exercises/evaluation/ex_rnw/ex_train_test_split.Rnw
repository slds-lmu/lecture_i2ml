We consider the \texttt{BostonHousing} data for which we would like to predict
the nitric oxides concentration (\texttt{nox}) from the distance to a number of 
firms (\texttt{dis}).

<<echo=TRUE, message=FALSE, fig.height=4, fig.width=8, warning=FALSE>>=

library(mlbench)
data(BostonHousing)
data_pollution <- data.frame(dis = BostonHousing$dis, nox = BostonHousing$nox)
data_pollution <- data_pollution[order(data_pollution$dis), ]
head(data_pollution)
@

<<echo=FALSE, message=FALSE, fig.height=4, fig.width=8, warning=FALSE>>=
ggplot2::ggplot(data_pollution, ggplot2::aes(x = dis, y = nox)) +
  ggplot2::geom_point() +
  ggplot2::theme_classic()
@

<<echo=TRUE, message=FALSE, engine="python">>=
import numpy as np
import pandas as pd

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
boston = pd.DataFrame(data[:,np.r_[4,7]], columns= ["NOX", "DIS"])
boston = boston.sort_values("DIS")
boston.reset_index(drop = True, inplace = True)
@

\begin{enumerate}[a)]

  \item Use the first ten observations as training data to compute a linear 
  model and evaluate the performance of your learner on the 
  remaining data using MSE.
  
  \item What might be disadvantageous about the train-test split in a)?
  
  \item Now, sample your training observations from the data set at random. 
  Use a share of 0.1 through 0.9, in 0.1 steps, of observations for training 
  and repeat this procedure ten times.
  Afterwards, plot the resulting test errors (in terms of MSE) in a suitable 
  manner. \\
  (\texttt{R} Hint: \texttt{rsmp} is a convenient function for splitting data -- 
  you will want to choose the "holdout" strategy. Afterwards, \texttt{resample}
  can be used to repeatedly fit the learner.)\\
  (\texttt{Python} Hint: \texttt{from sklearn.model\_selection import train\_test\_split} is a convenient function for splitting data. It has an optional parameter \texttt{random\_state}, which can be used to split the data randomly in each iteration.)
  
  \item Interpret the findings from c).
  
  % \item After this empirical experiment we take a look at the mathematical 
  % background of the bias-variance trade-off in choosing the split ratio 
  % (with no specific assumption about the kind of learner used).
  % First, consider the expected quadratic error between predictions 
  % $\hat y$ and target values $y(\xv)$:
  % $$\Exy ((\hat y - y)^2 ~|~ \xv)$$
  % and show that
  % $$\Exy ((\hat y - y(\xv))^2) = \left(\hat y - \Exy(y(\xv)) 
  % \right)^2 + \var_{xy}(y(\xv)).$$
  % 
  % \item We then go one step further, assuming repeated estimations of the 
  % generalization error (e.g., through repeated holdout splitting), and treat our 
  % prediction $\hat y$ as a 
  % random variable.
  % Usually, predictions are deterministic once we have found a model by empirical 
  % risk minimization, but now our model is itself not fixed because the training 
  % data are a random sample from the data-generating process $\Pxy$.
  % As we have seen in the experiments from exercise c), this results in 
  % a distribution of prediction values over the repetitions of our sampling 
  % procedure.
  % We therefore work with the expectation $\E_{\hat y}$ w.r.t. $\hat y$.
  % Building on the previous exercise, show that
  % $$\Exy (\E_{\hat y}((\hat y - y)^2) ~|~ \xv) =
  % \left(\E_{\hat y}(\hat y) - \Exy(y ~|~ \xv) \right)^2 +
  % \var_{\hat y}(\hat y) + \var_{xy}(y ~|~ \xv)$$
  % (note that the expectation $\E_{\hat y}$ must be placed on the outside of the 
  % quadratic expression because taking the square is not a linear transformation 
  % and, in general, $\E(a^2) \neq \E^2(a)$).
  % 
  % Identify the components that represent bias and error, respectively.
  % Can you imagine what the remaining term stands for?
  
\end{enumerate}