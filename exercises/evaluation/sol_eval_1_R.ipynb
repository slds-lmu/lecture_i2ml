{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ca41b68",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#| label: import\n",
    "\n",
    "library(ggplot2)\n",
    "library(mlr3)\n",
    "library(mlr3learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba043b79",
   "metadata": {},
   "source": [
    "## Solution 1: Evaluating regression learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f5834",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9be2a5",
   "metadata": {},
   "source": [
    "Since the polynomial learner clearly achieves a better fit for the training data and some observations lie rather far from the regression line, which is strongly penalized by $L2$ loss, it will have lower empirical risk than the linear learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d6a73b",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6718b2",
   "metadata": {},
   "source": [
    "First and foremost, evaluation on the training data is almost never a good idea. Under certain conditions the training error does tell us something about generalization ability, but for flexible learners and/or small training data it is deceptive due to optimistic bias. In this particular situation, we have few training observations and quite some points that look a little extreme. A low training error might be achieved by a learner that fits every quirk in the training data but generalizes poorly to unseen points with only slightly different distribution. Evaluation on separate test data is therefore non-negotiable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fd62c",
   "metadata": {},
   "source": [
    "### c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c38b61",
   "metadata": {},
   "source": [
    " We fit the polynomial and linear learner and then compute the squared and absolute differences between their\n",
    "respective predictions and the true target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36da4b69",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>x</th><th scope=col>y</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>10.00000</td><td>6.719762</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>10.10204</td><td>6.888379</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>10.20408</td><td>7.793217</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>10.30612</td><td>7.066415</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>10.40816</td><td>7.119966</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>10.51020</td><td>7.943824</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & x & y\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 10.00000 & 6.719762\\\\\n",
       "\t2 & 10.10204 & 6.888379\\\\\n",
       "\t3 & 10.20408 & 7.793217\\\\\n",
       "\t4 & 10.30612 & 7.066415\\\\\n",
       "\t5 & 10.40816 & 7.119966\\\\\n",
       "\t6 & 10.51020 & 7.943824\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | x &lt;dbl&gt; | y &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 10.00000 | 6.719762 |\n",
       "| 2 | 10.10204 | 6.888379 |\n",
       "| 3 | 10.20408 | 7.793217 |\n",
       "| 4 | 10.30612 | 7.066415 |\n",
       "| 5 | 10.40816 | 7.119966 |\n",
       "| 6 | 10.51020 | 7.943824 |\n",
       "\n"
      ],
      "text/plain": [
       "  x        y       \n",
       "1 10.00000 6.719762\n",
       "2 10.10204 6.888379\n",
       "3 10.20408 7.793217\n",
       "4 10.30612 7.066415\n",
       "5 10.40816 7.119966\n",
       "6 10.51020 7.943824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: legal-data\n",
    "\n",
    "set.seed(123)\n",
    "x_train <- seq(10, 15, length.out = 50)\n",
    "y_train <- 10 + 3 * sin(0.15 * pi * x_train) + rnorm(length(x_train), sd = 0.5)\n",
    "data_train <- data.frame(x = x_train, y = y_train)\n",
    "set.seed(321)\n",
    "x_test <- seq(10, 15, length.out = 10)\n",
    "y_test <- 10 + 3 * sin(0.15 * pi * x_test) + rnorm(length(x_test), sd = 0.5)\n",
    "data_test <- data.frame(x = x_test, y = y_test)\n",
    "head(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d0a74e9-0267-416e-8ab1-4f4d03828f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'MAE for Polynomial and Linear fit: 0.4140, 0.3828'"
      ],
      "text/latex": [
       "'MAE for Polynomial and Linear fit: 0.4140, 0.3828'"
      ],
      "text/markdown": [
       "'MAE for Polynomial and Linear fit: 0.4140, 0.3828'"
      ],
      "text/plain": [
       "[1] \"MAE for Polynomial and Linear fit: 0.4140, 0.3828\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "'MSE for Polynomial and Linear fit: 0.2731, 0.3032'"
      ],
      "text/latex": [
       "'MSE for Polynomial and Linear fit: 0.2731, 0.3032'"
      ],
      "text/markdown": [
       "'MSE for Polynomial and Linear fit: 0.2731, 0.3032'"
      ],
      "text/plain": [
       "[1] \"MSE for Polynomial and Linear fit: 0.2731, 0.3032\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: legal-learn\n",
    "\n",
    "# train learners\n",
    "polynomial_learner <- lm(y ~ poly(x, 21), data_train)\n",
    "linear_learner <- lm(y ~ x, data_train)\n",
    "\n",
    "# predict with both learners\n",
    "y_polynomial <- predict(polynomial_learner, data_test)\n",
    "y_lm <- predict(linear_learner, data_test)\n",
    "\n",
    "# compute errors\n",
    "abs_differences <- lapply(\n",
    "  list(y_polynomial, y_lm), \n",
    "  function(i) abs(data_test$y - i)\n",
    ")\n",
    "errors_mse <- sapply(abs_differences, function(i) mean(i^2))\n",
    "errors_mae <- sapply(abs_differences, mean)\n",
    "\n",
    "sprintf(\n",
    "    \"MAE for Polynomial and Linear fit: %.4f, %.4f\", \n",
    "    errors_mae[1], \n",
    "    errors_mae[2]\n",
    ")\n",
    "sprintf(\n",
    "    \"MSE for Polynomial and Linear fit: %.4f, %.4f\", \n",
    "    errors_mse[1], \n",
    "    errors_mse[2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd4550",
   "metadata": {},
   "source": [
    "The picture is inconclusive: based on MSE, we should prefer the complex polynomial model, while MAE tells us\n",
    "to pick the linear one. It is important to understand that the choices of inner and outer loss functions encode our\n",
    "requirements and may have substantial impact on the result. Our learners differ strongly in their complexity:\n",
    "we have an extremely flexible polynomial and a very robust (though perhaps underfitting) linear one. If, for\n",
    "example, our test data contains an extreme point far from the remaining observations, the polynomial model\n",
    "might be able to fit it fairly well, while the LM incurs a large MSE because the distance to this point enters\n",
    "quadratically. The MAE, on the other hand, is more concerned with small residuals, and there, our LM fares\n",
    "better. Here, following the MAE assessment would signify preference for a robust model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6596629",
   "metadata": {},
   "source": [
    "However, we must keep in mind that our performance evaluation is based on a single holdout split, which is\n",
    "not advisable in general and particularly deceptive with so little data. For different test data we quickly get\n",
    "in situations where the polynomial has both worse MSE and MAE – after all, slapping a learner with 21 + 1\n",
    "learnable parameters on a 50-points data set should strike you as a rather bad idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c8a3b",
   "metadata": {},
   "source": [
    "Take-home message: the choice of our performance metric matters, and making decisions based on a single\n",
    "train-test split is risky in many data settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f83b674",
   "metadata": {},
   "source": [
    "## Solution 2: Importance of train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c489ac56",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3445b",
   "metadata": {},
   "source": [
    "Get the data, define a task and corresponding train-test split, and predict with trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a292023f-0c1c-452a-9481-ff0df433fa84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 2</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>MedInc</th><th scope=col>MedValue</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>8.3252</td><td>4.526</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>8.3014</td><td>3.585</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>7.2574</td><td>3.521</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>5.6431</td><td>3.413</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>3.8462</td><td>3.422</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>4.0368</td><td>2.697</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 2\n",
       "\\begin{tabular}{r|ll}\n",
       "  & MedInc & MedValue\\\\\n",
       "  & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 8.3252 & 4.526\\\\\n",
       "\t2 & 8.3014 & 3.585\\\\\n",
       "\t3 & 7.2574 & 3.521\\\\\n",
       "\t4 & 5.6431 & 3.413\\\\\n",
       "\t5 & 3.8462 & 3.422\\\\\n",
       "\t6 & 4.0368 & 2.697\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 2\n",
       "\n",
       "| <!--/--> | MedInc &lt;dbl&gt; | MedValue &lt;dbl&gt; |\n",
       "|---|---|---|\n",
       "| 1 | 8.3252 | 4.526 |\n",
       "| 2 | 8.3014 | 3.585 |\n",
       "| 3 | 7.2574 | 3.521 |\n",
       "| 4 | 5.6431 | 3.413 |\n",
       "| 5 | 3.8462 | 3.422 |\n",
       "| 6 | 4.0368 | 2.697 |\n",
       "\n"
      ],
      "text/plain": [
       "  MedInc MedValue\n",
       "1 8.3252 4.526   \n",
       "2 8.3014 3.585   \n",
       "3 7.2574 3.521   \n",
       "4 5.6431 3.413   \n",
       "5 3.8462 3.422   \n",
       "6 4.0368 2.697   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: cali-data\n",
    "# Adapt to version available on sklearn following description from \n",
    "# https://gist.github.com/bgreenwell/b1330460eec5acf1c81fae71902e331c\n",
    "\n",
    "setwd(tempdir())\n",
    "url <- \"https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.tgz\"\n",
    "download.file(url, destfile = \"cal.tar.gz\")\n",
    "untar(\"cal.tar.gz\")\n",
    "\n",
    "# Read the data into R and provide column names\n",
    "df_california <- read.csv(\"CaliforniaHousing//cal_housing.data\", header = FALSE)\n",
    "columns_index <- c(9, 8, 3, 4, 5, 6, 7, 2, 1)\n",
    "df_california <- df_california[, columns_index]\n",
    "names(df_california) <- c(\n",
    "    \"MedValue\", \n",
    "    \"MedInc\", \n",
    "    \"HouseAge\", \n",
    "    \"AveRooms\",\n",
    "    \"AveBedrms\", \n",
    "    \"Population\", \n",
    "    \"AveOccup\", \n",
    "    \"Latitude\",\n",
    "    \"Longitude\"\n",
    ")\n",
    "df_california$MedValue <- df_california$MedValue / 100000\n",
    "df_california <- df_california[, c(\"MedInc\", \"MedValue\")]\n",
    "head(df_california)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6c298c2-ba7f-49cb-baff-7800ce27e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: cali-split\n",
    "\n",
    "df_california_sorted <- df_california[order(df_california$MedInc), ]\n",
    "task <- TaskRegr$new(\n",
    "    \"housing\", backend = df_california_sorted, target = \"MedValue\"\n",
    ")\n",
    "train_set = 1:100\n",
    "test_set = setdiff(seq_len(task$nrow), train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52622e78-a8d7-40dd-9de4-4d491df16472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'MSE of test data 4.1522:'"
      ],
      "text/latex": [
       "'MSE of test data 4.1522:'"
      ],
      "text/markdown": [
       "'MSE of test data 4.1522:'"
      ],
      "text/plain": [
       "[1] \"MSE of test data 4.1522:\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: cali-lm\n",
    "\n",
    "# train linear learner\n",
    "learner <- lrn(\"regr.lm\")\n",
    "learner$train(task, row_ids = train_set)\n",
    "\n",
    "# predict on test data\n",
    "predictions <- learner$predict(task, row_ids = test_set)\n",
    "sprintf(\"MSE of test data %.4f:\", predictions$score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67aaff5",
   "metadata": {},
   "source": [
    "### b) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85692d",
   "metadata": {},
   "source": [
    "We have chosen the first few observations from a data set that is ordered by feature value, which is not a good idea because this is not a random sample and covers only a particular area of the feature space. Consequently, we obtain a pretty high test MSE (relatively speaking -- we will see in the next exercise which error values we can usually expect for this task). Looking at the data, this gives us a steeply declining regression line that does not reflect the overall data situation. Also, the training set is pretty small and will likely lead to poor generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a122d",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb905b9",
   "metadata": {},
   "source": [
    "We repeat the above procedure for different train-test splits like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "840963c4-ba10-44a1-9ab6-4f49405efd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: cali-resample\n",
    "\n",
    "# define train-test splits\n",
    "repetitions <- 1:10\n",
    "split_ratios <- seq(0.1, 0.9, by = 0.1)\n",
    "\n",
    "# create resampling objects with holdout strategy, using lapply for efficient computation\n",
    "split_strategies <- lapply(split_ratios, function(i) rsmp(\"holdout\", ratio = i))\n",
    "\n",
    "# train linear learners and predict in one step (remember to set a seed)\n",
    "set.seed(123)\n",
    "results <- list()\n",
    "lgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n",
    "for (i in repetitions) {\n",
    "    results[[i]] <- lapply(\n",
    "        split_strategies, function(i) mlr3::resample(task, learner, i)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43d0567e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#| label: cali-error\n",
    "\n",
    "errors <- lapply(\n",
    "  repetitions,\n",
    "  function(i) sapply(results[[i]], function(j) j$score()$regr.mse)\n",
    ")\n",
    "\n",
    "# assemble everything in data.frame and convert to long format for plotting\n",
    "errors_df <- as.data.frame(do.call(cbind, errors))\n",
    "errors_df$split_ratios <- split_ratios\n",
    "errors_df_long <- reshape2::melt(errors_df, id.vars = \"split_ratios\")\n",
    "names(errors_df_long)[2:3] <- c(\"repetition\", \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68731c8d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAC/VBMVEUAAAABAQECAgIDAwME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\nIICQAAGEBAggJEAAIQEC/g8t0F/bgiaMZwAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| label: cali-boxplot-r\n",
    "\n",
    "# plot errors vs split ratio\n",
    "ggplot(errors_df_long, aes(x = as.factor(split_ratios), y = mse)) +\n",
    "  geom_boxplot(fill = \"lightgray\") +\n",
    "  theme_classic() +\n",
    "  labs(x = \"share of training samples\", y = \"average MSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
