---
title: "Exercise 11 -- Tuning"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:
  - notebook: ex_tuning_R.ipynb
    title: "Exercise sheet for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/tuning/ex_tuning_R.ipynb"
  - notebook: ex_tuning_py.ipynb
    title: "Exercise sheet for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/tuning/ex_tuning_py.ipynb"
  - notebook: sol_tuning_R.ipynb
    title: "Solutions for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/tuning/sol_tuning_R.ipynb"
#   - notebook: sol_tuning_py.ipynb
#     title: "Solutions for Python"
#     url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/tuning/sol_tuning_py.ipynb"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

<details> 
<summary>*Hint: Useful libraries for this exercise sheet*</summary>

::: {.panel-tabset}

### R
{{< embed ex_tuning_R.ipynb#import echo=true >}}
### Python
{{< embed ex_tuning_py.ipynb#import echo=true >}}
:::
</details>

## Exercise 1: Random search

::: {.callout-note title="Learning goals" icon=false}
1. Learn to write pseudo-code
1. Implement basic tuning procedure
:::

Random search (RS) is a simple yet effective tuning procedure.
We will implement RS from scratch to find the optimal degree $d \in \N$ in a polynomial regression problem.

Consider the following skeleton of pseudo-code:

:::{.callout-tip title="Algorithm: Random Search" icon=false}

***

***Requires*** ...

< main body >

***Returns*** ...
:::

***

What should this algorithm return as a result?

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>

The minimum required output will be the optimal degree $d^\ast$.
(Additional info might enhance user experience in a real-world implementation.)
</details>
:::

*** 
What should be the required user input? Add the inputs to the pseudo-code.

*Hint:* Use a single hold-out split in evaluation.

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>

We need (at least) the following input:

- A search space for $d$
- The number of RS trials (budget)
- Data to train and evaluate the learner on + the proportion to use as training 
data (aternatively, we might require separate training and test datasets)
- An evaluation criterion

Let's add that to the pseudo-code:

:::{.callout-tip title="Algorithm: Random Search" icon=false}

***

***Requires*** search space $\tilde \Lambda \subset \N$, budget $B \in \N$, dataset $\D \in \defAllDatasetsn$, train set proportion $s \in (0, 1)$, evaluation criterion $\rho$

< main body >

***Returns*** $d^\ast \in \N$
:::

</details> 
:::

*** 
Start to implement the main body by 

- defining elements that allow you to store 
the currently optimal candidate,
- performing a holdout split on the data, and
- setting up a loop for evaluation of each 
candidate.

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>

**Tracking optimal candidates**

- We'll need a variable that is set to the optimal degree in any given iteration, to be updated when a candidate performs better than previous ones.
- Likewise, we'll need to store the estimated generalization error associated with the optimal candidate so we can compare it to new candidates (we'll assume that smaller $\rho$ means lower GE)

**Holdout split**

Simple one-liner.

**Loop**

- The first thing to do is defining the set of candidate values, which we'll obtain by drawing as many random samples from the search space as our budget allows.
- Afterwards, we can run a `for` loop over this candidate set.

There's no official pseudo-code language--you should phrase your code such that a human with knowledge in any suitable programming language can unambiguously translate the pseudo-code into that programming language.

:::{.callout-tip title="Algorithm: Random Search" icon=false}

***

***Requires*** search space $\tilde \Lambda \subset \N$, budget $B \in \N$, dataset $\D \in \defAllDatasetsn$, train set proportion $s \in (0, 1)$, evaluation criterion $\rho$

| `set_seed` [// crucial to make the procedure reproducible]{style="color:gray;"}
| $d^\ast \leftarrow 0$ [// any starting value okay]{style="color:gray;"}
| $\text{GE}^\ast \leftarrow \infty$ [// ensures that first candidate is optimal until a better one comes along]{style="color:gray;"}
| `candidates` $\leftarrow$ `sample_uniform`(`from` = $\tilde \Lambda$, `number` = $B$, `replacement` = False)
| $\Dtrain$, $\Dtest$ $\leftarrow$ `split_holdout`(`data` = $\D$, `train_proportion` = $s$)
| **for** $d$ in `candidates` **do**
|   ...
| end **for**

***Returns*** $d^\ast \in \N$
:::

</details> 
:::

*** 
Finalize the pseudo-code by adding steps for training & evaluation and a rule to update the optimal candidate.

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>

:::{.callout-tip title="Algorithm: Random Search" icon=false}

***

***Requires*** search space $\tilde \Lambda \subset \N$, budget $B \in \N$, dataset $\D \in \defAllDatasetsn$, train set proportion $s \in (0, 1)$, evaluation criterion $\rho$

| `set_seed` [// crucial to make the procedure reproducible]{style="color:gray;"}
| $d^\ast \leftarrow 0$ [// any starting value okay]{style="color:gray;"}
| $\text{GE}^\ast \leftarrow \infty$ [// ensures that first candidate is optimal until a better one comes along]{style="color:gray;"}
| `candidates` $\leftarrow$ `sample_uniform`(`from` = $\tilde \Lambda$, `number` = $B$)
| $\Dtrain$, $\Dtest$ $\leftarrow$ `split_holdout`(`data` = $\D$, `train_proportion` = $s$, `replacement` = False)
| **for** $d$ in `candidates` **do**
|     `learner` $\leftarrow$ `polynomial_regression`(`degree` = $d$)
|     `train`(`learner` = `learner`, `data` = $\Dtrain$)
|     $\text{GE}$ $\leftarrow$ `evaluate`(`learner` = `learner`, `data` = $\Dtest$, `criterion` = $\rho$)
|     **if** $\text{GE}$ < $\text{GE}^\ast$ **do**
|       $d^\ast \leftarrow d$
|       $\text{GE}^\ast \leftarrow \text{GE}$
|     end **if**
| end **for**

***Returns*** $d^\ast \in \N$
:::

</details> 
:::

*** 
Describe how you could implement a more flexible resampling strategy.

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
A more general way to define the user input might be to accept sets of training and test datasets, respectively.
We could then add a `for` loop over those to compute the GE associated with each candidate $d$. 
</details> 
:::

## Exercise 2: Basic tuning techniques

::: {.callout-note title="Learning goals" icon=false}
Understand difference between grid and random search from different perspectives
:::

Explain the difference in implementation between random search (RS) and grid search (GS).

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
- The main difference lies in how we obtain the candidate values: RS samples them uniformly from the search space, while GS creates a multi-dimensional grid from the search space and tries all configurations in it.
- In the algorithm from Exercise 1, we'd thus need to adjust the `candidates` $\leftarrow \dots$ part (and slightly adapt the user input).

</details> 
:::

***

Consider the following objective function $c(\lambda)$ with a single hyperparameter $\lambda \in \R$.
The objective is to be minimized. 
Explain whether RS or GS will be more suited to find the optimal value $\lambda^\ast$, given

- a search space of $\lambda \in [0, 50]$, and
- a budget of 6 evaluations.

```{r}
#| echo: false
#| fig.height: 4
#| fig.width: 6
library(ggplot2)
set.seed(123)
x = runif(1000, 0, 50)
y = sin(1 / pi * x + pi)
p = ggplot(data.frame(x = x, y = y), aes(x, y)) +
  geom_line() +
  labs(x = expression(lambda), y = expression(c(lambda))) +
  theme_minimal()
print(p)
```

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
- In this (stylized) example, the discretization of GS is quite harmful: Choosing the grid as the lower and upper bounds of the search space plus 4 equidistant values within, as is common, will prevent us from ever exploring promising values for $\lambda$.
```{r}
#| echo: false
#| fig.height: 4
#| fig.width: 6
candidates = seq(0, 50, length.out = 6)
for (i in candidates) {
  p = p + geom_vline(xintercept = i, color = "blue")
}
print(p)
```
- With RS, every value in $[0, 50]$ is equally likely to be tried, so we have at least a chance to find one of the good values $\rightsquigarrow$ RS is preferable here.
</details> 
:::

***

Consider now a bivariate objective function $c(\lambda_1, \lambda_2)$ with $\lambda_1, \lambda_2 \in \R$.
The objective is to be minimized. 
Suppose that $\lambda_2$ is vastly more important for the objective than $\lambda_1$.

Visualize the number of different values of $\lambda_2$ that RS and (exhaustive) GS are expected to explore for a budget $B$ of 9, 25, 100, 400, 2500 evaluations.


::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>

```{r}
#| echo: false
#| fig.height: 4
#| fig.width: 6
library(ggplot2)
x = c(9, 25, 100, 400, 2500)
evals = c(sqrt(x), x)
algorithm = c(rep("GS", length(x)), rep("RS", length(x)))
p = ggplot(data.frame(x, evals, algorithm), aes(x = x, y = evals, col = algorithm)) +
  geom_line() +
  geom_point() +
  labs(x = "budget", y = expression(paste("distinct values of ", lambda[2]))) +
  theme_minimal()
print(p)
```

- Since GS comes with discretization, distributing the budget across both hyperparameter means that $\lambda_1$ receives $\sqrt{B}$ candidate values even though it doesn't matter for the objective. Consequently, we only have $\sqrt{B}$ values for $\lambda_2$ left to explore.
- RS, on the other hand, will (in expectation) evaluate $B$ different values of $\lambda_2$ for every budget, making it much more efficient in this situation. 
</details>
:::


## Exercise 3: Interpreting tuning results

::: {.callout-note title="Learning goals" icon=false}
1. Implement tuning procedure
1. Interpret effect of hyperparameters
:::

In this exercise we will perform hyperparameter optimization (HPO) for the task of classifying the `credit risk` data with a $k$-NN classifier.
We use `mlr3` ([tuning chapter in the book](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html)); see [Jupyter notebook](https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/tuning/sol_tuning_py.ipynb) for a similar case study in Python.

The `kknn` implementation used by `mlr3` contains several hyperparameters, three of which are to be tuned for our prediction:

- $k$ (number of neighbors)
- `kernel`
- `scale`

<details>
<summary>*Details about the hyperparameters*</summary>

- $k$: determines the size of the neighborhood and thus influences the 
    locality of the model. 
    Smaller neighborhoods reflect the belief that only very similar (close) 
    neighbors should be allowed to weigh into the prediction of a new 
    observation, and predictions may change strongly for only small changes of 
    the input variables. 
    If $k$ is chosen too small, we may encounter overfitting.
    Conversely, larger neighborhoods produce a more global model with larger 
    parts of the input space receiving the same prediction.
    Setting $k$ too large may result in underfitting.
- `kernel`: determines the importance weights in the $k$-neighborhood.

	![](figure/kernels.png)

	[figure source](https://en.wikipedia.org/wiki/Kernel_%28statistics%29#/media/File:Kernels.svg)
	
	Can you guess which kernel corresponds to unweighted $k$-NN?
    
- `scale` (logical): defines whether variables should be normalized to 
    equal standard deviation. 
    This is often reasonable to avoid implicit importance weighting through 
    different natural scales (for example, recall that neighborhoods in a 
    bivariate feature space are circular for quadratic distance -- scaling
    either dimension will change which observations end up in the neighborhood).
</details>

***

You receive the following `mlr3` output from tuning $k$-NN with random search.
Interpret the impact of each hyperparemeter on the objective.

{{< embed sol_tuning_R.ipynb#knn-viz echo=false >}}

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>

We plot the classification performance in terms of
AUC across different choices of each hyperparameter.
Let's look at each one in turn:

- Increasing $k$ initially leads to an improvement that plateaus after around 
  50 neighbors. However, there seem to be two quite distinct groups of candidate
  values.
- Scaling the variables boosts performance quite substantially.
- The choice of the kernel does not seem to have much 
  impact. Again, we see two clusters of candidate values.
  
Obviously, the interpretability of these plots is limited: we only see 
*marginal* effects of individual hyperparameters.
The fact that they really interact with each other contributes substantially 
to the difficulty of the tuning problem.
We can clearly see this in the plot for $k$ and `kernel`, where we have two quite distinct 
patterns corresponding to different values of `scale`.

</details>
:::

***

Now let's look at the code that generated the above results.
Start by defining the `german_credit` task, where you reserve 800 observations for training, and the `kknn` learner (the learner should output probabilities).

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
{{< embed sol_tuning_R.ipynb#knn-task echo=true >}}
</details>
:::

***

Set up the search space to tune over using the `ps` function.
Include choices for $k \in \{1, 2, \dots, 100\}$, `scale` $\in \{\text{yes}, \text{no}\}$, and `kernel` $\in \{\text{rectangular}, \text{epanechnikov}, \text{gaussian}, \text{optimal}\}$.

::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
{{< embed sol_tuning_R.ipynb#knn-sspace echo=true >}}
</details>
:::

***

Define the stopping criterion for random search with a so-called *terminator* (`trm`). We want the tuning procedure to finish after 200 evaluations or a maximum runtime of 30 seconds.

*Hint:* You can define this combinded terminator via a list of individual terminators.


::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
{{< embed sol_tuning_R.ipynb#knn-terminator echo=true >}}
</details>
:::

***

Set up a tuning instance using the function `ti`. This object combines all of the above components. Set the evaluation criterion to AUC.


::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
{{< embed sol_tuning_R.ipynb#knn-instance echo=true >}}
</details>
:::

***

Finally, define the tuner (`tnr`) of type "random_search" and run the optimization. Don't forget to make your code reproducible.


::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
Optimization
{{< embed sol_tuning_R.ipynb#knn-run echo=true >}}

Visualization
{{< embed sol_tuning_R.ipynb#knn-viz echo=true >}}
</details>
:::

***

With the hyperparameter configuration found via HPO, fit the model on all training observations and compute the AUC on your test data.


::: {.content-visible when-profile="solution"}
<details>
<summary>**Solution**</summary>
{{< embed sol_tuning_R.ipynb#knn-final echo=true >}}
</details>
:::

<!-- ## Exercise 1: Tuning $k$-NN -->

<!-- ::: {.callout-note title="Learning goals" icon=false} -->
<!-- Implement actual tuning procedure -->
<!-- ::: -->

<!-- In this exercise we will perform hyperparameter optimization (HPO) for the task of classifying the \texttt{credit risk} data with a $k$-NN classifier. -->

<!-- ::: {.panel-tabset} -->

<!-- ### R Exercise -->

<!-- The `kknn` implementation used by `mlr3` contains several hyperparameters, three of which are to be tuned for our prediction: -->

<!-- - $k$ (number of neighbors) -->
<!-- - `kernel` -->
<!-- - `scale` -->

<!-- *** -->
<!-- Describe briefly the role of each hyperparameter in the learning algorithm - which effects can be expected by altering them? -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->
<!-- Role of hyperparameters -->

<!-- - $k$: determines the size of the neighborhood and thus influences the locality of the model. Smaller neighborhoods reflect the belief that only very similar (close) neighbors should be allowed to weigh into the prediction of a new observation, and predictions may change strongly for only small changes of the input variables. If $k$ is chosen too small, we may encounter overfitting. Conversely, larger neighborhoods produce a more global model with larger parts of the input space receiving the same prediction. Setting $k$ too large may result in underfitting. -->

<!-- - `kernel`: corresponds to the choice of similarity metric. It determines the importance weights in the $k$-neighborhood.  -->

<!--   ![[https://en.wikipedia.org/wiki/Kernel_%28statistics%29#/media/File:Kernels.svg](https://en.wikipedia.org/wiki/Kernel_%28statistics%29#/media/File:Kernels.svg)](figure/kernels.png){width=50% fig-align="center"} -->

<!--   Some of the more widely applied kernels include: -->

<!--   - Rectangular/uniform: all observations within the support of the kernel receive the same degree of similarity, corresponding to unweighted $k$-NN. -->

<!--   - Triangle: weight is a linear function of distance within the kernel support. -->

<!--   - Epanechnikov: weight is a quadratic function of distance within the kernel support. -->

<!--   - Gaussian: weight is a quadratic function of distance. The Gaussian kernel has infinite support, meaning that each pair of observations, no matter how far apart, receives a positive degree of similarity. -->

<!--   Kernels have a number of canonical properties and  play an important role in statistics (most distance-based operations, such as smoothing, can be expressed using kernels). -->

<!-- - `scale` (logical): defines whether variables should be normalized to equal standard deviation. This is often reasonable to avoid implicit importance weighting through different natural scales (for example, recall that neighborhoods in a bivariate feature space are circular for quadratic distance -- scaling either dimension will change which observations end up in the neighborhood). -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- In `mlr3` (using the `mlr3tuning` library), define an appropriate search space to tune over. We want to explore a range between 1 and 100 for $k$ and the kernel to be chosen from "rectangular", "epanechnikov", "gaussian", "optimal". -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Each learner has a slot `param_set` containing all hyperparameters that can be set during tuning. The key function to define the search space is `ps` (short for parameter space). Furthermore, we need to instantiate every (hyper)parameter with an appropriate type - e.g., characterizing $k$ as integer ensures that only valid neighborhood sizes are queried. `scale` is logical (i.e., binary) and `kernel` is encoded as a factor with levels defining the choices to tune over: -->

<!-- {{< embed sol_tuning_R.ipynb#1-b echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- Perform the tuning procedure using `TuningInstanceSingleCrit`. Set aside 200 test observations first. Use 5-fold cross validation and random search, and terminate the process after either 30 seconds or 200 evaluations. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- We implement the HPO as follows: -->

<!-- {{< embed sol_tuning_R.ipynb#1-c-1 echo=true >}} -->

<!-- Then, press play: -->

<!-- {{< embed sol_tuning_R.ipynb#1-c-2 echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- You realize that a high AUC is the performance measure you are actually interested in. Modify the HPO procedure such that performance is optimized w.r.t. AUC. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Plug in custom performance metric (AUC needs the learner to output probabilities): -->

<!-- {{< embed sol_tuning_R.ipynb#1-d echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- Visualize the tuning result with a suitable command. What do you observe regarding the impact of different hyperparameters on predictive performance? What are limits of such a form of analysis? -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- We can make convenient use of the autoplot functions in `mlr3`: -->

<!-- {{< embed sol_tuning_R.ipynb#1-e echo=true >}} -->

<!-- Increasing $k$ initially leads to an improvement that plateaus after around 50 neighbors. Scaling the variables also boosts performance. The choice of the kernel, on the other hand, does not seem to have much impact. -->

<!-- Obviously, the interpretability of these plots is limited: we only see *marginal* effects of individual hyperparameters. The fact that they really interact with each other contributes substantially to the difficulty of the tuning problem. We can clearly see this in the plot for $k$, where we have two quite distinct patterns corresponding to different values of `scale`. -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- After analyzing the tuning results, you notice that changes in $k$ are more influential for smaller neighborhoods. Re-run the HPO procedure with a log-transformation for $k$. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Tuning $k$ on log-scale requires a modification of the search space. For this, we need to transform the boundaries of the search interval and undo the transformation afterwards via `exp` (plus a rounding operation to ensure $k$ remains an integer number): -->

<!-- {{< embed sol_tuning_R.ipynb#1-f echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- With the hyperparameter configuration found via HPO, fit the model on all training observations and compute the AUC on your test data. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Performance on test data: -->

<!-- {{< embed sol_tuning_R.ipynb#1-g echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->


<!-- ### Python Exercise -->

<!-- The `KNeighborsClassifier` implementation used by `sklearn` contains several hyperparameters, three of which are to be tuned for our prediction: -->

<!-- - `n_neighbors` -->
<!-- - `weigths` -->
<!-- - `metric` -->

<!-- *** -->
<!-- Describe briefly the role of each hyperparameter in the learning algorithm -- which effects can be expected by altering them? -->

<!-- Furthermore, read the [credit_for_py.csv](https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/data/german_credit_for_py.csv), separate 138 test observations and perform necessary preprocessing steps. -->

<!-- <details>  -->
<!-- <summary>*Hint*</summary> -->

<!-- Apply a `StandardScaler` on your feature space. What effect does scaling have on your $k$-NN-model? -->

<!-- </details> -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Role of hyperparameters: -->

<!-- - `n_neighbors`: determines the size of the neighborhood and thus influences the locality of the model. Smaller neighborhoods reflect the belief that only very similar (close) neighbors should be allowed to weigh into the prediction of a new observation, and predictions may change strongly for only small changes of the input variables. If `n_neighbors` is chosen too small, we may encounter overfitting. Conversely, larger neighborhoods produce a more global model with larger parts of the input space receiving the same prediction. Setting `n_neighbors` too large may result in underfitting. -->

<!-- - `weights`: It determines the importance weights in the k-neighborhood. Possible values are: -->
<!--     - ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally. -->
<!--     - ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. This setting is strongly dependent on the parameter `metric` -->
<!--     - [`callable`] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. -->

<!-- - `metric`: Metric to use for distance computation. Some of the more widely applied metric include: -->
<!--      - `euclidean`: distance is computed by the L2-norm -->
<!--      - `manhattan`: distance is computed by the L1-norm -->
<!--      - `cosine`: distance is computed by $1-\frac{u \cdot v}{\|u\|_2 \|v\|_2}$ -->
<!--      - `minkowski`: given the parameter p the distance is computed by $\|u - v\|_p$ -->

<!-- Load the data [credit_for_py.csv](https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/data/credit_for_py.csv) and seperate into train and test set: -->

<!-- {{< embed sol_tuning_py.ipynb#1-a echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- Define an appropriate search space to tune over. We want to explore a range between 1 and 100 for `n_neighbors` and the distance calculation to be chosen from `uniform`, `manhattan`, `euclidean`, `cosine`. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Each instance of RandomizedSearchCV has a slot *param_distributions* containing all hyperparameters that can be set during tuning. It has to be passed as a dictionary or a list of dictionaries. Note that single values also have to be passed as a list to the dictionary. -->

<!-- Furthermore, we need to consider valid (hyper)parameter tuples in the design of the hyperparameter space. -->

<!-- Due to the **duck typing** behaviour of Python, we do not need to specify the type of our hyperparameters (type or the class of an object is less important than the methods it defines): -->

<!-- {{< embed sol_tuning_py.ipynb#1-b echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- Perform the tuning procedure using `RandomizedSearchCV`. Use 5-fold cross validation, and terminate the process after 200 iterations. Also, utilize parallelization to fasten the computation. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- We implement the HPO as follows: -->

<!-- {{< embed sol_tuning_py.ipynb#1-c echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- You realize that a high AUC is the performance measure you are actually interested in. Modify the HPO procedure such that performance is optimized w.r.t. AUC. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- {{< embed sol_tuning_py.ipynb#1-d echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- You are interested in possible under- and overfitting of your hyperparameter setting. Use `validation_curve` from `sklearn.model_selection` to retrieve the training scores and cross-validation scores for a 5-fold-CV, depending on the AUC metric. -->

<!-- Visualize the tuning result with a suitable command. You may use the function provided below or a self-defined function. -->

<!-- Re-run the evaluation with unscaled features. -->

<!-- What do you observe regarding the impact of different hyperparameters and scaling on  -->
<!-- predictive performance? -->

<!-- What are limits of such a form of analysis?  -->

<!-- <details>  -->
<!-- <summary>*Function for plotting a validation curve*</summary> -->

<!-- {{< embed ex_tuning_py.ipynb#hint echo=true >}} -->

<!-- </details> -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Plot for scaled training data: -->

<!-- {{< embed sol_tuning_py.ipynb#1-e-1 echo=true >}} -->

<!-- Plot for unscaled training data: -->

<!-- {{< embed sol_tuning_py.ipynb#1-e-2 echo=true >}} -->

<!-- Increasing `n_neighbors` initially leads to an improvement that plateaus after around 50 neighbors. Scaling the variables also boosts performance. The choice of the metric, on the other hand, does not seem to have much impact. Note that the default metric compared with the uniform distribution is the euclidian one. -->

<!-- Obviously, the interpretability of these plots is limited: we only see marginal effects of individual hyperparameters. The fact that they really interact with each other contributes substantially to the difficulty of the tuning problem. We can clearly see this in the plots for k, where we have two quite distinct patterns corresponding to different values of scale. -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- After analyzing the tuning results, you notice that changes in `n_neighbors` are more influential for smaller neighborhoods. Re-run the HPO procedure with a log-transformation for `n_neighbors` parameter list. -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- {{< embed sol_tuning_py.ipynb#1-f-1 echo=true >}} -->
<!-- {{< embed sol_tuning_py.ipynb#1-f-2 echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- *** -->
<!-- With the hyperparameter configuration found via HPO, fit the model on all training observations and compute the AUC on your test data.  -->
<!-- Could you see any effect of the log-transformation for `n_neighbors`? -->

<!-- ::: {.content-visible when-profile="solution"} -->
<!-- <details>  -->
<!-- <summary>**Solution**</summary> -->

<!-- Performance on test data: -->

<!-- {{< embed sol_tuning_py.ipynb#1-g-1 echo=true >}} -->

<!-- As can be observed, the log-transformation does not have a high effect on the tuning process. -->

<!-- {{< embed sol_tuning_py.ipynb#1-g-2 echo=true >}} -->

<!-- </details>  -->
<!-- ::: -->

<!-- ::: -->
