
%
Assume we have a set 
$$
\D = \big\{ \, (\xi , \yi) \, \big\}_{i=1}^N \, \in \, ( \Xspace \times \Yspace)^N
$$
of training examples $(\xi , \yi) \in \Xspace \times \Yspace,$ where $\Xspace = \mathbb{R}^d$ and $\Yspace = \{-1,+1\}.$  
%
Recall that in logistic regression the probability $	p(y=  +1\,\vert\,\xv)$ is modeled as
%
\begin{align*}
	%	
	\pi_{\thetab}: \ \Xspace & \to [0,1] \\
	%	
	\xv  &\mapsto \frac{1}{1 + \exp \big(-  \langle \thetab ,  \xv  \rangle \big) } \, ,
	%	
\end{align*}
%
with $\thetab = (\theta_1, \ldots , \theta_d)^\top \in \mathbb{R}^d$ is a parameter vector.

Assume we have a partition of $\Xspace$ into $G\in \mathbb{N}$ groups\footnote{Here, we mean disjoint subsets of $\mathbb{R}^d$ whose union is $\Xspace = \mathbb{R}^d.$}, say $\Xspace_1,\ldots,\Xspace_G.$
%
Consider the following quantity
%
\begin{align*}
	%	
	H_{\thetab}(G) = \sum_{g=1}^G \frac{( O_{g,+1} - E_{g,+1|{\thetab}})^2 }{E_{g,+1|{\thetab}}} + \frac{( O_{g,-1} - E_{g,-1|{\thetab}})^2 }{E_{g,-1|{\thetab}}},
	%	
\end{align*}
%
where $O_{g,\pm1}$ is the number of \emph{observed} $y's$ which are $\pm 1$ and the corresponding $\xv$ is an element of $\Xspace_g,$ and $E_{g,\pm 1|{\thetab}}$ is the number of \emph{expected} $y's$ which are $\pm 1$ under the model $\pi_{\thetab}$ and the corresponding $\xv$ is an element of $\Xspace_g.$
% 
\begin{itemize}
	%	
	\item [(a)] Give a mathematical definition of $ O_{g,+1},$ $ O_{g,-1},$ $E_{g,+1|{\thetab}}$ and $E_{g,-1|{\thetab}}.$
	
	\textbf{Solution:} %			
	Recall the interpretation of $\pi_{\thetab}(\xv):$ It is giving the (fitted) probability that $y=+1$ for given $\xv.$
	%			
	In other words, we expect that $y=+1$ given $\xv$ with probability $\pi_{\thetab}(\xv).$ 
	%			
	Similarly, we expect that $y=-1$ given $\xv$ with probability $(1-\pi_{\thetab}(\xv)).$ 
	%			
	With this, we can write the quantities as follows:
	%
	\begin{align*}
		%				
		O_{g,+1} &= \sum_{i=1}^N   \mathds{1}_{[ \yi = +1  ]} \mathds{1}_{[ \xi \in \Xspace_g  ]}, \\
		%				
		O_{g,-1} &= \sum_{i=1}^N   \mathds{1}_{[ \yi = -1  ]} \mathds{1}_{[ \xi \in \Xspace_g  ]}, \\
		%								
		E_{g,+1|{\thetab}} &= \sum_{i=1}^N \pi_{\thetab}(\xi) \mathds{1}_{[ \xi \in \Xspace_g  ]},\\
		%								
		E_{g,-1|{\thetab}} &= \sum_{i=1}^N (1-\pi_{\thetab}(\xi)) \mathds{1}_{[ \xi \in \Xspace_g  ]}.
		%				
	\end{align*}
	%			
	%	
	\item [(b)] If the model $\pi_{\thetab}$ is (approximately) well-calibrated, what values should $H_{\thetab}(G)$ take?  What is a desirable property of the partition $\Xspace_1,\ldots,\Xspace_G$ of $\Xspace?$
	
	\textbf{Solution:}
	
	%			
	If the model is approximately well-calibrated, then it should hold for any $s\in[0,1]$
	%			
	$$ 	\P(y =  +1\,\vert\,  \pi_{\thetab}(\xv) = s   ) \approx s, \quad \forall  \xv\in \Xspace.$$
	%			
	In words, if the logistic model predicts that $y =  +1$ will occur with probability $s = \pi_{\thetab}(\xv),$ then $y =  +1$ should occur with probability (approximately) $s.$
	%
	In particular, the frequency with which $y =  +1$ is observed for some particular $\xv$ should match the expected frequency under $\pi_{\thetab}(\xv).$
	
	Thus, we would expect that $ O_{g,+1} \approx E_{g,+1|{\thetab}}$ and $ O_{g,-1} \approx E_{g,-1|{\thetab}}$ for every group $g \in\{1,\ldots,G\},$ if the model $\pi_{\thetab}$ is approximately well-calibrated.
	%			
	Hence, $H_{\thetab}(G)$ should be close to 0 or not ``too large''.
	%			
	
	However, as we rarely observe the same exact feature vector $\xv$ we could group some of them together.
	%			
	If we assume that the probability of occurrence for $y =  +1$ is similar if $\xv$ and $\xv'$ are close (with respect to some metric on $\mathbb{R}^d$), then it would be sensible to use a group $ \Xspace_g$ which is a connected subset of $\mathbb{R}^d,$ e.g., a hypercube.
	%			
	Moreover, it would be desirable if the number of $\xi$'s in $ \Xspace_g$ is roughly the same among all groups.
	%			
	This would make sure that the frequencies considered by $ O_{g,+1}$ (or $ O_{g,-1}$) are balanced.
	%			
	
	
	
	
	%	
	\item [(c)] Generate a data set $\D $ with $\Xspace = \R$ of size $N=100$ in the following way:  
	%	
	\begin{itemize}
		%		
		\item Sample each $x_i$ according to a standard normal distribution;
		%		
		\item Sample $u_i$ uniformly at random from the unit interval;
		%		
		\item Set $\yi = 2\cdot \mathds{1}_{[u_i < \exp(x_i)/(1+\exp(x_i))]} - 1$
		%		
	\end{itemize}  		
	%
	Fit a logistic regression model $\pi_{\theta}$ to the data and visualize whether the model is calibrated in a suitable way.
	%				
	Next, compute $H_{\theta}(G)$ for different values of $G,$ say $G\in\{5,\ldots,15\},$ where you use a suitable partition $\Xspace_1,\ldots,\Xspace_G$ of $\Xspace.$ 
	%				
	Repeat this whole procedure (of computing $H_{\theta}(G)$) for 1000 times and compute the average over the computed $H_{\theta}(G)$ values and plot these averages as a function of $G$ into one figure.
	%				
	\item [(d)]  Generate your data set $\D $ of size $N=100$ in the following way: 
	%	
	\begin{itemize}
		%		
		\item Sample each $x_i$ according to a standard normal distribution;
		%		
		\item Sample $u_i$ uniformly at random from the unit interval;
		%		
		\item Set $\yi= 2 \cdot \mathds{1}_{[u_i < \exp(x_i^2)/(1+\exp(x_i^2))] } -1 $
		%		
		%		
	\end{itemize}  
	%
	Repeat (c) for this data generating process and include the resulting average curve into the figure of (c).
	%	 
\end{itemize}

