
%
Assume we have a set 
$$
\D = \big\{ \, (\xi , \yi) \, \big\}_{i=1}^N \, \in \, ( \Xspace \times \Yspace)^N
$$
of training examples $(\xv_i , y_i) \in \Xspace \times \Yspace,$ where $\Xspace = \mathbb{R}^d$ and $\Yspace = \{-1,+1\}.$  
%
Recall that in logistic regression the probability $	p(y=  +1\,\vert\,\xv)$ is modeled as
%
\begin{align*}
%	
	\pi_{\thetab}: \ \Xspace & \to [0,1] \\
%	
	 \xv  &\mapsto \frac{1}{1 + \exp \big(-  \langle \thetab ,  \xv  \rangle \big) } \, ,
%	
\end{align*}
%
with $\thetab = (\theta_1, \ldots , \theta_d)^\top \in \mathbb{R}^d$ is a parameter vector.

Assume we have a partition of $\Xspace$ into $G\in \mathbb{N}$ groups\footnote{Here, we mean disjoint subsets of $\mathbb{R}^d$ whose union is $\Xspace = \mathbb{R}^d.$}, say $\Xspace_1,\ldots,\Xspace_G.$
%
Consider the following quantity
%
\begin{align*}
%	
	H_{\thetab}(G) = \sum_{g=1}^G \frac{( O_{g,+1} - E_{g,+1|{\thetab}})^2 }{E_{g,+1|{\thetab}}} + \frac{( O_{g,-1} - E_{g,-1|{\thetab}})^2 }{E_{g,-1|{\thetab}}},
%	
\end{align*}
%
where $O_{g,\pm1}$ is the number of \emph{observed} $y's$ which are $\pm 1$ and the corresponding $\xv$ is an element of $\Xspace_g,$ and $E_{g,\pm 1|{\thetab}}$ is the number of \emph{expected} $y's$ which are $\pm 1$ under the model $\pi_{\thetab}$ and the corresponding $\xv$ is an element of $\Xspace_g.$
% 
\begin{itemize}
%	
	\item [(a)] Give a mathematical definition of $ O_{g,+1},$ $ O_{g,-1},$ $E_{g,+1|{\thetab}}$ and $E_{g,-1|{\thetab}}.$
%	
	\item [(b)] If the model $\pi_{\thetab}$ is (approximately) well-calibrated, what values should $H_{\thetab}(G)$ take?  What is a desirable property of the partition $\Xspace_1,\ldots,\Xspace_G$ of $\Xspace?$
%	
	\item [(c)] Generate a data set $\D $ with $\Xspace = \R$ of size $N=100$ in the following way:  
%	
				\begin{itemize}
			%		
					\item Sample each $x_i$ according to a standard normal distribution;
			%		
					\item Sample $u_i$ uniformly at random from the unit interval;
			%		
					\item Set $y_i = 2\cdot \mathds{1}_{[u_i < \exp(x_i)/(1+\exp(x_i))]} - 1$
			%		
				\end{itemize}  		
%
				Fit a logistic regression model $\pi_{\theta}$ to the data and visualize whether the model is calibrated in a suitable way.
%				
				Next, compute $H_{\theta}(G)$ for different values of $G,$ say $G\in\{5,\ldots,15\},$ where you use a suitable partition $\Xspace_1,\ldots,\Xspace_G$ of $\Xspace.$ 
%				
				Repeat this whole procedure (of computing $H_{\theta}(G)$) for 1000 times and compute the average over the computed $H_{\theta}(G)$ values and plot these averages as a function of $G$ into one figure.
%				
	\item [(d)]  Generate your data set $\D $ of size $N=100$ in the following way: 
	%	
	\begin{itemize}
		%		
		\item Sample each $x_i$ according to a standard normal distribution;
		%		
		\item Sample $u_i$ uniformly at random from the unit interval;
		%		
		\item Set $y_i= 2 \cdot \mathds{1}_{[u_i < \exp(x_i^2)/(1+\exp(x_i^2))] } -1 $
%		
		%		
	\end{itemize}  
	%
	Repeat (c) for this data generating process and include the resulting average curve into the figure of (c).
%	 
%	\item [(e)] Let $\theta_{train}$ be the obtained parameter vector of the logistic regression after fitting it to the training data in (d). Now, generate another data set (a so-called calibration data set)
%%	
%	$$\D_{cal} = \big\{ (s_i, y_i)  \big\}_{i=1}^{N_{cal}} \subset \mathbb{S} \times \{ -1 , +1 \}$$ 
%%	
%	of size $N_{cal}=50$ in the following way: 
%	%	
%	\begin{itemize}
%		%		
%		\item Sample each $x_i$ according to a standard normal distribution;
%		%		
%		\item Sample $u_i$ uniformly at random from the unit interval;
%%		
%		\item Set $y_i= 2 \cdot \mathds{1}_{[u_i < \exp(x_i^2)/(1+\exp(x_i^2))] } -1; $
%%		
%		\item Set $s_i = \langle \theta_{train} ,  x_i  \rangle.$
%		%		
%	\end{itemize}  
%%
%	Let $\pi^{cal}_{\theta}$ be the 
%%
%	Repeat (d), but this time apply the method of empirical binning and Platt scaling to calibrate the logistic regression after the training phase, i.e., use the calibrated logistic regression model given by
%%	
%	\begin{align*}
%		%	
%		\pi_{\thetab}^C: \ &\Xspace \to \Yspace \\
%		%	
%		&\xv  \mapsto \frac{1}{1 + \exp \big(-  C(\langle \thetab ,  \xv  \rangle )\big) },
%		%	
%	\end{align*}
%%
%	where $C:\mathbb{S} \to [0,1]$ is the calibration function obtained by the corresponding calibration method.
%	\emph{Hint: Use a binning with bins of equal width for the empirical binning and vary the number of bins $M.$}
%
\end{itemize}