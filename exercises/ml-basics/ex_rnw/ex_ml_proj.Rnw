Imagine you work at a second-hand car dealer and are tasked with finding 
for-sale vehicles your company can acquire at a reasonable price. You decide to 
address this challenge in a data-driven manner and develop a model that predicts 
adequate market prices (in EUR) from vehicles’ properties.

\begin{enumerate}[a)]

  \item Characterize the task at hand: supervised or unsupervised? Regression or 
  classification? Learning to explain or learning to predict? Justify your 
  answers.
  \item How would you set up your data? Name potential features along with their 
  respective data type and state which is the target variable.
  \item Assume now that you have data on vehicles’ age (days), mileage (km), and 
  price (EUR). 
  Explicitly define the feature space $\Xspace$ and target space $\Yspace$.
  \item You choose to use a linear model (LM) for this task.
  For this, you assume the targets to be conditionally independent given the 
  features, i.e., $\yi|\xi \ind \yi[j]|\xi[j]$ for all $i,j \in 
  \{1, 2, \dots, n\}, i \neq j$, with sample size $n$.
  The LM models the target as a linear function of the features 
  with Gaussian error term: $\ydat = \Xmat \thetab + \epsilon$, \\ 
  $\epsilon \sim N(\bm{0}, \mathit{diag}(\sigma^2)), ~~ \sigma > 0$.
  % Furthermore, you have reason to believe that the effect of mileage might be 
  % non-linear, so you decide to include this quantity logarithmically (using the 
  % natural logarithm).
  
  State the hypothesis space for the corresponding model class.
  For this, assume the parameter vector $\thetab$ to include the intercept 
  coefficient.
  \item Which parameters need to be learned?
  Define the corresponding parameter space $\Theta$.
  \item State the loss function for the $i$-th observation using $L2$ loss. 
  \item In classical statistics, you would estimate the parameters via maximum 
  likelihood estimation (MLE). 
  The likelihood for the LM is given by:
  % \[
  % \ell(\thetab) = - \frac{n}{2} \log(2 \sigma^2 \pi) - \frac{1}{2 \sigma^2} 
  % (\ydat - \Xmat \thetab)^T(\ydat - \Xmat \thetab)
  % \]
  \begin{flalign*}
  \LL(\thetab | \xv) &= 
  \prodin \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-
  \frac{1}{2 \sigma^2} \left( \yi - \thetab^T \xi  \right)^2  \right) 
  % \\
  % &= \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{n}{2}} \exp \left(- 
  % \frac{1}{2 \sigma^2} \sumin \left(\yi - \thetat \xi \right)^2  \right) \\ 
  % &= \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{n}{2}} \exp \left(- 
  % \frac{1}{2 \sigma^2} \| \ydat - \Xmat \thetab \|^2 \right)
  \end{flalign*}
  Describe how you can make use of the likelihood in empirical risk minimization 
  (ERM) and write down the resulting empirical risk.
  \item Now you need to optimize this risk to find the best parameters, 
  and hence the best model, via empirical risk minimization. 
  State the optimization problem formally and list the necessary steps to solve 
  it. 

\end{enumerate}

Congratulations, you just designed your first machine learning project!

