
Consider the classification learning setting, i.e., $\mathcal{Y}=\{1,\ldots,g\},$ and the hypothesis space is $\Hspace = \{ h:\Xspace \to \Yspace  \}.$
%
The loss function of interest is the 0-1-loss:
%
$$
  \Lhxy = \mathds{1}_{\{y \ne \hx\}} =
     \footnotesize \begin{cases} 1, \quad \text{ if } y \ne \hx, \\ 0, \quad    \text{ if } y = \hx.  \end{cases}
  $$
%
%
\begin{enumerate}
	%
	\item Consider the hypothesis space of constant models 
	%
	$\Hspace = \{ h:\Xspace \to \Yspace \, | \,  \hx = \bm{\theta}  \ \forall \bm{x} \in \Xspace  \}.$
	%
	where $\Xspace$ is the feature space.
	%
	Show that 
	%
	$$\hxh = \text{mode} \left\{\yi\right\} $$
	%
	is the empirical risk minimizer for the 0-1-loss in this case.
	%
	\item What is the optimal constant model in terms of the (theoretical) risk for the 0-1-loss and what is its risk?
	%\bar{h}(\bm{x}) = \argmax_{l \in \Yspace} \P(y = l ),
	% 1 - \max_{l \in \Yspace} \P(y = l)
	\item Derive the approximation error if the hypothesis space $\Hspace$ consists of the constant models. 
	% 1 - \max_{l \in \Yspace} \P(y = l) - (1 - \E_x \left[\max_{l \in \Yspace} \P(y = l~|~ \xv = \xv)\right])
	\item Assume now $g=2$ (binary classification) and consider now the hypothesis space of probabilistic classifiers 
	%
	$\Hspace = \{ \pi:\Xspace \to [0,1]    \},$
  %
  that is, $\pi(\bm{x})$ (or $1-\pi(\bm{x})$) is an estimate of the posterior distribution $p_{y|x}(1 ~|~ \bm{x})$ (or $p_{y|x}(0 ~|~ \bm{x})$).
  %
  Further, consider the probabilistic 0-1-loss 
  %
  \begin{align*}
%  	
%  	L: \Yspace \times \Hspace &\to \{0,1\}  \\
%  	
%  	(y,\pix) &\mapsto 
  	\Lpixy = \begin{cases}
  		1, & \mbox{if ($\pix\geq 1/2$ and $y=0$) or ($\pix< 1/2$ and $y=1$), } \\
  		0, & \mbox{else. }
  	\end{cases}
%  	
  \end{align*}
%
Is the minimum of $\E_{xy}[\Lpixy ]  $ unique over $\pi \in \Hspace$\footnote{If it is unique, then the loss is a strictly proper scoring rule.}? 
%
Is the posterior distribution $p_{y|x}$ a resp.\ \emph{the} minimizer of $\E_{xy}[\Lpixy ]?$
%
Discuss the corresponding (dis-)advantages of your findings.
%

\emph{Hint:} First note that we can write $\Lpixy = \mathds{1}_{\{ \pix\geq 1/2  \}}  \mathds{1}_{\{ y=0 \}} + \mathds{1}_{\{ \pix< 1/2  \}} \mathds{1}_{\{ y=1 \}}  $ and then consider the ``unraveling trick'': $  \E_{xy}[\Lpixy ]  = \E_x \left[\E_{y|x}[\Lpixy ~|~ \bm{x}=\bm{x}] \right]. $ 

\end{enumerate}
  