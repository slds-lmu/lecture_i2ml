% !Rnw weave = knitr

<<setup-child, include = FALSE, echo=FALSE>>=
library('knitr')
knitr::set_parent("../../style/preamble_ueb_coll.Rnw")
@

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\kopf{Tuning \& Resampling}

\tableofcontents

% ------------------------------------------------------------------------------
% LECTURE EXERCISES
% ------------------------------------------------------------------------------

\dlz
\exlect
\lz

\aufgabe{benchmark experiment -- CART vs $k$-NN}{
<<child="ex_rnw/ex_cart_vs_knn.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_cart_vs_knn.Rnw">>=
@
}

\dlz
\aufgabe{benchmark study for classification}{
<<child="ex_rnw/ex_benchmark.Rnw">>=
@
}

\dlz
\loesung{

See
\href{https://github.com/compstat-lmu/lecture_i2ml/blob/master/exercises/forests/ex_rnw/sol_benchmark.R}{R code}
}

\dlz
\aufgabe{cross validation for $k$-NN}{
<<child="ex_rnw/ex_knn_cv.Rnw">>=
@
}

\dlz
\loesung{

See
\href{https://github.com/compstat-lmu/lecture_i2ml/blob/master/exercises/forests/ex_rnw/sol_knn_cv.R}{R code}
}

\newpage
\aufgabe{leave-one-out estimator}{
<<child="ex_rnw/ex_loo_estimator.Rnw">>=
@
}

\dlz
\loesung{
<<child="ex_rnw/sol_loo_estimator.Rnw">>=
@
}

% ------------------------------------------------------------------------------
% PAST EXAMS
% ------------------------------------------------------------------------------

\newpage
\exexams
\lz

\aufgabeexam{WS2020/21}{first}{6}{

Assume a polynomial regression model with a continuous target variable $y$ and 
a continuous, $p$-dimensional feature vector $\xv$ and polynomials of degree 
$d$, i.e., \\ $\fxi = \sum_{j=1}^p \sum_{k=0}^d \theta_{j,k} (\xi_j)^k$ and 
$\yi = \fxi + \epsi$ where the $\epsi$ are iid with $\var(\epsi)=\sigma^2 \ 
\forall i\in \{1, \dots, n\}$. 

\begin{enumerate}
  \item On a given data set with sample size $n=1000$, $p=1$ feature and 
  degree $d=1$ we want to estimate the generalization error. 
  Describe the advantages and disadvantages of the following three resampling 
  strategies. Additionally, state which strategy you would use here.
  \begin{enumerate}
    \item[(i)] hold-out sampling, i.e., a single train-test split
    \item[(ii)] leave-one-out cross-validation, i.e., $n$-fold cross-validation
    \item[(iii)] 5-fold cross-validation
  \end{enumerate}  
\end{enumerate}

}

\dlz
\loesung{

\begin{enumerate}
  \item 
  \begin{itemize}
    \item (i): fastest, but very dependent on the given split, i.e., can vary 
    quite a lot with varying seed
    \item (ii): slow, very robust - does not depend on seed $\Rightarrow$ 
    Perhaps best in that case, because linear model is fast
    \item (iii): trade-off: higher bias as loocv but not that dependent on the 
    seed $\Rightarrow$ Depending on computation time perhaps better than loocv
  \end{itemize}
\end{enumerate}

}

% ------------------------------------------------------------------------------
% INSPO
% ------------------------------------------------------------------------------

\dlz
\exinspo