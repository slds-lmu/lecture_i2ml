\begin{enumerate}
  \item[a)] Take a look at the \texttt{spam} dataset 
  (\texttt{?mlr3::mlr\_tasks\_spam}).
  Shortly describe what kind of classification problem this is and access the 
  corresponding task predefined in \texttt{mlr3}.

  \item[b)] Use a decision tree to predict \texttt{spam}. Re-fit the tree using 
  two random subsets of the data (each comprising 60\% of observations). 
  How stable are the trees?

  (Hint: Use \texttt{rpart.plot()} from the package \texttt{rpart.plot} to 
  visualize the trees.)

  \item[c)] Forests come with a built-in estimate of their generalization 
  ability via the out-of-bag (OOB) error.
  \begin{enumerate}[i)]
    \item Show that the probability for each observation to be OOB in an 
    arbitrary bootstrap sample converges to $\tfrac{1}{e}$.
    \item Verify this result empirically by a small simulation. 
    For this, draw 1000 bootstrap samples from 
    a set of 1000 IDs and compute the average relative frequency of 
    being OOB over all IDs.
    \item Use the random forest learner \texttt{classif.ranger} to fit the 
  model and state the out-of-bag (OOB) error.
  \end{enumerate}

  \item[d)] You are interested in which variables have the greatest influence 
  on the prediction quality. 
  Explain how to determine this in a permutation-based approach and compute the
  importance scores for the \texttt{spam} data.

  (Hint: use an adequate variable importance filter as described in \\
  \url{https://mlr3filters.mlr-org.com/#variable-importance-filters}.)
\end{enumerate}
