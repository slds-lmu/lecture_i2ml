\begin{enumerate}[a)]
  \item Take a look at the \texttt{spam} dataset and shortly describe what kind of classification problem this is.\\
  (R Hint: access the corresponding task \texttt{?mlr3::mlr\_tasks\_spam})\\
  (Python Hint: read \href{https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/data/spam.csv}{\texttt{spam.csv}})\\


  \item Use a decision tree to predict \texttt{spam}. Re-fit the tree using 
  two random subsets of the data (each comprising 60\% of observations). 
  How stable are the trees?

  (R Hint: Use \texttt{rpart.plot()} from the package \texttt{rpart.plot} to 
  visualize the trees.)\\
  (Python Hint: Use \texttt{from sklearn.tree import plot\_tree} to 
  visualize the trees.)

  \item Forests come with a built-in estimate of their generalization 
  ability via the out-of-bag (OOB) error.
  \begin{enumerate}[i)]
    \item Show that the probability for an observation to be OOB in an 
    arbitrary bootstrap sample converges to $\tfrac{1}{e}$.
    % \item Verify this result empirically by a small simulation. 
    % For this, draw 1000 bootstrap samples from 
    % a set of 1000 IDs and compute the average relative frequency of 
    % being OOB over all IDs.
    \item Use the random forest learner (R: \texttt{classif.ranger}, Python: \texttt{RandomForestClassifier()}) to fit the 
  model and state the out-of-bag (OOB) error.
  \end{enumerate}

  \item You are interested in which variables have the greatest influence 
  on the prediction quality. 
  Explain how to determine this in a permutation-based approach and compute the
  importance scorses for the \texttt{spam} data.

  (R Hint: use an adequate variable importance filter as described in \\
  \url{https://mlr3filters.mlr-org.com/#variable-importance-filters}.)\\
  (Python Hint: choose an adequate importance measure as described in \\
  \url{https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html})
\end{enumerate}
