\begin{align*}
 \ambifM &= \tfrac{1}{M}\sum^M_{m} \left(\blm- \fM\right)^2 \\
         &= \tfrac{1}{M}\sum^M_{m} \left(\left(\blm - y\right)  + \left(y - \fM\right)\right)^2\\
         &= \tfrac{1}{M}\sum^M_{m} L(y, \blm) + L(y, \fM) \underbrace{- 2 \left(y - \tfrac{1}{M}\sum^M_{m=1}\blm\right)\left(y - \fM\right)}_{- 2 L\left(y, \fM\right)} \\[-.5em] \intertext{So, if we take the expected value over the data's distribution:}
         \Exy\left[L\left(y, \fM\right)\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \blm \right)\right] - \Exy\left[\ambifM\right]
\end{align*}

$\Rightarrow$ The expected loss of the ensemble is lower than the average loss of the single base learner by the amount of instability in the ensemble's base learners.\\ The more accurate and diverse the base learners, the better.

How to make $\Exy\left[\ambifM\right]$ as large as possible?
\begin{align*}
\Exy\left[L\left(y, \fM \right)\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \blm \right)\right] - \Exy\left[\ambifM\right] \\
\end{align*}

Assume $\Exy\left[\blm\right] = 0$ for simplicity, $\var_{xy}\left[\blm\right] = \Exy\left[(\blm)^2\right] = \sigma^2$, $\corr_{xy}\left[\blm, \bl{m'}\right] = \rho$ for all $m, m'$.

\begin{align*}
\implies
\var_{xy}\left[\fM\right] &= \tfrac{1}{M} \sigma^2 +  \tfrac{M-1}{M} \rho \sigma^2 \qquad\left(... = \Exy\left[(\fM)^2\right]\right)\\
 \Exy\left[\ambifM\right] &= \tfrac{1}{M}\sum^M_{m} \Exy\left[\left(\blm- \fM\right)^2\right]\\
 & = \tfrac{1}{M}\left(M \Exy\left[(\blm)^2\right] + M \Exy\left[(\fM)^2\right] -
     2 M \Exy\left[\blm\fM\right]\right) \\
  & = \sigma^2  + \Exy\left[(\fM)^2\right] - 2 \tfrac{1}{M}\sum^M_{m'} \underbrace{\Exy\left[\blm \bl{m'} \right]}_{\mathclap{\qquad\qquad\qquad\qquad= \cov_{xy}\left[\blm, \bl{m'} \right] + \Exy\left[\blm\right]\Exy\left[\bl{m'}\right]}} \\
  &=  \sigma^2  + \left(\tfrac{1}{M} \sigma^2 +   \tfrac{M-1}{M} \rho \sigma^2\right) - 2\left(\tfrac{M-1}{M} \rho\sigma^2 + \tfrac{1}{M}\sigma^2 + 0 \cdot 0 \right)\\
  &= \tfrac{M-1}{M} \sigma^2 (1-\rho)
\end{align*}

\begin{align*}
\Exy\left[L\left(y, \fM\right)\right] &= \textcolor{blue}{\tfrac{1}{M}\sum^M_{m} \Exy\left[L\left(y, \blm \right)\right]} - \Exy\left[\ambifM\right]\\
\Exy\left[\ambifM\right] &\cong
\textcolor{purple}{\frac{M-1}{M}} \textcolor{cyan}{\var_{xy}\left[\blm\right]} \textcolor{violet}{\left(1 - \corr_{xy}\left[\blm, \bl{m'}\right]\right)}
\end{align*}

\begin{itemize}
\item[$\Rightarrow$] \textcolor{blue}{\textbf{better base learners}} are better {\small (... duh)}
\item[$\Rightarrow$] \textcolor{purple}{\textbf{more base learners}} are better {\small (theoretically, at least...)}\\
\item[$\Rightarrow$] \textcolor{cyan}{\textbf{more variable base learners}} are better {\small(as long as their risk stays the same, of course!)}
\item[$\Rightarrow$] \textcolor{violet}{\textbf{less correlation between base learners}} is better:\\ bagging helps more if base learners are wrong in different ways so that their errors \enquote{cancel} each other out.\\
\end{itemize}
