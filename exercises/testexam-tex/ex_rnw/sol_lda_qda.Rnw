In order to arrive at the equation for the decision boundary, we
  first need to understand that, on the boundary of classes 1 and 2, both
  discriminant functions $\delta_1(\xv)$ and $\delta_2(\xv)$ will be exactly
  equal.
  Therefore, we compute the equation as follows:

  \begin{align*}
    \delta_1(\xv) &= \delta_2(\xv) \\
    \Leftrightarrow \log \pik[1]  - \frac{1}{2} \bm{\mu}_1^\top \Sigma^{-1}
    \bm{\mu}_1 + \xv^\top \Sigma^{-1} \bm{\mu}_1 &=
    \log \pik[2] - \frac{1}{2} \bm{\mu}_2^\top
    \Sigma^{-1} \bm{\mu}_2 + \xv^\top \Sigma^{-1} \bm{\mu}_2 \\
    \Leftrightarrow \xv^\top \Sigma^{-1} \bm{\mu}_1 - \xv^\top \Sigma^{-1}
    \bm{\mu}_2  &= \log \frac{\pik[2]}{\pik[1]}
    + \frac{1}{2} \left( \bm{\mu}_1^\top \Sigma^{-1} \bm{\mu}_1 -
    \bm{\mu}_2^\top \Sigma^{-1} \bm{\mu}_2 \right) \\
    \Leftrightarrow \xv^\top \underbrace{\left( \Sigma^{-1} (\bm{\mu}_1 -
    \bm{\mu}_2) \right)}_{=: \bm{\nu} \in \R^{2 \times 1}}
    &= \log \frac{\pik[2]}{\pik[1]}
    + \frac{1}{2} \left( \bm{\mu}_1^\top \Sigma^{-1} \bm{\mu}_1 -
    \bm{\mu}_2^\top \Sigma^{-1} \bm{\mu}_2 \right) \\
    \Leftrightarrow \xv^\top \nu &= \underbrace{
    \log \frac{\pik[2]}{\pik[1]}
    + \frac{1}{2} \left( \bm{\mu}_1^\top \Sigma^{-1} \bm{\mu}_1 -
    \bm{\mu}_2^\top \Sigma^{-1} \bm{\mu}_2 \right)}_{=: a \in \R}.
  \end{align*}
  The right hand side might look somewhat complicated but simply evaluates to
  a scalar and we obtain the hyperplane equation $\xv^\top \nu = a$, in this
  case defining a line in $\R^2$.

  Again, we see that LDA is indeed a linear classifier.
