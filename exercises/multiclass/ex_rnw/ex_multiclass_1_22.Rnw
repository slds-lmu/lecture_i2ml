
Consider the multiclass classification scenario consisting of a feature space $\Xspace$ and a label space $\Yspace=\{1,\ldots,g\}$ with $g\geq 2$ classes.
%
Moreover, we consider the hypothesis space of models based on $g$ discriminant/scoring functions:
%
$$ \Hspace = \{  f=(f_1,\ldots,f_g)^\top: \Xspace \to \R^g ~|~ f_i:\Xspace \to \R, \ \forall i \in \Yspace  \}. $$
%
A model $f$ in $\Hspace$ is used to make a prediction by means of transforming the scores into classes by choosing the class with the maximum score:
%
\begin{align} \label{def_predictor}
%	
	h(\xv) = \argmax_{k \in \gset} \fkx[k]. 
%	
\end{align}
%
The multiclass hinge loss for models in $\Hspace$ is defined by
%
$$ L(y,\fx)= \max_{k} \left( f_k(\xv) - f_y(\xv) + \mathds{1}_{\{ y \ne k \}} \right).  $$
%
%
%
\begin{enumerate}
%
\item  Show that the 0-1-loss for a predictor $h$ as in \eqref{def_predictor} based on a model  $f \in \Hspace$ is at most the multiclass hinge loss for $f$ i.e., 
%
$$ L_{0-1}(y,\hx) = \mathds{1}_{\{ y \ne \hx \}} \leq L(y,\fx). $$ 

\item  Verify that the multiclass hinge loss of $f \in \Hspace$ on a data point $(\xv,y)\in \Xspace \times \Yspace$ is bounded from above by $ \sum_{k\neq y} \max \{  0, 1 +  f_k(\xv) - f_y(\xv)  \}.$

\emph{Hint:} Note that this upper bound is sometimes referred to as the multiclass hinge loss.

\item In the case of binary classification, i.e., $g=2$ and $\Yspace=\{-1,+1\},$ we use a single discriminant model $\fx = f_{1}(\xv) - f_{-1}(\xv)$  based on two scoring functions $f_{1},f_{-1}:\Xspace \to \R$ for the prediction by means of
%
$\hx = \text{sgn}(\fx).$
%
Here, $f_{1}$ is the score for the positive class and $f_{-1}$ is the score for the negative class.
%
Show that the upper bound in (b) coincides with the binary hinge loss $L(y,\fx)=\max\{0,1- y\fx \}.$
%
%
\item Recall the statement of the lecture regarding the binary hinge loss: 
%
\begin{center}
	``$\ldots$ the hinge loss only equals zero for a margin $\geq 1$ encouraging confident (correct) predictions.''.
\end{center}
%
Can we say something similar for the alternative multiclass hinge loss in (b)?
%
%
\item Now consider the case in which the score functions are linear, i.e., $f_i(\xv)=\thetab_i^\top \xv$ for each $i \in \Yspace.$
%
What is the difference between
%
\begin{itemize}
	%	
	\item a model which is obtained by (empirical) loss minimization of the alternative multiclass hinge loss in (b), and
	%	
	\item a one-vs-rest model obtained by (empirical) loss minimization of the binary hinge loss for the binary classifiers?
	%	
\end{itemize}
%
%
%\item Denote by $\thetab= (\thetab_1^\top, \ldots, \thetab_g^\top)^\top \in \R^{g\cdot p}$ the ``stacked'' parameter vector of the score functions obtained by stacking the parameters of the score functions. 
%%
%Show that the multiclass hinge loss is convex with respect to $\thetab,$ i.e., $\thetab \mapsto \max\limits_{k} \left( \thetab_k^\top \xv - \thetab_y^\top \xv + \mathds{1}_{\{ y \ne k \}} \right)$ is convex.
%
%\emph{Hint:} Use the following two facts:
%\begin{itemize}
%%	
%	\item If $\phi_1,\ldots,\phi_m:$ are convex, then also $\phi(\xv)=\max\{\phi_1(\xv),\ldots,\phi_m(\xv)\}.$
%%	
%	\item 
%%	
%\end{itemize}

\end{enumerate}