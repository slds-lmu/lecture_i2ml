In this exercise you will build a small CART regression tree by hand on a tiny dataset.

%\section*{Data A (no missing, one feature)}
\[
\begin{array}{c|cc}
\text{Obs} & X_1 & Y\\\hline
1 & 1.0 & 0\\
2 & 2.0 & 4\\
3 & 3.0 & 6\\
4 & 4.0 & 9
\end{array}
\]

\textbf{Parent stats:} $\bar y = 4.75$, $\mathrm{SSE}_{\text{parent}} = \sum\nolimits_i (y_i-\bar y)^2 = 42.75$.

\textbf{Candidate splits (precomputed means, just plug into SSE):}
\[
\begin{array}{c|c|c|c|c}
\text{Split on }X_1 & \text{Left obs} & \text{Right obs} & \bar y_L & \bar y_R\\\hline
1.5 & \{1\} & \{2,3,4\} & 0 & 19/3\ (\approx 6.33)\\
2.5 & \{1,2\} & \{3,4\} & 2 & 7.5\\
3.5 & \{1,2,3\} & \{4\} & 10/3\ (\approx 3.33) & 9
\end{array}
\]
Compute $\mathrm{SSE}_{\text{split}} = \sum_{i \in L}(y_i-\bar y_L)^2 + \sum_{i \in R}(y_i-\bar y_R)^2$ for each row.

\begin{enumerate}
    \item Sketch a scatter plot of $(x_1,y)$ and the globally optimal 2-split tree. Show that the total SSE equals 2.
    \item Run CART's greedy search:
    \begin{enumerate}
    \renewcommand{\labelenumii}{(\roman{enumii})}
        \item List all candidate split points for $X_1$.
        \item For each candidate, compute SSE and identify the best first split.
        \item Grow the second split greedily and report the final 2-split tree.
    \end{enumerate}
    \item Compare the greedy 2-split tree with the optimal one. Are they the same?
\end{enumerate}

