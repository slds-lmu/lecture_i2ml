
In this exercise you will build a small CART regression tree by hand on three toy datasets using the greedy split search. You'll then practice \textbf{surrogate splits}, which are helpful for reducing the model's bias toward features with many missing values (NAs). For background, see \url{https://slds-lmu.github.io/i2ml/chapters/06_cart/06-05-computationalaspects/}.

% \medskip
\section*{Data A (no missing, one feature)}
\[
\begin{array}{c|cc}
\text{Obs} & X_1 & Y\\\hline
1 & 1.0 & 0\\
2 & 2.0 & 5\\
3 & 3.0 & 5\\
4 & 4.0 & 10
\end{array}
\]

\begin{enumerate}[]
    \item Make a scatter plot of $(x_1,y)$. Sketch a \emph{2-split} tree that will make perfect predictions on the given data.
    \item Run one step of CART's greedy search on \textbf{Data A}:
    \begin{enumerate}[label=(\roman*), nosep]
        \item List all candidate split points for $X_1$.
        \item For each candidate, compute left/right means and risks (sum of squared errors (SSE)), and the \emph{total} post-split risk.
        \item Pick the best first split, then grow the second split greedily and report the final 2-split tree.
    \end{enumerate}
    \item Compare the greedy 2-split tree with the one you sketched earlier. Are they the same?
\end{enumerate}


% \medskip
\section*{Data B (no missing, two features)}
\[
\begin{array}{c|ccc}
\text{Obs} & X_1 & X_2 & Y\\\hline
1 & 1.0 & 2.0 & 0\\
2 & 2.0 & 2.1 & 5\\
3 & 3.0 & 2.2 & 5\\
4 & 4.0 & 3.0 & 10
\end{array}
\]

\begin{enumerate}[]
\setcounter{enumi}{3}
    \item Suppose the \emph{first split at the root} is the primary split $X_1<1.5$. Build a \textbf{surrogate split} at the root that mimics this primary split using only $X_2$.  
    State (i) the surrogate rule and (ii) its \emph{agreement} with the primary partition, where  
    \[
    \text{agreement} \;=\; 
    \frac{\#\{\text{rows routed to the same child by both splits}\}}
                {\text{total \# rows in the node}}.
    \]
\end{enumerate}

% \medskip
\section*{Data C (missing value in $X_1$)}
\[
\begin{array}{c|ccc}
\text{Obs} & X_1 & X_2 & Y\\\hline
1 & 1.0 & 2.0 & 0\\
2 & 2.0 & 2.1 & 5\\
3 & \text{NA} & 2.2 & 5\\
4 & 4.0 & 3.0 & 10
\end{array}
\]

\begin{enumerate}[]
\setcounter{enumi}{4}
\item Find the best first split at the root while \emph{discarding} every row that has an NA in the feature currently being evaluated.  
State the chosen feature, threshold, and the resulting post-split risk.

\item Repeat the search but now rank candidate splits by the \emph{adjusted improvement}
    \[
        \Delta^{\mathrm{adj}}_j \;=\; 
        \frac{m_j}{m}\,\bigl(R_{\text{parent on }j}-R_{\text{children on }j}\bigr),
    \]
where $m$ is the node size and $m_j$ the number of non-missing values for
feature~$j$.  
Report the selected split and its adjusted improvement.

\item Compare the two splits and explain why the adjustment can be preferred when many values are missing.

\item How could surrogate splits be used to tackle the NA issue?
\end{enumerate}
