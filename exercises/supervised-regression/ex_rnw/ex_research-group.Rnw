A medical research group wants to train a supervised regression model to predict how long a patient stays in the hospital after admission (measured in days). They have gathered data on each patientâ€˜s age (in years), systolic blood pressure (in mmHg, a continuous scale), and weight (in kg).
Formally, this gives rise to the target space $\Yspace = \R$ (where we technically allow for negative values as a prerequisite of the linear model) and the feature space $\Xspace = (\R_{0}^{+})^3$, with $\xi = (x_{age},\;x_{blood\;pressure},\;x_{weight})^{(i)} \in \Xspace$ 
  for $i = 1, 2, \dots, n$ observations. 
At first, they believe all of these three features should be modeled to have a linear effect on the target variable. To construct their supervised regression model, they remember how most of such models can be described:
\medbreak
\textbf{Learning = Hypothesis Space + Risk + Optimization.}
\medbreak
With the first of the three components of learning in his mind, researcher 1 (Holger) comes up with the following hypothesis space:
\begin{align}
\Hspace = \{f: \fx = \theta_0 + \theta_1 \cdot x_{age} + \theta_2 \cdot x_{blood\;pressure} + \theta_3 \cdot x_{weight} \enspace | \enspace \thetab \in \N^{3}\}
\end{align}

\begin{enumerate}\bfseries
  \item[1)] Is the hypothesis space formulated above correct? If not, find the correct one.
\end{enumerate}

Researcher 2 (Lisa) recalls the second component of learning: risk. She remembers two slightly different functions for calculating the empirical risk, but is not sure what difference they will make in optimization:
\begin{align}
\riskef = & \sumin \Lxyi \\
\riskeb(f) = & \frac{1}{n}\sumin \Lxyi
\end{align}

\begin{enumerate}\bfseries
  \item[2)] What difference may the two functions make for optimization?
\end{enumerate}

As a next step, the research group debates the influence the loss function (L1 or L2 loss) they choose has on the three components of learning.

\begin{enumerate}\bfseries
  \item[3)] Which of the three components of learning are impacted by the choice of the loss function in the case of the linear regression model?
\end{enumerate}

Researcher 3 (Son) suggests to use L2 loss for their linear regression model, and notes this would require no numerical optimization to find the parametrization with lowest empirical risk.

\begin{enumerate}\bfseries
  \item[4)] Is this true or false? Explain your answer.
\end{enumerate}

After training the linear regression model, researcher 1 (Holger) proposes to add more flexibility by training a polynomial regression model with degree $d = 1$ for age and systolic blood pressure, and $d = 2$ for weight. Once again, he formulates a hypothesis space:
\begin{align}
\Hspace = \{f: \fx = \theta_0 + \theta_1 \cdot x_{age} + \theta_2 \cdot x_{blood\;pressure} + \theta_3 \cdot (x_{weight}+x_{weight}^2) \enspace | \enspace \thetab \in \R^{4}\}
\end{align}

\begin{enumerate}\bfseries
  \item[5)] Is the hypothesis space formulated above correct? If not, find the correct one.
\end{enumerate}

The research group decides to train the polynomial regression model using L2 loss. Again, researcher 3 (Son) is worried about the optimization component of learning. He is not entirely sure how to find the parametrization with lowest empirical risk.

\begin{enumerate}\bfseries
  \item[6)] Explain how the model with the lowest empirical risk can be found in the case of the polynomial regression model with L2 loss.
\end{enumerate}