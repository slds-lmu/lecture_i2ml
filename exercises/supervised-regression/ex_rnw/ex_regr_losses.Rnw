In this exercise, we will examine loss functions for regression tasks 
somewhat more in-depth.

<<echo=FALSE, message=FALSE, fig.align="left", fig.height=2.5, fig.width=4>>=

set.seed(123L)
x <- runif(100L)
y <- 0.2 + 0.8 * x
y <- y + rnorm(length(x), sd = 0.1)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 
  ggplot2::theme_bw() + 
  ggplot2::geom_smooth(formula = y ~ x, method = "lm", se = FALSE) +
  ggplot2::annotate("point", x = 0.95, y = 0.3, color = "orange", size = 2)

@

\begin{enumerate}[a)]
  \item Consider the above linear regression task. How will the model 
  parameters be affected by adding the new outlier point (orange) if you use
  \begin{enumerate}[i)]
    \item $L1$ loss
    \item $L2$ loss
  \end{enumerate}
  in your empirical risk? (You do not need to actually compute the 
  parameter values.)
\end{enumerate}

<<echo=FALSE, message=FALSE, fig.align="left", fig.height=2.5, fig.width=4>>=

huber_loss <- function(res, delta = 0.5) {
  if (abs(res) <= delta) {
    0.5 * (res^2)
  } else {
    delta * abs(res) - 0.5 * (delta^2)
  }
}

x <- seq(-10L, 10L, length.out = 1000L)
y <- sapply(x, huber_loss, delta = 5L)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_line() + 
  ggplot2::theme_bw()

@

\begin{itemize}
  \item[b)] The second plot visualizes another loss function popular in 
  regression tasks, the so-called \textit{Huber loss} (depending on 
  $\delta > 0$; here: $\delta = 5$). 
  Describe how the Huber loss deals with residuals as compared to $L1$ and $L2$ 
  loss.
  Can you guess its definition? 
  \item[c)] \textcolor{orange}{Zu schwer?}
  Show that $\mathit{median}(y)$ is the optimal constant prediction $c$
  when using $L1$ loss. \\
  Hint:
  \begin{itemize}
    \item Employing the law of total expectation, we can find $c$ via
    $$\argmin_c \E\left[|y - c|\right] = \argmin_c
    \int_{-\infty}^\infty |y - c| ~ p(y) \text{d}y =
    \argmin_c \int_{-\infty}^c -(y - c)~p(y)~\text{d}y + \int_c^\infty
    (y - c)~p(y)~\text{d}y .$$
    \item Setting the derivative of this expression, found by application
    of Leibniz's rule, to zero yields
    $$0 \overset{^\text{!}}{=} \int_{-\infty}^c  ~p(y)~\text{d}y -
    \int_c^\infty ~p(y)~\text{d}y.$$
  \end{itemize}
  \item[d)] Derive the least-squares estimator, i.e., the solution to the linear 
  model when using $L2$ loss, analytically via
  $$\argmin_{\thetab} \| \yv - \Xmat \thetab \|_2^2.$$
\end{itemize}

