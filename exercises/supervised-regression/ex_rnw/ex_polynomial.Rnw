Assume the following (noisy) data-generating process from which we have 
observed 50 realizations: $y = -3 + 5 * 
\sin(0.4 \pi x) + \epsilon$ with $\epsilon ~ \sim \mathcal{N}(0, 1)$.

<<echo=FALSE, message=FALSE, fig.align="left", fig.height=2.5, fig.width=4>>=

set.seed(123L)
x <- seq(-3, 3, length.out = 50)
y <- -3 + 5 * sin(0.4 * pi * x) + rnorm(50, sd = 1)

ggplot2::ggplot(data.frame(x = x, y = y), ggplot2::aes(x = x, y = y)) +
  ggplot2::geom_point() + 

@

\begin{enumerate}[a)]
  \item We decide to model the data with a cubic polynomial (including intercept 
  term). State the corresponding hypothesis space.
  \item Demonstrate that this hypothesis space is simply a parameterized family 
  of curves by plotting in \texttt{R} curves for 3 different models belonging to 
  the considered model class.
  \item State the empirical risk w.r.t. $\thetab$ for a member of the hypothesis 
  space. Use $L2$ loss and be as explicit as possible.
  \item We can minimize this risk using gradient descent. In order to make 
  this somewhat easier, we will denote the transformed feature matrix, 
  containing $x$ to the power from 0 to 3, by $\tilde \Xmat$, such that we can 
  express our model by $\tilde \Xmat \thetab$ (note that the model is still 
  linear in its parameters, even if $\Xmat$ has been transformed in a non-linear 
  manner!). Derive the gradient of the empirical risk w.r.t $\thetab$.
  \item Using the result from d), state the calculation to update the current 
  parameter $\thetat$.
  \item You will not be able to fit the data perfectly with a cubic polynomial.
  Describe the advantages and disadvantages that a more 
  flexible model class would have. 
  Would you opt for a more flexible learner?
\end{enumerate}