---
title: "Exercise 8 -- CART"
subtitle: "[Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)"
notebook-view:
  - notebook: ex_trees_R.ipynb
    title: "Exercise sheet for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/trees/ex_trees_R.ipynb"
  - notebook: ex_trees_py.ipynb
    title: "Exercise sheet for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/trees/ex_trees_py.ipynb"
  - notebook: sol_trees_R.ipynb
    title: "Solutions for R"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/trees/sol_trees_R.ipynb"
  - notebook: sol_trees_py.ipynb
    title: "Solutions for Python"
    url: "https://github.com/slds-lmu/lecture_i2ml/blob/master/exercises/trees/sol_trees_py.ipynb"
---

::: {.content-hidden when-format="pdf"}
::: {.hidden}
{{< include ../_quarto/latex-math.qmd >}}
:::
:::

<details> 
<summary>*Hint: Useful libraries*</summary>

::: {.panel-tabset}

### R
{{< embed ex_trees_R.ipynb#import echo=true >}}
### Python
{{< embed ex_trees_py.ipynb#import echo=true >}}
:::
</details>

## Exercise 1: Splitting criteria

::: {.callout-note title="Learning goals" icon=false}
1) Perform split computation with pen and paper
2) Derive optimal constant predictor for regression under L2 loss
:::

Consider the following dataset:

```{r, echo=FALSE, eval=TRUE, results=FALSE, message=FALSE, warning=FALSE, fig.align="center", fig.height=5, fig.width=5}
x = c(1, 2, 7, 10, 20)
y = c(1, 1, 0.5, 10, 11)
data = t(data.frame(x = x, y = y))
knitr::kable(data, "latex", digits = 3L)
log_x = log(x)
data = t(data.frame(log_x, y))
knitr::kable(data, "latex", digits = 2L)
```
| $i$  | $x^{(i)}$ | $y^{(i)}$ |
| ---: | ---:      | ---:      | 
| 1    | 1.0       | 1.0       | 
| 2    | 2.0       | 1.0       | 
| 3    | 7.0       | 0.5       | 
| 4    | 10.0      | 10.0      | 
| 5    | 20.0      | 11.0      | 

***
Compute the first split point the CART algorithm would find for each data set (with pen and paper or in `R`, resp. `Python`).
Use mean squared error (MSE) to assess the empirical risk.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

### Pen-and-paper solution

::: {.callout-tip}
## Procedure

1. We have only one split variable $x$. We probe all binary splits, where thresholds are placed equidistant between the observed feature values (think about why this might help generalization).

2. For each threshold:
  * Map observations to child nodes resulting from split at that threshold
  * Compute optimal constant prediction and resulting loss in both child nodes
  * Aggregate to obtain overall loss of split

3. Choose optimal split w.r.t. empirical risk reduction.
:::

::: {.callout-warning}
## Accounting for node size

Two scenarios in aggregating the risk contributions from both candidate child nodes:

- Child node risk is sum of pointwise losses without accounting for number of observations $\rightsquigarrow$ *simple* average
- Child node risk is average of pointwise losses, thus accounting for number of observations  $\rightsquigarrow$ *weighted* average
:::

### Split points

Split candidates $t \in \{1.5, 4.5, 8.5, 15\}$, leading to the following child nodes:

  \begin{align*}
    \Nl(t) &= \{ (x,y) \in \Np: x \leq t \}, ~~ 
    \Nr(t) = \{ (x,y) \in \Np: x > t \}.
  \end{align*}
  
### Risk per split point

Calculate $\risk(\Np, t)$ for each split point:

- $x \leq 1.5$
  - Predictions: $c_1 = \tfrac{1}{1} \sum_{i = 1}^1 y_i = 1, ~~ c_2 = \tfrac{1}{4} \sum_{i = 2}^5 y_i = 5.625$
  - Risk left: $\rho_{\text{MSE}}(\Nl, 1.5) = 0$
  - Risk right: $\rho_{\text{MSE}}(\Nr, 1.5) = \tfrac{1}{4}((1 - 5.625)^2 + (0.5 - 5.625)^2 + (10 - 5.625)^2 + (11 - 5.625)^2)$
  - Aggregate risk: $\risk(\Np, 1.5) = \tfrac{|\Nl|}{|\Np|} \rho_{\text{MSE}}(\Nl, 1.5) +\tfrac{|\Nr|}{|\Np|} \rho_{\text{MSE}}(\Nr, 1.5) = \tfrac{1}{5} \cdot 0 + \tfrac{4}{5} \cdot 23.925 = 19.14$

- $x \leq 4.5$ $\rightsquigarrow$ $\risk(\Np, 4.5) = 13.43$

- $x \leq 8.5$ $\rightsquigarrow$ $\risk(\Np, 8.5) = 0.13$ $\rightsquigarrow$ **optimal**

- $x \leq 15$ $\rightsquigarrow$ $\risk(\Np, 15) = 12.64$


::: {.panel-tabset}

### R
Write function to perform MSE-based splits:
{{< embed sol_trees_R.ipynb#split-fun echo=true >}}
Apply to dataset:
{{< embed sol_trees_R.ipynb#risk-comp echo=true >}}

### Python
Imports
{{< embed ex_trees_py.ipynb#import echo=true >}}
Write function to perform MSE-based splits:
{{< embed sol_trees_py.ipynb#split-fun echo=true >}}
Apply to dataset:
{{< embed sol_trees_py.ipynb#risk-comp echo=true >}}

:::

</details> 
:::

***
Derive the optimal constant predictor for a node $\Np$ when minimizing the empirical risk under $L2$ loss and explain why this is equivalent to minimizing variance impurity.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

Constant $L2$ risk minimizer for a node $\Np$:
  $$
  \bar y = \argmin_c \frac{1}{|\Np|}  \sum_{i = 1}^{|\Np|} (\yi - c)^2, 
  $$

  because

  \begin{align*}
  \min_{c \in \R} \frac{1}{|\Np|} \sum_{i = 1}^{|\Np|} (\yi - c)^2 &\Longleftrightarrow
  \pd{}{c} \left( \frac{1}{|\Np|} \sum_{i = 1}^{|\Np|} (\yi - c)^2 \right) = 0 \\
  &\Longleftrightarrow \frac{1}{|\Np|} \pd{}{c} \left( \sum_{i = 1}^{|\Np|} 
  \left({\yi}^2 - 2 \yi c + c^2\right) \right) = 0 \\
  &\Longleftrightarrow \left( \sum_{i = 1}^{|\Np|} \left(
  -2 \yi + 2c\right) \right) = 0 \\
  &\Longleftrightarrow |\Np| \cdot c =  \sum_{i = 1}^{|\Np|} \yi \\
  &\Longrightarrow \hat c = \frac{1}{|\Np|}  \sum_{i = 1}^{|\Np|} \yi
  = \bar y.
  \end{align*}

It's easy to see that the mean prediction minimizes *variance impurity*. 

- We just found that
$$
  \bar y = \argmin_{c \in \R} \frac{1}{|\Np|}  \sum_{i = 1}^{|\Np|} (\yi - c)^2.
  $$
- Looking closely at this minimization objective reveals: it's simply the (biased) sample variance for sample mean $c$.
- Therefore, predicting the sample mean minimizes both $L2$ risk and variance impurity.

::: {.callout-tip}
## Thinking a little further

- The mean of a sample is precisely the value for which the sum of squared distances to all points is minimal (the "center" of the data).
- Constant mean prediction is equivalent to an intercept LM (trained with $L2$ loss) $\rightsquigarrow$ regression trees with $L2$ loss perform piecewise constant linear regression.
:::

</details> 
:::


***
Explain why performing further splits can never result in a higher overall risk with $L2$ loss as a splitting criterion.
<details> 
<summary>**Solution**</summary>

The variance of a subset of the observations in a node cannot be higher than the variance of the entire node, so it's not possible to make the tree worse (w.r.t. training error) by introducing a further split.

</details> 

## Exercise 2: Splitting criteria

::: {.callout-note title="Learning goals" icon=false}
Understand the effect of CART hyperparameters
:::

In this exercise, we will have a look at two of the most important CART hyperparameters, i.e., design choices exogenous to training. Both `minsplit` and `maxdepth` influence the number of input space partitions the CART will perform.

***
How do you expect the number of splits to affect the model fit and generalization performance?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

Allowing for more splits will make the model more complex, thus---all else being equal---achieving a better data fit but also increasing the risk of overfitting. 

</details> 
:::

***
Using `mlr3`, fit a regression tree learner (`regr.rpart`) to the `bike_sharing` task (omitting the `date` feature) for

- `maxdepth` $\in \{2, 4, 8\}$ with `minsplit` $= 2$

- `minsplit` $\in \{5, 1000, 10000\}$ with `maxdepth` $= 20$

What do you observe?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, fig.align="center", fig.height=4, fig.width=9}
library(mlr3verse)
library(rattle)
task <- tsk("bike_sharing")
task$select(task$feature_names[task$feature_names != "date"])

for (i in c(2, 4, 8)) {
  learner <- lrn("regr.rpart", minsplit = 2, maxdepth = i, minbucket = 1)
  learner$train(task)
  fancyRpartPlot(learner$model, caption = sprintf("maxdepth: %i", i))
}

for (i in c(5, 1000, 10000)) {
  learner <- lrn("regr.rpart", minsplit = i, maxdepth = 20, minbucket = 1)
  learner$train(task)
  fancyRpartPlot(learner$model, caption = sprintf("minsplit: %i", i))
}
```

Higher values of `maxdepth` and lower values of `minsplit`, respectively, produce more complex trees.

</details> 
:::

***
Which of the two options should we use to control tree appearance?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

Both hyperparameters can be used to control tree depth, and their effect depends on the properties of the data-generating process (e.g., at least 100 observations to split further can mean very pure or very impure nodes). Sometimes requirements like interpretability might steer our decision (e.g., a tree of depth 15 is probably hard to interpret). Usually, however, we will employ **hyperparameter tuning** to determine the values for both (and other) hyperparameters, deferring the responsibility from the practitioner to the data.

</details> 
:::

## Exercise 3: Impurity reduction

::: {.callout-tip icon=false title="Only for lecture group A"}
:::

::: {.callout-note title="Learning goals" icon=false}
1. Develop intuition for use of Brier score in classification trees
2. Reason about distribution and expectations of random variables
3. Handle computations involving expectations
:::

::: {.callout-warning title="TLDR;"}
This exercise is rather involved and requires some knowledge of probability theory.

Main take-away (besides training proof-type questions): *In constructing CART with minimal Gini impurity, we minimize the expected rate of misclassification across the training data.*
:::

We will now build some intuition for the Brier score / Gini impurity as a splitting criterion by showing that it is equal to the expected MCE of the resulting node.

The fractions of the classes $k=1,\ldots, g$ in node $\Np$ of a decision tree are $\pi^{(\Np)}_1,\ldots,\pi^{(\Np)}_g$, where 
$$
\pikN = \frac{1}{|\Np|} \sum_{(x^{(i)},y^{(i)}) \in \Np} [y^{(i)} = k].
$$

For an expression that holds in expectation over arbitrary data, we need to introduce stochasticity. Assume we replace the (deterministic) classification rule in node $\Np$
$$
\hat{k}~|~\Np=\arg\max_k \pikN
$$

by a randomizing rule
$$
\hat k \sim \text{Cat} \left(\pi^{(\Np)}_1,\ldots,\pi^{(\Np)}_g \right),
$$ 

in which we draw the classes from the categorical distribution of their estimated probabilities (i.e., class $k$ is predicted with probability $\pi^{(\Np)}_k$).

***
Explain the difference between the deterministic and the randomized classification rule.

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

The deterministic rule tells us to pick the class with the highest empirical frequency. With the randomizing rule, we draw from a categorical distribution that is parameterized with these same empirical frequencies, meaning we draw the most frequent class with probability $\gamma = \max_k \pikN$. If we repeat this process many times, we will predict this class $\gamma$ \% of the time. Therefore, in expectation, we will also predict the most frequent class in most cases, but the rule is more nuanced as the magnitude of $\gamma$ makes a difference (the closer to 1, the more similar both rules).

</details> 
:::

***
Using the randomized rule, compute the expected MCE in node $\Np$ that contains $n$ random training samples. What do you notice?

::: {.content-visible when-profile="solution"}
<details> 
<summary>**Solution**</summary>

In order to compute the expected MCE, we need some random variables (RV) because we want to make a statement that holds for arbitrary training data drawn from the data-generating process. More precisely, we define $n \in \N$ i.i.d. RV $Y^{(1)}, \dots, Y^{(n)}$ that are distributed according to the categorical distribution induced by the observed class frequencies:

$$
\mathbb{P}(Y^{(i)} = k| \mathcal{N}) = \pikN \quad \forall i \in
\nset, \quad k \in \Yspace.
$$

The label $\yi$ of the $i$-th training observation is thus a realization of the corresponding RV $Y^{(i)}$.

Since our new randomization rule is stochastic, the predictions for the training observations will also be realizations of RV that we denote by  $\hat{Y}^{(1)}, ..., \hat{Y}^{(n)}$. By design, they follow the same categorical distribution:

$$
\mathbb{P}(\hat Y^{(i)} = k| \mathcal{N}) = \pikN \quad \forall i \in
\nset, \quad k \in \Yspace.
$$

Then, we can define the MCE for a node with $n = |\Np|$ when the randomizing rule is used:

$$
\rho_{\text{MCE}}(\Np) = \meanin \left[ Y^{(i)} \neq \hat Y^{(i)}\right].
$$

Taking the expectation of this MCE leads to a statement about a node with arbitrary training data:

\begin{align*}
  \E_{Y^{(1)}, \dots, Y^{(n)}, \hat{Y}^{(1)}, \dots, \hat{Y}^{(n)}}
  \left(\rho_{\text{MCE}}(\Np) \right)
  &=  \E_{Y^{(1)}, \dots,Y^{(n)}, \hat{Y}^{(1)}, \dots, \hat{Y}^{(n)}}
  \left(\meanin \left[Y^{(i)} \neq \hat{Y}^{(i)} \right] \right) \\
  & = \meanin \E_{Y^{(i)}, \hat{Y}^{(i)}} \left( \left[Y^{(i)}
  \neq \hat{Y}^{(i)} \right]\right) ~~ \text{i.i.d. assumption + linearity} \\
  & = \meanin\E_{Y^{(i)}}\left(
  \E_{\hat{Y}^{(i)}} \left( \left[Y^{(i)} \neq \hat{Y}^{(i)} \right] \right)
  \right) ~~ \text{Fubini's theorem} \\
  & = \meanin\E_{Y^{(i)}} \left( \sum_{k \in \Yspace} \pikN \cdot
  \left[ Y^{(i)} \neq k \right]  \right) \\
  % & = \meanin \E_{Y^{(i)}} \left( \sum_{k \in \Yspace
  % \setminus \{Y^{(i)}\}} \pikN \right) \\
  & = \meanin \E_{Y^{(i)}} \left( 1 - \pi^{(\Np)}_{k = Y^{(i)}}
  \right) \\
  & = \meanin \sumkg \pikN \cdot \left(1 - \pikN \right) \\
  & = n \cdot \frac{1}{n} \sumkg \pikN \cdot \left(1 - \pikN \right) \\
  &= \sumkg \pikN \cdot \left(1 - \pikN \right).
\end{align*}

This is precisely the Gini index CART use for splitting with Brier score. Gini impurity can thus be viewed as the expected frequency with which the training samples will be misclassified in a given node $\Np$.

In other words, in constructing CART with minimal Gini impurity, we minimize the expected rate of misclassification across the training data.

</details> 
:::
