Given are the data set

<<echo = FALSE, results=FALSE>>=
x = c(1, 2, 7, 10, 20)
y = c(1, 1, 0.5, 10, 11)
data = t(data.frame(x = x, y = y))
knitr::kable(data, "latex", digits = 3L)
log_x = log(x)
data = t(data.frame(log_x, y))
knitr::kable(data, "latex", digits = 2L)
@

\begin{tabular}{|l|r|r|r|r|r|}
\hline
$x$ & 1.0 & 2.0 & 7.0 & 10.0 & 20.0\\
\hline
$y$ & 1.0 & 1.0 & 0.5 & 10.0 & 11.0\\
\hline
\end{tabular}

and the same with log-transformed feature $x$:

\begin{tabular}{|l|r|r|r|r|r|}
\hline
$\log x$ & 0.0 & 0.7 & 1.9 & 2.3 & 3.0\\
\hline
$y$ & 1.0 & 1.0 & 0.5 & 10.0 & 11.0\\
\hline
\end{tabular}

\begin{enumerate}[a)]
  \item Compute the first split point the CART algorithm would find for each 
  data set (with pen and paper or in \texttt{R}, resp. \texttt{Python}).
  \item State the optimal constant predictor for a node $\Np$ when minimizing 
  the empirical risk under $L2$ loss and explain why this is 
  equivalent to minimizing \enquote{variance impurity}.
\end{enumerate}