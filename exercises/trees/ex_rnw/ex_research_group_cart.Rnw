Our medical research group has just learned about a new learner: Classification and Regression Trees (CART).
Enthusiastic about its ability to be applied to both classification and regression tasks, the team is eager to use CART for their practical research. \\
However, there is still some discussion about some properties of the algorithm.
Researcher Son describes how the tree is grown by greedy optimization.
This means each time a node $\Np$ is split, all possible points $t$ for all features $x_j$ are compared in terms of their empirical risk $\risk(\Np, j, t)$.
The split is then made with regards to the combination of feature and split point $(j,t)$ that results in the lowest risk on the training data.
Starting with the root node (containing all observations), this is recursively applied until a stopping criterion is hit. \\
Researcher Holger concludes this means that CART picks the optimal element of the hypothesis space $\Hspace$, i.e. there can be no other element of $\Hspace$ with lower empirical risk on the training data under consideration of the stopping criterion.

\begin{enumerate}\bfseries
  \item[1)] Is his conclusion correct? Explain your choice.
\end{enumerate}

Furthermore, the team has learned that CART can perform feature selection.
This describes the learners ability to discern between relevant and irrelevant features in model construction, and potentially select only a subset of the feature space for the final model.

\begin{enumerate}\bfseries
  \item[2)] Explain how CART performs automatic feature selection.
  Is there a way to gauge how relevant or irrelevant some feature $x_j$ could be after a CART model has been trained?
\end{enumerate}

Remember the logistic regression model our research group had implemented some weeks ago? 
The task was to predict whether a patient admitted to the hospital will require intensive care, which is a binary classification task with target space $\Yspace = \setzo$.
The feature space is:
$\Xspace = (\R_{0}^{+})^3$, with $\xi = (x_{age},\;x_{blood\;pressure},\;x_{weight})^{(i)} \in \Xspace$ for $i = 1, 2, \dots, n$ observations.
It turns out that although the logistic regression model itself generates useful predictions, its utility for the hospital is rather limited.
This is because there is not always time for the nurses to ask patients about their weight, leaving $x_{weight}$ as NA.
As a consequence, the logistic regression model cannot be used as it has no possibility to reasonably deal with missing values.
Researcher Lisa wonders whether using a CART model instead would allow for a more efficient use in an everyday hospital setting.

\begin{enumerate}\bfseries
  \item[3)] Can a CART model handle missing feature values during prediction? Explain your choice.
\end{enumerate}

The group decides to train the CART model. However, they are no entirely sure how exactly the CART algorithm works in detail. For example, they do not know how many possible split points there are for splitting a node, e.g. the root node.

\begin{enumerate}\bfseries
  \item[4)] Calculate an upper bound for the number of possible split points for splitting the root node.
\end{enumerate}

