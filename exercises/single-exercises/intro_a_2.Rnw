Derive the estimator of a normal linear regression model.
Proceed as follows: Write down the quadratic loss of the training data (OLS criterion) and minimize it for $\theta$. We don't need any assumption about the distribution, but define the functional form of the model and optimize it in regard to the loss.

Now, do the same for the ridge regression model:

$$\Lxyt = \sum_{i=1}^n (\yi - \theta^T \xi)^2 + \lambda||\theta||_2^2 $$

Some hints:
\begin{enumerate}
\item[a)] We will talk about ridge regression and $L_2$ penalization later in-depth.

\item[b)] The above approach is not perfect, as we are now also penalizing the intercept -
which is at least unusual. We do this here to simplify notation and the solution.
\end{enumerate}
