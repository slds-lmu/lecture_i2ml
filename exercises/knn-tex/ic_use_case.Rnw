% !Rnw weave = knitr

<<setup-child, include = FALSE>>=
knitr::set_parent("../../style/preamble_ueb.Rnw")
options(warn = -1)
library(AppliedPredictiveModeling)
library(data.table)
library(dplyr)
library(DiceKriging)
library(ggplot2)
library(iml)
library(mlbench)
library(mlr3verse)
library(mvtnorm)
library(rpart.plot)
library(patchwork)
library(precrec)
library(scales)
library(skimr)
library(visdat)
if (FALSE) pak::pak("slds-lmu/vistool")
library(vistool)
knitr::knit_theme$set("edit-vim")
knitr::opts_chunk$set(
  out.width = "50%",
  fig.width = 6,
  fig.height = 4,
  fig.align = "center",
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
@
%\input{../../latex-math/basic-math.tex}
%\input{../../latex-math/basic-ml.tex}


\kopfic{8}{Use Case}

<<echo=FALSE>>=
incladditionalcode = FALSE # for additional demos (R stuff)
@
%\incladditionalcodetrue % for additinoal demos (latex stuff)

<<>>=
# Simulate data as a mixture of Gaussians (pertaining to 2 classes, see later)
set.seed(42)
n_points = 1000
share_c0 = 0.4
class_0 = rmvnorm(
    n_points * share_c0,
    mean = c(3, 1.5), 
    sigma = matrix(c(1, 0.55, 0.55, 0.4), ncol = 2)
)
class_1 = rmvnorm(
    n_points * (1 - share_c0), 
    mean = c(5, 4.8), 
    sigma = matrix(c(0.3, 0.6, 0.6, 1.5), ncol = 2)
)
dt = data.table(
    cbind(
        rbind(class_0, class_1), 
        c(rep(0, n_points * share_c0), rep(1, n_points * (1 - share_c0)))
    )
)
colnames(dt) = c("x1", "x2", "class")
dt[, class := as.factor(class)]
# Trim a few more extreme points
for (i in c(0, 1)) {
    qx =  quantile(dt[class == i, x1], c(0.01, 0.99), na.rm = TRUE)
    dt = dt[!(class == i & (x1 < qx[1] | x1 > qx[2]))]
}
@

\section{Predicting tree biomass}

Estimating the biomass of trees is essential for assessing forest carbon stocks, but direct measurement is destructive and labor-intensive. Since tree diameter at breast height (DBH) is easy to record and closely related to total wood mass, it serves as a key variable for predicting biomass. Consider the following data on above-ground tree \texttt{biomass} ($x_2$) in \textit{t} and trunk \texttt{DBH} ($x_1$) in \textit{m}.

\begin{enumerate}[a)]

\item Explain which variable plays the role of \textit{feature} and \textit{target}, respectively.
Assume we obtain the following linear model. Estimate the coefficient associated with $x_1$ from the plot and interpret it.

<<>>=
p_empty = ggplot(data = dt, aes(x = x1, y = x2)) + 
    labs(x = expression(x[1]), y = expression(x[2])) +
    theme_bw() 
p_point = p_empty +
    geom_point(col = "darkgray", alpha = 0.3)
p_lm = p_point + 
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE, col = "black") 
p_lm
@


% ------------------------------------------------------------------------------
% Features: diameter; target: biomass; task: regression.
% The model could either be used for explanation (maybe a bit more likely), but also for "coarse" prediction
% ------------------------------------------------------------------------------

% ------------------------------------------------------------------------------
% Eyeballing: 1.5 --> every increase in diameter by 1m leads to an expected increase in biomass by 1.5t.
% ------------------------------------------------------------------------------

\item It seems that the model does not fit the data too well. What is the model's tendency for observations with large $x_1$ value?
Come up with a suitable transformation to $x_1$ that might improve the model fit. Describe how the transformed point cloud and model will look. How do the computation of the $x_1$ coefficient and its interpretation change?

% ------------------------------------------------------------------------------
% Model underestimates target for large x1 values. Transformation: square x1. Point cloud then looks roughly linear (SEE CODE BELOW). Computation stays the same (except for using transformed values), interpretation changes: now increase in x1^2 by 1 unit leads to increase in target by ... units.
% ------------------------------------------------------------------------------

<<eval=incladditionalcode, fig.height=3, out.width="100%">>=
cat("ADDITIONAL CODE FOR IN-CLASS DEMO ---------------------------------------")
dt_trafo = copy(dt)[, x1sq := x1**2]
p_trafo = p_empty +
    geom_point(data = dt_trafo, aes(x = x1sq, y = x2), alpha = 0.2) + 
  geom_smooth(
    data = dt_trafo,
    mapping = aes(x = x1sq, y = x2),
    method = "lm", 
    formula = y ~ x, 
    se = FALSE
  ) +
  labs(x = expression(x[1]^2))
p_lm + p_trafo
@

\end{enumerate}

\section{Predicting tree species}

\begin{enumerate}[a)]

\item Assume now that we want to use both tree \texttt{DBH} ($x_1$) and \texttt{biomass} ($x_2$) to predict a third variable, tree \texttt{species} ($y$; 0 = ``beech'', 1 = ``oak''). 
A \textit{logistic regression} model yields the \textit{decision boundary} pictured below (dashed line). What can you say for the respective values of the training loss function at the two highlighted points?

<<fig.width=8, out.width="75%">>=
# Train log reg
learner = lrn("classif.log_reg", predict_type = "prob")
task = TaskClassif$new(
  id = "trees", backend = dt, target = "class", positive = "1"
)
learner$train(task)
coeffs_logreg = coef(learner$model)
slope = -coeffs_logreg[2] / coeffs_logreg[3]
intercept = -coeffs_logreg[1] / coeffs_logreg[3]
# Visualize dec regions & boundary
p_classif = as_visualizer(task, learner = learner, retrain = FALSE)
p_theme = vistool_theme(
  palette = "viridis", theme = "bw", alpha = 0.3, legend_position = "right"
)
p_classif$set_theme(p_theme)
p_classif$add_boundary(alpha = 1)
p_classif$add_training_data(alpha = 1)
miscl_0 = dt[class == 0 & (x2 > intercept + slope * x1)]
miscl_1 = dt[class == 1 & (x2 < intercept + slope * x1)]
hlight_1 = miscl_0[which.min(miscl_0$x2)] # barely misclassified
hlight_2 = miscl_1[which.min(miscl_1$x2)] # very misclassified
p_classif$add_points(
  rbind(hlight_1, hlight_2), col = "red", size = 5, shape = 1, alpha = 1
)
p_classif$plot() +
  labs(title = "", fill = "probability for 1 ('oak')")
@


% ------------------------------------------------------------------------------
% Both have positive loss values (both are misclassified). The loss of the class-1 point is larger, as it is further away from the decision boundary.
% ------------------------------------------------------------------------------

\item Use the parameters of the decision boundary (intercept $a =$ \Sexpr{round(intercept, 2)}, slope $b =$ \Sexpr{round(slope, 2)}) to derive a decision rule for classifying a tree. The rule should be of the following form, where the conditions depend on $x_1$ and $x_2$:
$$y = \begin{cases} \text{``oak''} & \text{if \ldots } \\  \text{``beech''} & \text{if } \ldots  \end{cases}$$

% ------------------------------------------------------------------------------
% Use line equation x2 = slope * x1 + intercept to derive rule: if x2 > slope * x1 + intercept (or 1 * x2 - slope * x1 - intercept > 0), classify as oak.
% ------------------------------------------------------------------------------

% \item Recall that logistic regression has the benefit of providing not only ``hard'' labels but also \textit{class probabilities}. 
% We compute the probability of a tree being an oak as follows (with $s$ the logistic function):
% 
% $$\pixt = s(\thx) = s(\theta_0 + \theta_1 x_1 + \theta_2 x_2)$$
% 
% The logistic regression model from above has coefficients 
% $$\thetav = (\theta_0, \theta_1, \theta_2)^T = (\Sexpr{round(coeffs_logreg[1], 2)}, \Sexpr{round(coeffs_logreg[2], 2)}, \Sexpr{round(coeffs_logreg[3], 2)})^T$$
% How do these coefficients relate to the intercept $a$ and slope $b$ in the line equation of the decision boundary?
% 
% \textit{Hint:} Think about how your decision rule for hard labels translates to probabilities (assuming a threshold probability of 0.5 for predicting ``oak'').

% ------------------------------------------------------------------------------
% Decision boundary is where pi(x|theta) = 1 / (1 + exp(-theta^T * x)) = 0.5. Rearrange (^{-1}, -1, log(), *(-1)) to obtain theta^T * x = 0. Isolate x2 to obtain line equation x2 = -theta_0/theta_2 - (theta_1/theta_2) * x1.
% ------------------------------------------------------------------------------

\ifincladditionalcode \color{blue}
Decision boundary:
\begin{align*}
\pixt = \frac{1}{1 + \exp(-\thx)} & = 0.5 \\
1 + \exp(-\thx) & = 2 \\
\exp(-\thx) & = 1 \\
-\thx & = 0 \\
\thx & = 0
\end{align*}
Rearranging for $x_2$:
\begin{align*}
\theta_0 + \theta_1 x_1 + \theta_2 x_2 & = 0 \\
\theta_2 x_2 & = -\theta_0 - \theta_1 x_1 \\
x_2 & = -\frac{\theta_0}{\theta_2} - \frac{\theta_1}{\theta_2} x_1
\end{align*}
\color{black} \fi

\item Focusing on a small region near the decision boundary, how would you classify the highlighted point (red diamond) if you were to use $k$-\textit{nearest neighbors} with Euclidean distance and $k = 3$? What if $k = 5$?

<<fig.width=8, out.width="75%">>=
p_knn = p_classif$clone()
p_knn$add_points(
  data.table(x1 = 4.18, x2 = 2.35), col = "red", size = 5, shape = "diamond", alpha = 1
)
p_knn$plot() +
  labs(title = "", fill = "probability for 1 ('oak')") +
  xlim(c(4, 4.4)) +
  ylim(c(2.2, 2.7)) +
  theme(legend.position = "none")
@

% ------------------------------------------------------------------------------
% k = 3: oak (2 oaks, 1 beech); k = 5: beech (3 beeches, 2 oaks)
% ------------------------------------------------------------------------------
% 
% \item We want to get a feeling for neighborhoods in higher dimensions. Assume, for the sake of simplicity, that our features $x_1$ and $x_2$ are measured on the unit interval $[0, 1]$. Suppose we want to use only neighbors within 10\% of the range of each feature from the target point  (e.g., for $\xv = (0.5, 0.3)^T$, observations with $x_1 \in [0.45, 0.55], x_2 \in [0.25, 0.35]$). On average, what fraction of the available observations will we use to make the prediction?
% What if we had 10 features?
% 
% (You can disregard cases where the range falls outside the unit interval.)
% 
% % ------------------------------------------------------------------------------
% % 2D: 0.01 = 1% (0.1 * 0.1); 10D: 0.1**10
% % ------------------------------------------------------------------------------

\end{enumerate}

\end{document}