\begin{enumerate}
\item[1)] The generalization error is the expected (future) performance of a learner $\ind$ trained on $\ntrain$ observations with performance measure $\rho$.
As any such performance estimate depends on the concrete sampling of $\Dtest$ from $\Pxy$, we are interested in the limit of this expectation value, as $\ntest\rightarrow\infty$.
\item[2)] One samples $\Dtrain$ of size $\ntrain = 100$ and $\Dtest$ of size $\ntest$ $K$ times from $\Pxy$ (independently).
Each time, the learner $\ind$ is trained on $\Dtraini[k]$, and the respective performance $\rho$ is evaluated on $\Dtesti[k]$.
For $K,\ntest\rightarrow\infty$, the average performance $\frac{1}{K}{\sum\limits_{k=1}^K} \rho \left(\yv_{\Jtesti[k]}, {\F_{\Jtesti[k],\ind(\Dtraini[k])}}\right)$ converges to $\GE(\ind, \lamv, \ntrain = 100, \rho)$.
\item[3)] As $\ntrain$ must be smaller than $n$, the estimator is a pessimistically biased estimator of $\GE(\ind, \lamv, n, \rho)$, as we are not using all available data for training.
In the context of regression tasks and performance measures MSE or MAE, pessimistic bias means:
$\E\left[{\GEh_{\Jtrain, \Jtest}(\ind, |\Jtrain|, \lamv, \rho)}\right] > \GE(\ind, \lamv, n, \rho)$
%\end{align}
\item[4)] If one chooses a large $\ntrain$, $\ntest$ is small, and the estimator has a large variance. 
\end{enumerate}