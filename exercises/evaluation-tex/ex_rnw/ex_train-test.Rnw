Imagine you work in industry and have a data set $\D = \Dset$. You train a model $\fxh$ on this data set and now you want to bring it into production. Your customer wants to know which performance they can expect from your model, when using it from now on. As an answer, you want to provide an estimate for the generalization error of this model, i.e., $\GEfL$.

Since you have no data left to test your model on, you try to estimate, as a proxy for $\GEfL$, how good a model could be that would have been learned on $n$ data points, i.e., $\GEfull$ with $\ntrain = n$. But, since also in this case you would have no data points left to test your model on, you try the next best thing:

%In supervised learning, we typically assume that the data set $\D = \Dset$ originates from a data generating process $\Pxy$ in an i.i.d manner, i.e., $\D \sim \left(\Pxy\right)^n$.
%One could split data set $\D$ with $n$ observations into subsets $\Dtrain$ and $\Dtest$ of sizes $\ntrain$ and $\ntest$ with $\ntrain + \ntest = n$.
%Both subsets can be represented with index vectors $\Jtrain \in \JtrainSpace$ and $\Jtest \in \JtestSpace$, respectively.
%For such an index vector $J$ of length $m$, one can define a corresponding vector of labels $\yJ = \yJDef \in \Yspace^m$ and a corresponding matrix of prediction scores $\FJf = \FJfDef \in \R^{m\times g}$ for a model $f$.
%For regression tasks, $g = 1$ and $\FJf$ is a vector.

For a learner $\ind$, $\ntrain$ training observations and a performance measure $\rho$, the \textbf{generalization error} can be formally expressed as:
\begin{align}
\GEfull = \lim_{\ntest \rightarrow \infty} \E_{\Dtrain,\Dtest \sim \Pxy} \left[ \rho \left(
  \yv, \F_{\Dtest, \ind(\D_{\mathrm{train}}, \lamv)}
  \right)\right],
\end{align}
where for now we assume that $\Dtrain$ and $\Dtest$ can be independently sampled from $\Pxy$.
\begin{enumerate}\bfseries
  \item[1)] What is the generalization error? Describe the formula above in your own words.
\end{enumerate}
In practice, the data generating process $\Pxy$ is usually unknown and we cannot directly sample observations from it (instead, we typically use the available data $\D$ as a proxy). However, let's for now assume we can sample as many times as we like from $\Pxy$.
\begin{enumerate}\bfseries
  \item[2)] Explain how you could empirically estimate the generalization error $\GEfull$ with $\ntrain = 100$ of a learner $\ind$ with configuration $\lamv$ trained on $\ntrain = 100$ observations and evaluated on performance measure $\rho$, given that you can sample from $\Pxy$ as often as you like.
\end{enumerate}
In addition to an unknown data-generating process $\Pxy$, supervised learning is often restricted to a data set $\D$ of fixed size $n$.
Therefore, the true generalization error $\GEfull$ (with $\ntrain = n$ referring to all available data) remains unknown.
In this case, hold-out splitting is a simple procedure that can be used to estimate the generalization error:
\begin{align}
{\GEh(\ind, \lamv, (\Jtrain, \Jtest), \rho)} = \rho\left(\yv_{\Jtest}, {\F_{\Jtest,\ind(\Dtrain, \lamv)}}\right),
\end{align}
where $(\Jtrain, \Jtest)$ with $\Jtrain \in \JtrainSpace$ and $\Jtest \in \JtestSpace$ are index vectors that specify the subset of $\D$ the learner $\ind$ is trained on, with $|\Jtrain| < n$ (we train our model on less data as we have at hand) and $|\Jtrain| + |\Jtest| = n$.
Note the change in notation compared to the theoretical generalization error $\GEfull = \GE(\ind, \lamv, \ntrain, \rho)$ above:
\begin{itemize}
  \item $\GEfull$ is the \emph{theoretical} generalization error (the estimand). It is a single number that depends only on the learner, its hyperparameters, the training size $\ntrain$ (typically all available data as we are interested in the generalization error of a model trained on all available data) and the performance measure $\rho$, and averages over all possible training and test samples from $\Pxy$.
  \item ${\GEh(\ind, \lamv, (\Jtrain, \Jtest), \rho)}$ is an \emph{empirical estimator} based on a concrete split $(\Jtrain, \Jtest)$ of the fixed data set $\D$. Here the effective training size is $|\Jtrain| < \ntrain$ (as we need also some data points for testing). For a fixed $|\Jtrain|$, different choices of $(\Jtrain, \Jtest)$ generally lead to different values of ${\GEh}$.
\end{itemize}
\begin{enumerate}\bfseries
  \item[3)] Explain how the choice of training size $|\Jtrain|$ may influence the bias of ${\GEh(\ind, \lamv, (\Jtrain, \Jtest), \rho)}$ wrt $\GEfull$.
  \item[4)] Explain how the choice of training size $|\Jtrain|$ may influence the variance of ${\GEh(\ind, \lamv, (\Jtrain, \Jtest), \rho)}$.
\end{enumerate}
%Assume we know the true generalization error $\GE(\ind, \ntrain = 100, \rho)$ of a learner $\ind$ that is evaluated on performance measure $\rho$ and can sample as many times as we like from $\Pxy$.
%\begin{enumerate}\bfseries
%  \item[4)] How could you empirically estimate the bias of ${\GEh_{\Jtrain, \Jtest}(\ind, |\Jtrain|, \rho)}$ for a given $10 < |\Jtrain| < 90$?
%\end{enumerate}
