We are building models to detect a \textbf{rare disease}. Labels:
\begin{itemize}
  \item \textbf{1 = sick (positive)}
  \item \textbf{0 = healthy (negative)}
\end{itemize}

\subsection*{Step 1: Accuracy on Imbalanced Data (Two Models)}

We have 10 patients with true labels and predictions from two models:

\begin{center}
\begin{tabular}{c|c|c|c}
ID & True label $y$ & Model A & Model B \\ \hline
1 & 0 & 1 & 0 \\ \hline
2 & 0 & 0 & 1 \\ \hline
3 & 0 & 0 & 0 \\ \hline
4 & 1 & 0 & 1 \\ \hline
5 & 0 & 0 & 0 \\ \hline
6 & 0 & 0 & 0 \\ \hline
7 & 0 & 0 & 1 \\ \hline
8 & 0 & 0 & 0 \\ \hline
9 & 0 & 0 & 1 \\ \hline
10 & 0 & 0 & 0
\end{tabular}
\end{center}

Only 1 sick person (ID 4). The data is highly imbalanced.

\begin{enumerate}
  \item Mark each prediction as correct or incorrect.
  \item Compute the accuracy for Model A and Model B.
  \item Which model seems better based on accuracy?
  \item Is accuracy a good metric here? Explain briefly.
\end{enumerate}

\subsection*{Step 2: Understanding the Confusion Matrix}

Now when we've seen that it's important to look beyond accuracy, and actually analyze the types of errors our models make, we introduce the confusion matrix.

For the positive class (``sick'' = 1) we define:

\begin{itemize}
  \item \textbf{TP} (True Positives): actual 1, predicted 1
  \item \textbf{FP} (False Positives): actual 0, predicted 1
  \item \textbf{FN} (False Negatives): actual 1, predicted 0
  \item \textbf{TN} (True Negatives): actual 0, predicted 0
\end{itemize}

Confusion matrix form:

\begin{center}
\begin{tabular}{cc|c|c}
 & & \multicolumn{2}{c}{\textbf{Predicted}} \\
 & & 1 & 0 \\ \hline
\multirow{2}{*}{\textbf{Actual}} & 1 & TP & FN \\ \cline{2-4}
 & 0 & FP & TN \\
\end{tabular}
\end{center}

\textbf{Four "Universes" (sets from which we choose denominators):}
\begin{itemize}
  \item \textbf{Real Positives} (Condition Positive): $P = TP + FN$
  \item \textbf{Real Negatives} (Condition Negative): $N = TN + FP$
  \item \textbf{Predicted Positives}: $PP = TP + FP$
  \item \textbf{Predicted Negatives}: $PN = TN + FN$
\end{itemize}

\textbf{Key Rule for Understanding Metrics:}
\begin{itemize}
  \item Everything called a ``\ldots Rate'' (TPR, FPR, TNR, FNR) divides by a \emph{true class total} $\rightarrow$ denominator is $P$ or $N$.
  \item Everything with ``Precision / Predictive Value'' divides by a \emph{prediction total} $\rightarrow$ denominator is $PP$ or $PN$.
\end{itemize}

\textbf{Tasks:}
\begin{enumerate}
  \item Fill in the confusion matrix for Model A.
  \item Fill in the confusion matrix for Model B.
  \item For each model, compute $P$, $N$, $PP$, and $PN$.
\end{enumerate}

\subsection*{Step 3: Deriving Metrics from the Confusion Matrix}

Now that we understand the four "universes" (P, N, PP, PN), we can systematically derive any metric by choosing which universe to condition on. This means we'll never need to memorize formulas -- we just need to understand what question each metric answers.

\textbf{The Minimal Algorithm to Derive Any Confusion Matrix Metric:}
\begin{enumerate}
  \item \textbf{Determine the perspective:}
  \begin{itemize}
    \item Looking at \emph{real positives}? $\rightarrow$ Denominator: $TP + FN$
    \item Looking at \emph{real negatives}? $\rightarrow$ Denominator: $TN + FP$
    \item Looking at \emph{predicted positives}? $\rightarrow$ Denominator: $TP + FP$
    \item Looking at \emph{predicted negatives}? $\rightarrow$ Denominator: $TN + FN$
  \end{itemize}
  \item \textbf{Determine what counts as "success":}
  \begin{itemize}
    \item Success = correctly identified $\rightarrow$ TP or TN
    \item Failure = incorrectly classified $\rightarrow$ FP or FN
  \end{itemize}
  \item \textbf{Construct the fraction:}
  \begin{itemize}
    \item Example: ``Success on real positives'' $\rightarrow$ $\frac{TP}{TP + FN}$ = TPR
    \item Example: ``Failure on real negatives'' $\rightarrow$ $\frac{FP}{FP + TN}$ = FPR
  \end{itemize}
\end{enumerate}

\textbf{Common Metrics:}

Notice how metrics naturally group into three categories based on what they condition on. This structure helps us understand relationships between metrics and remember formulas.

\emph{Metrics conditioned on real class (denominators $P$ or $N$):}

These answer: "Among all actual positives/negatives, how many did we correctly/incorrectly classify?"
\begin{align*}
  \text{True Positive Rate (TPR, Recall, Sensitivity)} &= \frac{TP}{TP + FN} = \frac{TP}{P} \\
  \text{True Negative Rate (TNR, Specificity)} &= \frac{TN}{TN + FP} = \frac{TN}{N} \\
  \text{False Positive Rate (FPR, Fallout)} &= \frac{FP}{FP + TN} = \frac{FP}{N} = 1 - TNR \\
  \text{False Negative Rate (FNR, Miss Rate)} &= \frac{FN}{TP + FN} = \frac{FN}{P} = 1 - TPR
\end{align*}

\emph{Metrics conditioned on predictions (denominators $PP$ or $PN$):}

These answer: "Among all predicted positives/negatives, how many were actually correct/incorrect?"
\begin{align*}
  \text{Precision (Positive Predictive Value, PPV)} &= \frac{TP}{TP + FP} = \frac{TP}{PP} \\
  \text{Negative Predictive Value (NPV)} &= \frac{TN}{TN + FN} = \frac{TN}{PN} \\
  \text{False Discovery Rate (FDR)} &= \frac{FP}{TP + FP} = \frac{FP}{PP} = 1 - \text{Precision} \\
  \text{False Omission Rate (FOR)} &= \frac{FN}{TN + FN} = \frac{FN}{PN} = 1 - NPV
\end{align*}

\emph{Overall metrics (conditioned on total population $P + N$):}
\begin{align*}
  \text{Accuracy (ACC)} &= \frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{P + N} \\
  \text{Balanced Accuracy} &= \frac{TPR + TNR}{2} = \frac{\text{Sensitivity} + \text{Specificity}}{2}
\end{align*}

\textbf{Tasks:}
\begin{enumerate}
  \item Using the algorithm above, explain in your own words what \emph{TPR (Recall)} means. Which "universe" does it condition on?
  \item Using the algorithm above, explain in your own words what \emph{Precision} means. Which "universe" does it condition on?
  \item For Model A and Model B, compute: TPR (Recall), FPR, TNR (Specificity), Precision, and NPV.
  \item Verify the complementary relationships: Check that FPR = 1 - TNR and FNR = 1 - TPR for both models.
  \item If missing a sick patient is very costly, which model would you prefer based on the metrics? Why?
\end{enumerate}

\subsection*{Step 4: F1 Score (Harmonic Mean of Precision and Recall)}

Sometimes we want a single metric that balances precision and recall. We define the F1 score as the harmonic mean of precision and recall:
\[
  F1 = \frac{2}{\frac{1}{\text{Precision}} + \frac{1}{\text{Recall}}} = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\]

\textbf{Note:} If both precision and recall are 0, the F1 formula becomes undefined ($0/0$). By convention, we define F1 = 0 in this case, since the model has no predictive value for the positive class. In practice, implementations usually define F1 = 0 whenever precision = 0 or recall = 0, because the model is useless for the positive class in either case.

\textbf{Tasks:}
\begin{enumerate}
  \item Why would we want to combine precision and recall into a single metric? 
  \item Using your values for Model B, compute F1 for Model B.
  \item Compute F1 Score for Model A. What happens with the formula?
  \item Conceptual: why might we prefer the \emph{harmonic} mean instead of the arithmetic mean or the geometric mean?
\end{enumerate}

\subsection*{Step 5: Thresholds and Misleading 0.5}

So far we've worked with hard predictions (0 or 1). But many models output probability scores. Let's see how the choice of threshold affects our confusion matrix and all the metrics we derived.

Now we use probabilistic model outputs for 6 patients:

\begin{center}
\begin{tabular}{c|c|c|c}
ID & True $y$ & Model C score & Model D score \\ \hline
1 & 0 & 0.20 & 0.80 \\ \hline
2 & 1 & 0.40 & 0.60 \\ \hline
3 & 0 & 0.10 & 0.55 \\ \hline
4 & 0 & 0.30 & 0.20 \\ \hline
5 & 1 & 0.35 & 0.52 \\ \hline
6 & 0 & 0.05 & 0.10
\end{tabular}
\end{center}

We predict class 1 if the score is at least 0.5, otherwise class 0.

\textbf{Tasks:}
\begin{enumerate}
  \item For each model, convert the scores to hard predictions using threshold 0.5.
  \item For each model, build the confusion matrix and identify the four universes: $P$, $N$, $PP$, $PN$.
  \item Compute accuracy and recall for each model at threshold 0.5.
  \item Looking at Model C's scores and the true labels, conclude whether Model C is really a bad model, or there is another issue in play.
  \item For Model C, using the perspective-based algorithm from Step 3, compute TPR and FPR at threshold 0.5. What do these metrics tell you about the model's behavior?
  \item How would the confusion matrix and metrics for Model C change if we used threshold 0.3 instead of 0.5?
\end{enumerate}