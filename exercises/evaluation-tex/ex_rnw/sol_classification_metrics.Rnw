\textbf{Part 1: Confusion Matrix and Accuracy}

\begin{enumerate}
  \item Assigning terms to the confusion matrix:

\[
\begin{array}{c|c|c|}
    \multicolumn{1}{c}{} & \textbf{Actual Positive} & \textbf{Actual Negative} \\
    \hline
    \textbf{Predicted Positive} & (A) TP & (B) FP \\
    \hline
    \textbf{Predicted Negative} & (C) FN & (D) TN \\
    \hline
\end{array}
\]

%\textbf{Explanation of terms:}
\begin{itemize}
  \item \textbf{TP (True Positives)}: Cases where model correctly predicted positive and actual label is positive.
  \item \textbf{FP (False Positives)}: Cases where model incorrectly predicted positive but actual label is negative. Also called false alarms.
  \item \textbf{FN (False Negatives)}: Cases where model incorrectly predicted negative but actual label is positive. Also called misses.
  \item \textbf{TN (True Negatives)}: Cases where model correctly predicted negative and actual label is negative.
\end{itemize}

  \item Looking at the confusion matrix structure:
\begin{itemize}
  \item \textbf{Column sums:} The first column sum ($TP + FN$) represents all actual positives ($P$). The second column sum ($FP + TN$) represents all actual negatives ($N$).
  \item \textbf{Row sums:} The first row sum ($TP + FP$) represents all predicted positives ($PP$). The second row sum ($FN + TN$) represents all predicted negatives ($PN$).
\end{itemize}

  \item Accuracy represents the proportion of all cases that were correctly classified:

Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{TP + TN}{P + N}$

  \item The row and column sums are often used as denominators in metrics and can be understood as the different ``universes'' we condition on when calculating these metrics.
Simplified forms of each metric using the universes $P$, $N$, $PP$, and $PN$:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Metric} & \textbf{Formula} & \textbf{Simplified} \\
\hline
Precision (Positive Predictive Value, PPV) & $\frac{TP}{TP + FP}$ & $\frac{TP}{PP}$ \\
\hline
False Positive Rate (FPR, Fallout) & $\frac{FP}{FP + TN}$ & $\frac{FP}{N}$ \\
\hline
True Negative Rate (TNR, Specificity) & $\frac{TN}{TN + FP}$ & $\frac{TN}{N}$ \\
\hline
False Omission Rate (FOR) & $\frac{FN}{TN + FN}$ & $\frac{FN}{PN}$ \\
\hline
True Positive Rate (TPR, Recall, Sensitivity) & $\frac{TP}{TP + FN}$ & $\frac{TP}{P}$ \\
\hline
False Discovery Rate (FDR) & $\frac{FP}{TP + FP}$ & $\frac{FP}{PP}$ \\
\hline
Negative Predictive Value (NPV) & $\frac{TN}{TN + FN}$ & $\frac{TN}{PN}$ \\
\hline
False Negative Rate (FNR, Miss Rate) & $\frac{FN}{TP + FN}$ & $\frac{FN}{P}$ \\
\hline
\end{tabular}
\end{center}

\textbf{Categorization and interpretations:}

\emph{Question 1 (conditioned on real class):} ``Among all actual positives/negatives, how many did we correctly/incorrectly classify?'' (metrics condition on real class: denominators positives ($P$) or negatives ($N$))
\begin{itemize}
  \item \textbf{TPR (Recall, Sensitivity):}
  \begin{itemize}
    \item Proportion of actual positives correctly identified.
    \item Interprets how many sick patients we detect.
  \end{itemize}
  \item \textbf{TNR (Specificity):}
  \begin{itemize}
    \item Proportion of actual negatives correctly identified.
    \item Interprets how many healthy patients we correctly classify as healthy.
  \end{itemize}
  \item \textbf{FPR (Fallout):}
  \begin{itemize}
    \item Proportion of actual negatives incorrectly classified as positive.
    \item Interprets the false alarm rate among healthy patients (healthy people wrongly called sick).
  \end{itemize}
  \item \textbf{FNR (Miss Rate):}
  \begin{itemize}
    \item Proportion of actual positives incorrectly classified as negative.
    \item Interprets how many sick patients we miss (sick people wrongly told they are healthy).
  \end{itemize}
\end{itemize}

\emph{Question 2 (conditioned on predictions):} ``Among all predicted positives/negatives, how many were actually correct/incorrect?'' (metrics condition on predictions: denominators predicted positives ($PP$) or predicted negatives ($PN$))
\begin{itemize}
  \item \textbf{Precision (PPV):}
  \begin{itemize}
    \item Proportion of positive predictions that are correct.
    \item Interprets how trustworthy our positive predictions are (if we say a patient is sick, what is the chance they really are sick?).
  \end{itemize}
  \item \textbf{NPV:}
  \begin{itemize}
    \item Proportion of negative predictions that are correct.
    \item Interprets how trustworthy our negative predictions are (if we say a patient is healthy, what is the chance they are really healthy?).
  \end{itemize}
  \item \textbf{FDR:}
  \begin{itemize}
    \item Proportion of positive predictions that are incorrect.
    \item Interprets the false discovery rate among our positive predictions (if we say a patient is sick, how often are we wrong?).
  \end{itemize}
  \item \textbf{FOR:}
  \begin{itemize}
    \item Proportion of negative predictions that are incorrect.
    \item Interprets the false omission rate among our negative predictions (if we say a patient is healthy, how often are we wrong and miss a sick patient?).
  \end{itemize}
\end{itemize}
\end{enumerate}

\bigskip

\textbf{Part 2: Metrics Derived from the Confusion Matrix}

\begin{enumerate}
%   \item Confusion matrices for the data:

% \emph{Model A:}
% \begin{center}
% \begin{tabular}{cc|c|c}
%  & & \multicolumn{2}{c}{\textbf{Predicted}} \\
%  & & 1 & 0 \\ \hline
% \multirow{2}{*}{\textbf{Actual}} & 1 & 0 & 1 \\ \cline{2-4}
%  & 0 & 1 & 8 \\
% \end{tabular}
% \end{center}

% Model A: $TP = 0$, $FP = 1$, $FN = 1$, $TN = 8$

% \emph{Model B:}
% \begin{center}
% \begin{tabular}{cc|c|c}
%  & & \multicolumn{2}{c}{\textbf{Predicted}} \\
%  & & 1 & 0 \\ \hline
% \multirow{2}{*}{\textbf{Actual}} & 1 & 1 & 0 \\ \cline{2-4}
%  & 0 & 3 & 6 \\
% \end{tabular}
% \end{center}

% Model B: $TP = 1$, $FP = 3$, $FN = 0$, $TN = 6$

  \item Computing accuracy:

\emph{Model A:} Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{0 + 8}{0 + 8 + 1 + 1} = \frac{8}{10} = 0.8$

\emph{Model B:} Accuracy = $\frac{TP + TN}{TP + TN + FP + FN} = \frac{1 + 6}{1 + 6 + 3 + 0} = \frac{7}{10} = 0.7$

Based on accuracy alone, Model A appears better (0.8 vs 0.7).

  \item Computing metrics and comparing models:

\emph{Model A:}

\begin{minipage}{0.35\textwidth}
\begin{center}
\begin{tabular}{cc|c|c}
 & & \multicolumn{2}{c}{\textbf{Predicted}} \\
 & & 1 & 0 \\ \hline
\multirow{2}{*}{\textbf{Actual}} & 1 & 0 (TP) & 1 (FN) \\ \cline{2-4}
 & 0 & 1 (FP) & 8 (TN) \\
\end{tabular}
\end{center}
\end{minipage}%
\begin{minipage}{0.65\textwidth}
\begin{align*}
  \text{TPR} &= \frac{TP}{TP + FN} = \frac{0}{0 + 1} = \frac{0}{1} = 0 \\
  \text{TNR} &= \frac{TN}{TN + FP} = \frac{8}{8 + 1} = \frac{8}{9} \approx 0.889 \\
  \text{Precision} &= \frac{TP}{TP + FP} = \frac{0}{0 + 1} = \frac{0}{1} = 0 \\
  \text{NPV} &= \frac{TN}{TN + FN} = \frac{8}{8 + 1} = \frac{8}{9} \approx 0.889
\end{align*}
\end{minipage}

\vspace{0.5cm}

\emph{Model B:}

\begin{minipage}{0.35\textwidth}
\begin{center}
\begin{tabular}{cc|c|c}
 & & \multicolumn{2}{c}{\textbf{Predicted}} \\
 & & 1 & 0 \\ \hline
\multirow{2}{*}{\textbf{Actual}} & 1 & 1 (TP) & 0 (FN) \\ \cline{2-4}
 & 0 & 3 (FP) & 6 (TN) \\
\end{tabular}
\end{center}
\end{minipage}%
\begin{minipage}{0.65\textwidth}
\begin{align*}
  \text{TPR} &= \frac{TP}{TP + FN} = \frac{1}{1 + 0} = \frac{1}{1} = 1.0 \\
  \text{TNR} &= \frac{TN}{TN + FP} = \frac{6}{6 + 3} = \frac{6}{9} \approx 0.667 \\
  \text{Precision} &= \frac{TP}{TP + FP} = \frac{1}{1 + 3} = \frac{1}{4} = 0.25 \\
  \text{NPV} &= \frac{TN}{TN + FN} = \frac{6}{6 + 0} = \frac{6}{6} = 1.0
\end{align*}
\end{minipage}

\textbf{Model comparison and practical implications:}

\begin{itemize}
  \item \textbf{TPR (Recall) – catching sick patients:}
  \begin{itemize}
    \item Model B is better (1.0 vs 0): it catches all sick patients, while Model A misses all.
    \item For medical diagnosis, Model B is safer because no sick patients are overlooked.
  \end{itemize}
  \item \textbf{TNR (Specificity) – avoiding false alarms in healthy patients:}
  \begin{itemize}
    \item Model A is better (0.889 vs 0.667): it correctly identifies more healthy patients.
    \item This leads to fewer false alarms and fewer unnecessary medical interventions and costs.
  \end{itemize}
  \item \textbf{Precision – reliability of positive predictions:}
  \begin{itemize}
    \item Model B is better (0.25 vs 0), though both are low.
    \item A positive prediction from Model B is correct 25\% of the time, but never for Model A.
    \item If Model B predicts someone is sick, there is a 1 in 4 chance they actually are sick.
  \end{itemize}
  \item \textbf{NPV – reliability of negative predictions:}
  \begin{itemize}
    \item Model B is better (1.0 vs 0.889).
    \item Its negative predictions are always correct, while Model A's have a small error rate.
    \item Model B's ``healthy'' predictions are completely reliable; Model A's are slightly less trustworthy.
  \end{itemize}
\end{itemize}

\textbf{Overall:} Model B is much preferred in medicine, since it never misses a sick patient, even though it makes more false positive errors. Model A is less useful as it completely fails to identify any sick patient.
\end{enumerate}

\bigskip

\textbf{Part 3: F1 Score (Harmonic Mean of Precision and Recall)}

\begin{enumerate}
  \item Using your values for Model B, compute F1 for Model B:

\[
  F1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \cdot 0.25 \cdot 1.0}{0.25 + 1.0} = \frac{0.5}{1.25} = 0.4
\]

  \item Compute F1 Score for Model A. What happens with the formula?

Since both Precision and Recall are 0 for Model A, the F1 formula would result in $\frac{0}{0}$ (undefined). By convention, we define F1 = 0 in this case, since the model has no predictive value for the positive class. (In practice, most implementations set F1 = 0 whenever either precision or recall is 0.)

  \item Why might we prefer the \emph{harmonic} mean instead of the arithmetic mean for combining precision and recall?

The \textbf{harmonic mean} is more sensitive to low values than the arithmetic mean: if either precision or recall is low, the F1 score will also be low, which discourages a model from having poor performance in either metric.

\begin{itemize}
  \item For example, with precision $= 0.25$ and recall $= 1.0$:
    \begin{itemize}
      \item Arithmetic mean: $(0.25 + 1.0)/2 = 0.625$ (may seem acceptable)
      \item Harmonic mean (F1): $0.4$ (more accurately reflects the imbalance)
    \end{itemize}
\end{itemize}

F1 is only high when \emph{both} precision and recall are high:
    \[
      F1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
          = \frac{2 TP}{2 TP + FP + FN}
    \]
Each false positive (FP) or false negative (FN) directly lowers F1, so the F1 score will sharply penalize poor precision or recall, unlike the arithmetic mean.

We use the harmonic (not arithmetic) mean for F1 because we are averaging two rates 
  
  $$\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}, \quad
  \text{Recall}    = \frac{\text{TP}}{\text{TP} + \text{FN}},$$

with a common numerator (TP), just like averaging speeds over equal distances.

\begin{framed}
\noindent\textbf{Two-leg journey analogy (harmonic vs.\ arithmetic mean)}\\[0.3em]
This behavior is analogous to averaging speeds on a two-leg journey with speeds $30$ km/h and $60$ km/h:
\begin{itemize}
  \item \textbf{Harmonic mean (equal distances):} With equal distances ($30$ km each):
  \[
    t_1 = \frac{30}{30} = 1 \text{ hour}, \quad t_2 = \frac{30}{60} = 0.5 \text{ hours},
  \]
  \[
    d_{\text{total}} = 30 + 30 = 60 \text{ km}, \quad
    t_{\text{total}} = 1 + 0.5 = 1.5 \text{ hours},
  \]
  \[
    \text{Average speed} = \frac{d_{\text{total}}}{t_{\text{total}}}
      = \frac{60}{1.5} = 40 \text{ km/h} = \frac{2ab}{a + b} = \frac{2 \cdot 30 \cdot 60}{30 + 60} = 40 \text{ km/h}.
  \]
  The slower speed dominates total time.

  \item \textbf{Arithmetic mean (equal times):} With equal times ($1$ hour each):
  \[
    d_1 = 30 \cdot 1 = 30 \text{ km}, \quad d_2 = 60 \cdot 1 = 60 \text{ km},
  \]
  \[
    d_{\text{total}} = 30 + 60 = 90 \text{ km}, \quad
    t_{\text{total}} = 1 + 1 = 2 \text{ hours},
  \]
  \[
    \text{Average speed} = \frac{d_{\text{total}}}{t_{\text{total}}}
      = \frac{90}{2} = 45 \text{ km/h} = \frac{a + b}{2} = \frac{30 + 60}{2} = 45 \text{ km/h}.
  \]
  Each speed is weighted equally.

  \item \textbf{Sensitivity to low values:} Reducing the slower leg to $15$ km/h (equal distances):
  \[
    H = \frac{2ab}{a + b} = \frac{2 \cdot 15 \cdot 60}{15 + 60}
      = \frac{1800}{75} = 24 \text{ km/h},
  \]
  \[
    A = \frac{a + b}{2} = \frac{15 + 60}{2} = 37.5 \text{ km/h}.
  \]
\begin{itemize}
  \item Harmonic mean drops from $40$ to $24$ km/h (closer to $15$).
  \item Arithmetic mean drops from $45$ to $37.5$ km/h.
  \item Harmonic mean reacts more strongly to low values.
  \end{itemize}
\end{itemize}
\end{framed}

%In F1, precision and recall play the role of two speeds (like in a two-leg journey).
\end{enumerate}

\bigskip

\textbf{Part 4: Thresholds and the Choice of 0.5}

\begin{enumerate}
  \item For Model C, compute TPR and FPR at threshold 0.5 and at threshold 0.3. Describe qualitatively how the confusion matrix and the metrics change when moving from threshold 0.5 to 0.3, and explain why this demonstrates the importance of choosing an appropriate threshold.

\textbf{TPR and FPR for Model C at two thresholds:}

\begin{center}
\begin{minipage}[t]{0.47\textwidth}
\begin{framed}
\textbf{Threshold 0.5}
\begin{itemize}
  \item \textbf{Predictions:} All scores (0.40, 0.35, 0.30, 0.20, 0.10, 0.05) are below 0.5 $\Rightarrow$ all cases predicted as 0 (negative).
  \item \textbf{Confusion matrix:}
  \[
  \begin{array}{cc|c|c}
   & & \multicolumn{2}{c}{\textbf{Predicted}} \\
   & & 1 & 0 \\ \hline
  \multirow{2}{*}{\textbf{Actual}} & 1 & 0 & 2 \\ \cline{2-4}
   & 0 & 0 & 4 \\
  \end{array}
  \]
  \item \textbf{Metrics:}
  \begin{itemize}
    \item TPR $= \dfrac{TP}{P} = \dfrac{0}{2} = 0$
    \item FPR $= \dfrac{FP}{N} = \dfrac{0}{4} = 0$
  \end{itemize}
  \item \textbf{Interpretation:} Extremely conservative: no false positives, but misses all truly sick patients (no true positives).
\end{itemize}
\end{framed}
\end{minipage}
\hfill
\begin{minipage}[t]{0.47\textwidth}
\begin{framed}
\textbf{Threshold 0.3}
\begin{itemize}
  \item \textbf{Predictions:} IDs 2 (0.40) and 5 (0.35) predicted positive (the two truly sick patients); IDs 1, 3, 4, 6 predicted negative.
  \item \textbf{Confusion matrix:}
  \[
  \begin{array}{cc|c|c}
   & & \multicolumn{2}{c}{\textbf{Predicted}} \\
   & & 1 & 0 \\ \hline
  \multirow{2}{*}{\textbf{Actual}} & 1 & 2 & 0 \\ \cline{2-4}
   & 0 & 0 & 4 \\
  \end{array}
  \]
  \item \textbf{Metrics:}
  \begin{itemize}
    \item TPR $= \dfrac{2}{2} = 1.0$ (perfect recall)
    \item FPR $= \dfrac{0}{4} = 0$ (no false alarms)
    \item Precision $= \dfrac{2}{2} = 1.0$
    \item Accuracy $= \dfrac{6}{6} = 1.0$
  \end{itemize}
  \item \textbf{Interpretation:} With a lower threshold, the model becomes perfect on this dataset: all sick patients are detected and no healthy patients are flagged.
\end{itemize}
\end{framed}
\end{minipage}
\end{center}

Lowering the threshold from 0.5 to 0.3 changes the confusion matrix and metrics dramatically. This illustrates how crucial the threshold choice is and motivates tools like ROC curves that evaluate performance across thresholds.
\end{enumerate}
