
We are building models to detect a \textbf{rare disease}. Labels:
\begin{itemize}
  \item \textbf{1 = sick (positive)}
  \item \textbf{0 = healthy (negative)}
\end{itemize}

\subsection*{Step 1: Accuracy on Imbalanced Data (Two Models)}

We have 10 patients with true labels and predictions from two models:

\begin{center}
\begin{tabular}{c|c|c|c}
ID & True label $y$ & Model A & Model B \\ \hline
1 & 0 & 1 & 0 \\ \hline
2 & 0 & 0 & 1 \\ \hline
3 & 0 & 0 & 0 \\ \hline
4 & 1 & 0 & 1 \\ \hline
5 & 0 & 0 & 0 \\ \hline
6 & 0 & 0 & 0 \\ \hline
7 & 0 & 0 & 1 \\ \hline
8 & 0 & 0 & 0 \\ \hline
9 & 0 & 0 & 1 \\ \hline
10 & 0 & 0 & 0
\end{tabular}
\end{center}

Only 1 sick person (ID 4). The data is highly imbalanced.

\textbf{Tasks:}
\begin{enumerate}
  \item Mark each prediction as correct or incorrect.
  \item Compute the accuracy for Model A and Model B.
  \item Which model seems better based on accuracy?
  \item Is accuracy a good metric here? Explain briefly.
\end{enumerate}

\subsection*{Step 2: Confusion Matrix, Precision, Recall}

For the positive class (``sick'' = 1) we define:

\begin{itemize}
  \item TP: actual 1, predicted 1,
  \item FP: actual 0, predicted 1,
  \item FN: actual 1, predicted 0,
  \item TN: actual 0, predicted 0.
\end{itemize}

Confusion matrix form:

\begin{center}
\begin{tabular}{c|c|c}
 & Predicted 1 & Predicted 0 \\ \hline
Actual 1 & TP & FN \\ \hline
Actual 0 & FP & TN \\
\end{tabular}
\end{center}

Definitions:
\[
  \text{Precision} = \frac{TP}{TP + FP},\qquad
  \text{Recall} = \frac{TP}{TP + FN}.
\]

\textbf{Tasks:}
\begin{enumerate}
  \item In your own words, give a verbal interpretation of what \emph{precision} means.
  \item In your own words, give a verbal interpretation of what \emph{recall} means.
  \item Fill in the confusion matrix for Model A.
  \item Fill in the confusion matrix for Model B.
  \item Compute precision and recall for Model A and for Model B.
  \item If missing a sick patient is very costly, which model would you prefer? Why?
\end{enumerate}

\subsection*{Step 3: F1 Score (Harmonic Mean of Precision and Recall)}

We define the combined metric F1 score as the harmonic mean of precision $P$ and recall $R$:
\[
  F1 = \frac{1}{\frac{1}{P} + \frac{1}{R}} = \frac{2PR}{P + R}.
\]

\textbf{Tasks:}
\begin{enumerate}
  \item Why would we want to combine precision and recall into a single metric? 
  \item Using your values for Model B, compute F1 for Model B.
  \item Compute F1 Score for Model A.
  \item Conceptual: why might we prefer the \emph{harmonic} mean instead of the arithmetic mean $(P+R)/2$ or the geometric mean $\sqrt{P \times R}$?
\end{enumerate}

\subsection*{Step 4: Thresholds and Misleading 0.5}

Now we use probabilistic model outputs for 6 patients:

\begin{center}
\begin{tabular}{c|c|c|c}
ID & True $y$ & Model C score & Model D score \\ \hline
1 & 0 & 0.20 & 0.80 \\ \hline
2 & 1 & 0.40 & 0.60 \\ \hline
3 & 0 & 0.10 & 0.55 \\ \hline
4 & 0 & 0.30 & 0.20 \\ \hline
5 & 1 & 0.35 & 0.52 \\ \hline
6 & 0 & 0.05 & 0.10
\end{tabular}
\end{center}

We predict class 1 if the score is at least 0.5, otherwise class 0.

\textbf{Tasks:}
\begin{enumerate}
  \item For each model, convert the scores to hard predictions using threshold 0.5.
  \item For each model, build the confusion matrix.
  \item Compute accuracy and recall for each model at threshold 0.5.
  \item Looking at Model C's scores and the true labels, conclude whether Model C is really a bad model, or there is another issue in play.
\end{enumerate}

\subsection*{Step 5: ROC Curve Construction (Model C)}

Now that we have seen how looking into a single threshold (0.5) can be misleading, as a natural next step we want to analyze model performance across many possible thresholds. That's why we introduce the ROC curve (Receiver Operating Characteristic curve).

We focus on Model C from Step 4. 
Let $k$ be the number of top scores predicted as positive. There are 2 positives and 4 negatives.

Fill in the following table for $k = 0,1,2,3$:

\begin{center}
\begin{tabular}{c|c|c|c|c|c|c}
$k$ & TP & FP & FN & TN & TPR & FPR \\ \hline
0 &   &   &   &   &   &   \\ \hline
1 &   &   &   &   &   &   \\ \hline
2 &   &   &   &   &   &   \\ \hline
3 &   &   &   &   &   &  \\
\end{tabular}
\end{center}

Here TPR (true positive rate) and FPR (false positive rate) are:
\[
  \text{TPR} = \frac{TP}{\text{number of positives}},\qquad
  \text{FPR} = \frac{FP}{\text{number of negatives}}.
\]

\textbf{Tasks:}
\begin{enumerate}
  \item Fill in TP, FP, FN, TN for each value of $k$.
  \item Compute TPR and FPR for each row.
  \item Plot the points (FPR, TPR) on a graph with FPR on the x-axis and TPR on the y-axis.
  \item Connect the points to form an ROC curve for Model C.
\end{enumerate}

\subsection*{Step 6: AUROC (Area Under the ROC Curve)}

Since it's hard to judge model quality just by looking at the ROC curve, we often summarize it using the Area Under the ROC Curve (AUROC or ROC AUC). 

\textbf{Tasks:}
\begin{enumerate}
  \item What's the range of possible AUC values? Would a higher or lower value indicate a better model?
  \item Look at your ROC curve for Model C. Is it closer to the diagonal line from $(0,0)$ to $(1,1)$, or closer to the top-left corner $(0,1)$? What does this tell you about model quality?
  \item Describe in words how you could approximate the area under the ROC curve (AUC), e.g., using rectangles or trapezoids.
\end{enumerate}

\subsection*{Step 7: Precision--Recall Curve and Imbalanced Data}

\textbf{Tasks:}
\begin{enumerate}
  \item Our original 10-patient dataset is highly imbalanced (only 1 positive). Why might we want to change the axis of the curve from FPR/TPR to precision/recall? Resulting curve is conveniently called the precision-recall (PR) curve.
  \item In the rare disease setting, what does a point with high recall but low precision mean in practical terms?
\end{enumerate}

\subsection*{Step 8: Threshold Tuning}

Now that we know what's the best model which performs well across all thresholds, we now need to choose a specific threshold that will be used in practice. What approaches can we use to choose the best threshold?