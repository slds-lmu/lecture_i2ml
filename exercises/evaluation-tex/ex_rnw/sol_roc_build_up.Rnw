
\subsection*{Step 1 -- Solutions}

Model A has 8 correct predictions out of 10, so
\[
  \text{Accuracy}_A = 0.8.
\]
Model B has 7 correct predictions out of 10, so
\[
  \text{Accuracy}_B = 0.7.
\]
Based on accuracy alone, Model A looks better. However, because there is only 1 positive case, accuracy can be very misleading.

\subsection*{Step 2 -- Solutions}

\textbf{Model A:}
\begin{itemize}
  \item TP = 0, FP = 1, FN = 1, TN = 8.
  \item Precision = $0/(0+1) = 0$.
  \item Recall = $0/(0+1) = 0$.
\end{itemize}

\textbf{Model B:}
\begin{itemize}
  \item TP = 1, FP = 3, FN = 0, TN = 6.
  \item Precision = $1/(1+3) = 0.25$.
  \item Recall = $1/(1+0) = 1.0$.
\end{itemize}

If missing a sick patient is very costly, Model B is preferable: it catches all positives (recall $=1$), even though its accuracy is lower.

\subsection*{Step 3 -- Solutions}

For Model B (precision $P=0.25$, recall $R=1$),
\[
  F1^2 = \frac{2PR}{P + R} = \frac{2(0.25)(1)}{0.25 + 1} = \frac{0.5}{1.25} = 0.4.
\]
For Model A, both precision and recall are 0, so F1$^2 = 0$.

The harmonic mean is low if either precision or recall is low, so you only get a high F1$^2$ if both are high. The arithmetic mean would hide such imbalances.

\subsection*{Step 4 -- Solutions}

At threshold 0.5 for Model C: all scores are below 0.5, so the model predicts all 0.
\begin{itemize}
  \item TP = 0, FP = 0, FN = 2, TN = 4.
  \item Accuracy = $4/6 \approx 0.67$, recall = 0.
\end{itemize}

For Model D at threshold 0.5, predicted 1 for IDs 1, 2, 3, 5:
\begin{itemize}
  \item TP = 2, FP = 2, FN = 0, TN = 2.
  \item Accuracy = $4/6 \approx 0.67$.
  \item Recall = 1.0, precision = $2/4 = 0.5$.
\end{itemize}

At this threshold, Model D clearly looks better. But Model C is not truly bad: it gives the highest scores to the positive cases. The threshold 0.5 is simply a poor choice for this model.

\subsection*{Step 5 -- Solutions}

Sorting Model C scores (highest to lowest):
\begin{center}
\begin{tabular}{c|c|c|c}
Rank $k$ & ID & $y$ & Score C \\ \hline
1 & 2 & 1 & 0.40 \\ \hline
2 & 5 & 1 & 0.35 \\ \hline
3 & 4 & 0 & 0.30 \\ \hline
4 & 1 & 0 & 0.20 \\ \hline
5 & 3 & 0 & 0.10 \\ \hline
6 & 6 & 0 & 0.05
\end{tabular}
\end{center}

There are 2 positives and 4 negatives. Filling the table for $k=0,1,2,3$:

\begin{center}
\begin{tabular}{c|c|c|c|c|c|c}
$k$ & TP & FP & FN & TN & TPR & FPR \\ \hline
0 & 0 & 0 & 2 & 4 & 0/2 = 0.0 & 0/4 = 0.0 \\ \hline
1 & 1 & 0 & 1 & 4 & 1/2 = 0.5 & 0/4 = 0.0 \\ \hline
2 & 2 & 0 & 0 & 4 & 2/2 = 1.0 & 0/4 = 0.0 \\ \hline
3 & 2 & 1 & 0 & 3 & 2/2 = 1.0 & 1/4 = 0.25 \\
\end{tabular}
\end{center}

The ROC points (FPR, TPR) are: $(0,0)$, $(0,0.5)$, $(0,1)$, $(0.25,1)$. The curve hugs the left and top edges, indicating a very good classifier.

\subsection*{Step 6 -- Solutions}

The ROC curve is much closer to the top-left corner than to the diagonal, showing that Model C is strong. Because it ranks all positives above all negatives, the area under the ROC curve (AUC) is 1. In general, AUC can be approximated by splitting the FPR axis into intervals and summing the areas of rectangles or trapezoids under the curve.

\subsection*{Step 7 -- Solutions}

With highly imbalanced data, there are many more negatives than positives. Even if there are many false positives, FPR can remain small because it is divided by the large number of negatives. ROC curves can therefore look overly optimistic.

Precision--recall curves focus only on the positive class: recall tells us how many positives we catch, and precision tells us how many of the predicted positives are actually positive. High recall but low precision means we find most or all sick patients but also generate many false alarms.

\subsection*{Step 8 -- Solutions}

Using Model D's scores from Step~4:

\begin{center}
\begin{tabular}{c|c|c}
ID & True $y$ & Model D score \\ \hline
1 & 0 & 0.80 \\ \hline
2 & 1 & 0.60 \\ \hline
3 & 0 & 0.55 \\ \hline
4 & 0 & 0.20 \\ \hline
5 & 1 & 0.52 \\ \hline
6 & 0 & 0.10
\end{tabular}
\end{center}

\textbf{Threshold $t = 0.3$ and $t=0.5$}: in this tiny dataset, the set of scores $\ge t$ is the same (IDs 1,2,3,5), so:
\begin{itemize}
  \item TP = 2 (IDs 2,5), FP = 2 (IDs 1,3), FN = 0, TN = 2.
  \item Precision = $2/(2+2) = 0.5$, recall = $2/(2+0) = 1.0$.
\end{itemize}

\textbf{Threshold $t = 0.7$}:
\begin{itemize}
  \item Only ID 1 is predicted positive: TP = 0, FP = 1, FN = 2, TN = 3.
  \item Precision = 0, recall = 0.
\end{itemize}

If missing a sick patient is twice as bad as a false alarm, thresholds $0.3$ and $0.5$ are clearly preferable to $0.7$ because they have recall 1.0 (no missed positives). In this small example, $0.3$ and $0.5$ behave identically; in a larger dataset they would typically differ.

In practice, we would use a validation set and try many thresholds between 0 and 1. For each threshold we compute a metric such as F1$^2$ or a cost-weighted score, and then choose the threshold that maximizes this metric. This process is called \emph{threshold tuning}.
