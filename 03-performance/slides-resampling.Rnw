% Introduction to Machine Learning
% Day 3

% Set style/preamble.Rnw as parent.
<<set-parent, include = FALSE>>=
knitr::set_parent("../style/preamble.Rnw")
@

% Load all R packages and set up knitr
<<setup, child="../style/setup.Rnw", include = FALSE>>=
@

\lecturechapter{Evaluation: Resampling and Cross-Validation}
\lecture{Introduction to Machine Learning}



\begin{vbframe}{Resampling}

\begin{itemize}
  \item We need to construct a better performance estimator through \emph{resampling},
  that uses the data more efficiently.
  \item All resampling variants operate similar: The data set is split repeatedly into
  training and tests sets, and we later aggregate (e.g. average) the results.
  \item The usual trick is to make training sets quite larger (to keep the pessimistic bias small),
  and to handle the variance introduced by smaller test sets through many repetitions and averaging
  of results.
  \item Aim: Assess the performance of learning algorithm.
  \item Uses the data more efficiently then simple train-test.
  \item Repeatedly split in train and test, then aggregate (e.\,g. average) results.
\end{itemize}

\begin{center}
\includegraphics[width=7cm]{figure_man/ml_abstraction-crop.pdf}
\end{center}

\end{vbframe}

\begin{vbframe}{Expected Test Error}

Let $a$ be a learning algorithm that is applied to training data $\D$ of size $n$ which produces a prediction model $\fh_{\D}$ and suppose that the training data consists of observations which were randomly drawn from $\Pxy$.

\lz

The \emph{expected generalization error} ($EGE_n$) of the learning algorithm $a$ is the expectation of the (conditional) generalization error with respect to all possible training sets from $\Pxy$, of size $n$:

\begin{eqnarray*}
\EGEn(a) = \EDn(\GED) =  \EDn( L(y, \fh_{\D}(x)) | \D)
%&=& \frac{1}{B} \sum_{b = 1}^{B} \widehat{GE}(\fh_{\Dtrain^b}, \Dtest^b)\\
%&=& \frac{1}{B} \sum_{b = 1}^{B} \frac{1}{|\Dtest^b|} \sum_{\xyi \in \Dtest^b} L(\yi, \fh_{\Dtrain^b}(\xi))
\end{eqnarray*}

Note that the expectation $\EDn$ averages over the randomness in the data $\D$ of size $n$
that is used to produce $\fh_{\D}$.
%averages over everything that is random, including the randomness in the data $\D$ that produced $\fh_{\D}$.

\lz

$\Rightarrow$ Estimating the \emph{expected generalization error} is our goal!

\framebreak

Resampling methods are based on repeatedly splitting the observed data set into training and test sets and fitting a learning algorithm on training sets of equal size $n$.

\lz

Consider $B$ sets of training and test data generated by a resampling method (denoted by $\Dtrain^b$ and $\Dtest^b$, $b=1,\dots,B$, respectively), the \emph{expected generalization error} can then be estimated by

\begin{eqnarray*}
\widehat{EGE_n}(a) &=&
\frac{1}{B} \sum_{b = 1}^{B} \GEh{\Dtest^b} (\fh_{\Dtrain^b})\\
&=& \frac{1}{B} \sum_{b = 1}^{B} \frac{1}{|\Dtest^b|} \sum_{\xy \in \Dtest^b} L(y, \fh_{\Dtrain^b}(x))
\end{eqnarray*}

Note that the expected generalization error will depend on the cardinality of the generated training sets.

% \framebreak
%
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]\setstretch{1.2}
%   \State {\bf Input: }A data set $\D$ with $n$ observations, the number
%     of subsets $B$ to compute and an outer loss $L$ to measure the performance.
%   \State{\bf Output: }Summary of the validation statistics.
%   \State Generate $B$ subsets of $\D$ named $\Dtrain^{(1)}$ to $\Dtrain^{(B)}$
%   \State Set $S \leftarrow \emptyset$
%   \For {$b = 1 \to B$}
%     \State $\Dtest^{(b)} \leftarrow \D \setminus \Dtrain^{(b)}$
%     \State $\hat{f} \leftarrow $ Fitmodel($\Dtrain^{(b)}$)
%     \State $s_i \leftarrow \sum\limits_{(x, y) \in \Dtest^{(b)}}
%       L(y, \fh_{\Dtrain}(x))$
%     \State $S \leftarrow S \cup \{s_i\}$
%   \EndFor
%   \State Summarize $S$, e.g. $\mathrm{mean}(S)$
%   \caption{\footnotesize Generic resampling}
%   \label{alg:genresamp}
%   \end{algorithmic}
% \end{algorithm}
\end{vbframe}

\begin{vbframe}{Cross-Validation}

\begin{itemize}
  \item Split the data into $k$ roughly equally-sized partitions.
  \item Use each part once as test set and joint $k-1$ other as train.
  \item Obtain $k$ test errors and average.
\end{itemize}

Example: 3-fold cross-validation:
\begin{center}
\includegraphics[width=7cm]{figure_man/crossvalidation.png}
\end{center}

\framebreak

\textbf{Stratification}

Stratification tries to keep the distribution of the target class (or any specific categorical feature of interest) in each fold.

Example of stratified 3-fold Cross-Validation:

<<echo=FALSE, fig.height=3>>=
layout(cbind(rep(1, 3), 2:4, matrix(5:16, ncol = 4, byrow = TRUE)))
par(mar = c(2,2,4,2))

plot(as.factor(c(rep(1, 3), 2)), axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), main = "Class Distribution")
box()

par(mar = c(1,1,1,1))
red.ind = c(1, 6, 11)
for (i in 1:3) {
    plot.new()
    text(0.5, 0.5, cex = 1.2, paste("Iteration", i))
}
for (i in 1:12) {
  if (i %% 4 == 0) {
    plot.new()
    # text(0.5, 0.5, cex = 2, bquote(paste("=> ",widehat(GE)[D[test]^.(i/4)])))
  } else {
    plot(as.factor(c(rep(1, 3), 2)),
      axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"))
    if (i %in% red.ind) {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#E69F0055")
      box(col = "#E69F00")
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    } else {
      rect(par("usr")[1], par("usr")[3], par("usr")[2], par("usr")[4], col = "#56B4E955")
      box()
      plot(as.factor(c(rep(1, 3), 2)),
        axes = FALSE, ylim = c(0, 5), col = c("#E69F00","#56B4E9"), add = TRUE)
    }
  }
}
@

\framebreak

\textbf{Comments}

\begin{itemize}
  \item $k = n$ is known as leave-one-out (LOO) cross-validation or jackknife.
  \item The performance estimates for each fold are NOT independent, because
  of the structured overlap of the training sets.
  Hence, the variance of the estimator increases again for very large $k$ (close to LOO),
  when training sets nearly completely overlap.
  \item LOO is nearly unbiased, but has high variance.
  \item Repeated $k$-fold CV (multiple random partitions)
  can improve error estimation for small sample size.
  \item Common choices for $k$ are 5 or 10 ($k = n$ is known as leave-one-out (LOO)
cross-validation or jackknife).
  \item Estimates of the generalization error tend to be somewhat pessimistically biased
    (because the size of the training sets is $ n- (n/k) < n$), bias increases as $k$ gets smaller.
\end{itemize}

\framebreak

\begin{blocki}{Why should we prefer Cross-Validation over Holdout?}
  \item Estimate prediction error for a knn-classifier on \code{iris} data
  \item Run 10-fold CV and Holdout with $\frac{2}{3}$ train size 50 times
  \item Holdout estimate has more variance than CV because it depends on a single random data split\\
  $\Rightarrow$ danger of getting a biased performance estimate due to an "outlier-split" of train and test data
\end{blocki}

<<fig.height=2.5, echo = FALSE>>=
data("iris")
set.seed(1337)
task = mlr::makeClassifTask(data = iris, target = "Species")
rdescCV = mlr::makeResampleDesc(method = "CV", iters = 10)
rdescHO = mlr::makeResampleDesc(method = "Holdout")

# repeat 10 times to show that holdout is more depending on the randomness
storage = as.data.frame(matrix(0, nrow = 50, ncol = 2))
colnames(storage) = c("10-fold CV", "Holdout")

for (i in 1:nrow(storage)) {
  rCv = mlr::resample(learner = "classif.knn", k = 4, task = task,
    resampling = rdescCV, show.info = FALSE)
  rHO = mlr::resample(learner = "classif.knn", k = 4, task = task,
    resampling = rdescHO, show.info = FALSE)

  storage[i, "10-fold CV"] = as.numeric(rCv$aggr)
  storage[i, "Holdout"] = as.numeric(rHO$aggr)
}

# storage
ggstorage = as.data.frame(cbind(c(storage$`10-fold CV`, storage$Holdout),
  c(rep("CV", nrow(storage)), rep("Holdout", nrow(storage)))))
colnames(ggstorage) = c("mmce", "method")
ggstorage$mmce = as.numeric(as.character(ggstorage$mmce))

# boxplot(storage$`10-fold CV`, storage$Holdout)
p = ggplot2::ggplot(data = ggstorage) +  ggplot2::geom_boxplot(aes(x = method, y = mmce, fill = method)) + ggplot2::ggtitle("Repeated err. est. using holdout vs. using 10-fold cv." )

p

@

\end{vbframe}


\begin{vbframe}{The failure of leave-one-out}
\begin{itemize}
\item The \code{iris} data set has 50 instances of each class (\code{setosa}, \code{versicolor}, \code{virginica}).
\item A majority classifier should have 33.3\% accuracy.
\item But majority classification with LOO is unstable and will lead to a generalization error of 1 (0\% accuracy) in each fold.
\item For demonstration let's assume \code{iris} only consists of 3 instances:
\end{itemize}
<<fig.height=3, echo = FALSE>>=
par(mar = c(0, 0, 0, 0))
plot(1, type = "n", xlim = c(-2, 10), ylim = c(0, 4), axes = FALSE)
rect(seq(0, 4, by = 2), 0.1, seq(2, 6, by = 2), 1, col = c("#56B4E944","#56B4E944","#E69F0044"))
rect(seq(0, 4, by = 2), 1.1, seq(2, 6, by = 2), 2, col = c("#56B4E944","#E69F0044","#56B4E944"))
rect(seq(0, 4, by = 2), 2.1, seq(2, 6, by = 2), 3, col = c("#E69F0044", "#56B4E944","#56B4E944"))

text(c(3, 7.3, 9.5), 3.55,
  c("Training set (blue), test set (pink)", "Predicted label", "GE"),
  cex = 1.3)

text(seq(1, 5, by = 2), 2.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(seq(1, 5, by = 2), 1.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(seq(1, 5, by = 2), 0.55,
  c("Setosa", "Versicolor", "Virginica"), cex = 1.3)
text(7.3, 2.75, "Versicolor/", cex = 1.3)
text(7.3, 2.45, "Virginica", cex = 1.3)
text(7.3, 1.75, "Setosa/", cex = 1.3)
text(7.3, 1.45, "Virginica", cex = 1.3)
text(7.3, 0.75, "Setosa/", cex = 1.3)
text(7.3, 0.45, "Versicolor", cex = 1.3)

text(rep(-1, 3), c(0,1,2) + 0.55, paste("Iteration", 3:1),
  cex = 1.3)

for (i in 1:3) {
  text(9.5, 2 - (i - 1) + 0.55,
    bquote(widehat(GE)[D[test]^.(i)]), cex = 1.3)
  text(10.2, 2 - (i - 1) + 0.55, c("= 1"), cex = 1.3)
}
@
\end{vbframe}

\begin{vbframe}{Bootstrap}

The basic idea is to randomly draw $B$ training sets of size $n$ with
replacement from the original training set $\Dtrain$:
% \begin{eqnarray*}
% \Dtrain^1 &=& \{z^1_1, \ldots, z^1_n\}\\
% \vdots& \\
% \Dtrain^B &=& \{z^B_1, \ldots, z^B_n\}
% \end{eqnarray*}

\begin{center}
\begin{tikzpicture}[scale=1]
% style
\tikzstyle{rboule} = [circle,scale=0.7,ball color=red]
\tikzstyle{gboule} = [circle,scale=0.7,ball color=green]
\tikzstyle{bboule} = [circle,scale=0.7,ball color=blue]
\tikzstyle{nboule} = [circle,scale=0.7,ball color=black]
\tikzstyle{sample} = [->,thin]

% title initial sample
\path (3.5,3.75) node[anchor=east] {$\Dtrain$};

% labels
\path (3.5,3)   node[anchor=east] {$\Dtrain^1$};
\path (3.5,2.5) node[anchor=east] {$\Dtrain^2$};
\path (3.5,1.5) node[anchor=east] {$\Dtrain^B$};

\path (3.5,2) node[anchor=east] {$\vdots$};
\path[draw,dashed] (3.75,2.0) -- (4.5,2.0);

% initial sample
\path ( 3.75,3.75) node[rboule] (j01) {};
\path ( 4.00,3.75) node[gboule] (j02) {};
\path ( 4.25,3.75) node[bboule] (j03) {};
\path ( 4.5,3.75) node[nboule] (j20) {};

% bootstrap 1
\path ( 3.75, 3.0) node[rboule] {};
\path ( 4.00, 3.0) node[rboule] {};
\path ( 4.25, 3.0) node[bboule] {};
\path ( 4.5, 3.0) node[nboule] (b1) {};

% bootstrap 2
\path ( 3.75, 2.5) node[gboule] {};
\path ( 4.00, 2.5) node[bboule] {};
\path ( 4.25, 2.5) node[gboule] {};
\path ( 4.5, 2.5) node[rboule] (b2) {};

% bootstrap N
\path (3.75,1.5) node[gboule] {};
\path (4,1.5) node[rboule] {};
\path (4.25,1.5) node[nboule] {};
\path (4.5,1.5) node[nboule] (bN) {};

% arrows
\path[sample] (j20.east) edge [out=0, in=60] (b1.east);
\path[sample] (j20.east) edge [out=0, in=60] (b2.east);
\path[sample] (j20.east) edge [out=0, in=60] (bN.east);
\end{tikzpicture}
\end{center}

We define the test set in terms of out-of-bootstrap observations
$\Dtest^b = \Dtrain \setminus \Dtrain^b$.

\framebreak

\begin{itemize}
  \item Typically, $B$ is between $50$ and $200$.
  \item The variance of the bootstrap estimator tends to be smaller than the
  variance of $k$-fold CV, as training sets are independently drawn, discontinuities are smoothed out.
  \item The more iterations, the smaller the variance of the estimator.
  \item Tends to be pessimistically biased
  (because training sets contain only about $63.2 \%$ unique the observations).
  \item Bootstrapping framework might allow the use of formal inference methods
  (e.g. to detect significant performance differences between methods).
  \item The OOB error of a random forest is based on the same idea.
  \item Extensions exist for very small data sets, that also use the training error for
    estimation: B632 and B632+.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Subsampling}

\begin{itemize}
  \item Use a randomly selected proportion of the data for training, the other for
  test. Repeat $k$ times.
  \item $k = 30$ or $k=100$ are popular choices, which quickly can become computationally demanding.
  \item Holdout: Subsampling with $k = 1$.
  \item Bootstrapping: Draw observations with replacement.
  \item Sampling is done \textit{without} replacement, i.e., in each iteration $\eta < n$ observations without replacement are drawn for the training set.
  \item Typical choices for the subsampling rate $\frac{\eta}{n}$ : $4/5$ or $9/10$.
  \item The smaller the subsampling rate, the larger the bias.
  \item The more sampling iterations, the smaller the variance.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Bias-Variance of holdout}

  \begin{itemize}
    \item If the size of our initial, complete data set $\D$ is limited,
      single train-test splits can be problematic.
    \item The smaller our single test set is, the higher the variance
      of our estimated performance error (e.g., if we test on one observation, in the extreme case).
      But note that by just making the test set smaller, we do not introduce any bias,
      as we simply average losses on i.i.d. observations from $\Pxy$.
    \item The smaller training set becomes, the more pessimistic bias we introduce into the model.
      Note that if $|D| = n$, our aim is to estimate the performance of a model fitted
      on n observations (as this is what we will do in the end). If we fit on less data during
      evaluation, our model will learn less, and perform worse. Very small training sets will also
      increase variance a bit.
  \end{itemize}

  \framebreak

  Experiment:
  \begin{itemize}
    \item Data: simulate spiral data (sd = 0.1) from the \texttt{mlbench} package.
    \item Learner: CART (\texttt{classif.rpart} from \texttt{mlr}).
    \item Goal: estimate real performance of a model with $|\Dtrain| = 500$.
    \item Get the "true" estimator by repeatedly sampling 500 observations from the simulator,
      fit the learner, then evaluate on a really large number of observation.
    \item Analyse different types of holdout and subsampling (= repeated holdout), with different split rates:
    \begin{itemize}
    \item Sample $\D$ with $|\D| = 500$ and use split-rate $s \in \{0.05, 0.1, ..., 0.95\}$ for training with $|\Dtrain| = s \cdot 500$.
    \item Estimate performance on $\Dtest$ with $|\Dtest| = 500 \cdot (1 - s)$.
    \item Repeat the sampling of $\D$ 50 times for both methods and additionaly the splitting with $s$ 50 times in the subsampling \\($\Rightarrow$ 2500 experiments for each split-rate).
    \end{itemize}
  \end{itemize}

\framebreak

Visualize the perfomance estimator - and the MSE of the estimator - in relation to the true error rate.

<<>>=
# rsrc data from rsrc/holdout-biasvar.R
load("rsrc/holdout-biasvar.RData")
@

<<echo = FALSE, fig.height = 5>>=
library(plyr)
library(gridExtra)

ggd1 = melt(res)
colnames(ggd1) = c("split", "rep", "ssiter", "mmce")
ggd1$split = as.factor(ggd1$split)
ggd1$mse = (ggd1$mmce -  realperf)^2
ggd1$type = "holdout"
ggd1$ssiter = NULL
mse1 = ddply(ggd1, "split", summarize, mse = mean(mse))
mse1$type = "holdout"

ggd2 = ddply(ggd1, c("split", "rep"), summarize, mmce = mean(mmce))
ggd2$mse = (ggd2$mmce -  realperf)^2
ggd2$type = "subsampling"
mse2 = ddply(ggd2, "split", summarize, mse = mean(mse))
mse2$type = "subsampling"

ggd = rbind(ggd1, ggd2)
gmse = rbind(mse1, mse2)

ggd$type = as.factor(ggd$type)
pl1 = ggplot(ggd, aes(x = split, y = mmce, col = type))
pl1 = pl1 + geom_boxplot()
pl1 = pl1 + geom_hline(yintercept = realperf)
#pl1 = pl1 + theme(axis.text.x = element_text(angle = 45))

gmse$split = as.numeric(as.character(gmse$split))
gmse$type = as.factor(gmse$type)

pl2 = ggplot(gmse, aes(x = split, y = mse, col = type))
pl2 = pl2 + geom_line()
pl2 = pl2 + scale_y_log10()
pl2 = pl2 + scale_x_continuous(breaks = gmse$split)

grid.arrange(pl1 + theme_minimal(), pl2 + theme_minimal(), layout_matrix = rbind(1,1,2))
@

\framebreak

\begin{itemize}
  \item Overall, the error estimate is biased for small train data proportion $s$.
  \item Holdout introduces positive bias and overestimates performance of the model: for split sizes $s \geq 0.9, 0.95$  the average estimated test error is below the "true" mmce of 0.1.
  \item Subsampling does not introduce this bias and is less variant. (Why?)
  \item Optimal data set split before model starts overfitting according to holdout: roughly $\frac{2}{3}$ train and $\frac{1}{3}$ test data. Those values are often recommended in literature as a rule of thumb without general applicability.
  \item Subsampling allows for a higher $s \approx 0.9$ which means that we could use more data for training. The price for this is that subsampling is computationally more costly than simple holdout.
\end{itemize}


\end{vbframe}


\begin{vbframe}{Summary of Concepts}

\begin{itemize}
  \item In ML we fit, at the end, a model on all our given data.
  \item Problem: We need to know how well this model performs in the future.
  But no data is left to reliably do this.
  \item In order to approximate this, we do the next best thing. We estimate how well
  the learner works when it sees nearly $n$  points from the same data distribution.
  \item Holdout, CV, resampling estimate exactly this number. The "pessimistic bias" refers to
  when use much less data in fitting than $n$. Then we "hurt" our learner unfairly.
  \item Strictly speaking, resampling only produces one number, the performance estimator.
  It does NOT produce models, paramaters, etc. These are intermediate results and discarded.
  \item The model and parameters are obtained when we fit the learner finally on the complete data.
  \item This is a bit weird and complicated, but we have to live with this.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Comments}

\begin{itemize}
  \item 5CV or 10CV have become standard, 10-times repeated 10CV for small sample sizes, but THINK about whether that makes sense in your application.
  \item Do not use Hold-Out, CV with few iterations, or subsampling with a low subsampling rate for small samples, since this can cause the estimator to be extremely biased, with large variance.
  \item A $\D$ with $|\D| = 100.000$ can have small sample size properties if one class has only 100 observations \ldots
  \item For some models, computationally extremely fast calculations or approximations for the LOO exist.
  \item Recommendation: Use the LOO-CV for efficient model selection.
    Other cross-validation methods are superior most of the time.
  \item Picking the number $B$ of samples in bootstrapping and subsampling is not that simple (to be time efficient).
    A reasonable setting has to be data and task dependent.
  \item Modern results seem to indicate that subsampling has somewhat better properties than
    bootstrapping. The repeated observations can can cause problems in training algorithms,
    especially in nested setups where the \enquote{training} set is split up again.
\end{itemize}

\end{vbframe}


%
% \begin{vbframe}{How to split up your data?}
% \begin{block}{Data-rich situations:}
%
% <<data-rich,fig.width=5.5,fig.height=1, echo = FALSE>>=
% par(mar = c(0, 0, 0, 0))
% plot(1, type = "n", xlim = c(1, 10), ylim = c(0, 1),
%   axes = FALSE, xlab = "", ylab = "")
% rect(0, 0, 5, 0.5, col = 2L, border = 2L)
% text(2.5, 0.5, "Train", pos = 3L)
% rect(5, 0, 7.5, 0.5, col = 3L, border = 3L)
% text(6.25, 0.5, "Validation", pos = 3L)
% rect(7.5, 0, 10, 0.5, col = 4L, border = 4L)
% text(8.75, 0.5, "Test", pos = 3L)
% @
%
% \lz
% The best approach is to randomly divide the data set into three parts:
% \begin{itemize}
%   \item A training set, that is used to fit the model.
%   \item A validation set, to estimate prediction error for model selection.
%   \item And a test set, to assess the generalization error
%   of the final chosen model.
% \end{itemize}
%
% \end{block}
%
% \framebreak
%
% \begin{block}{But what do we do when the available data set is small?}
%   In those cases, which we will find us in most of the time, it is
%   usually inefficient to split the data into three parts.
%   We can now follow two approaches:
%
%   \lz
%
%   \begin{itemize}
%     \item \textbf{In-sample-estimates:} As the name suggests, these methods
%     use the training set, to approximate the validation step
%     analytically. Some well known methods amongst others are AIC, BIC
%     or VC-dimension.
%     \item \textbf{Resampling:} By effectively re-using samples performance can
%     still be estimated on unseen data without sacrifizing
%     cardinality of training an test sets.
% \end{itemize}
% \end{block}
%
% \end{vbframe}









% Sweet example from bernd. FCIM slide 12 in chapter 6




%
% \begin{vbframe}{Resampling in \texttt{mlr}}
%
% Returns aggregated values, predictions and some useful extra
% information.
%
% <<echo = TRUE, cache = TRUE>>=
% rdesc = makeResampleDesc("CV", iters = 3)
% r = resample("regr.rpart", bh.task, resampling = rdesc)
% # for the lazy
% r = crossval("regr.rpart", bh.task, iters = 3)
% print(r)
% @
%
% \framebreak
%
% <<echo = TRUE, cache = TRUE>>=
% print(r$aggr)
% print(head(r$measures.test))
% print(head(as.data.frame(r$pred)))
% @
%
% \framebreak
%
% Can be instantiated to ensure a fair comparison of multiple learners.
%
% <<echo = TRUE, cache = TRUE>>=
% rin = makeResampleInstance(rdesc, bh.task)
% str(rin$train.inds)
% model1 = resample("regr.rpart", bh.task, resampling = rin)
% model2 = resample("regr.randomForest", bh.task, resampling = rin)
% @
%
% \end{vbframe}

% \begin{vbframe}{Benchmarking in \texttt{mlr}}
%
% \begin{itemize}
% \item In a benchmark experiment different learning methods are applied to one or several data sets with the aim to compare and rank the algorithms with respect to one or more performance measures.
% \item It is important that the train and test sets are synchronized, i.e. all learning methods see the same data splits so that they are better comparable.
% \end{itemize}
%
% We will first create a list of tasks and a list of learners:
%
% <<echo = TRUE>>=
% data("BostonHousing", "mtcars", "swiss", package = c("mlbench", "datasets"))
% tasks = list(
%   makeRegrTask(data = BostonHousing, target = "medv"),
%   makeRegrTask(data = swiss, target = "Fertility"),
%   makeRegrTask(data = mtcars, target = "mpg")
% )
% learners = list(
%   makeLearner("regr.rpart"),
%   makeLearner("regr.randomForest"),
%   makeLearner("regr.lm")
% )
% @
%
%
% \framebreak
%
% The \texttt{benchmark} function from \texttt{mlr} allows you to compare all tasks and learners w.r.t. one or more measures based on the resampling method specified in the \texttt{resamplings} argument:
%
% <<echo = -1, cache = TRUE>>=
% set.seed(1)
% (bmr = benchmark(learners, tasks, resamplings = cv10, measures = mlr::mse))
% @
%
% \end{vbframe}
%
% \begin{vbframe}{Benchmarking in \texttt{mlr}: Access Data}
%
% <<echo = TRUE, cache = TRUE>>=
% head(getBMRAggrPerformances(bmr, as.df = TRUE), 3)
% head(getBMRPerformances(bmr, as.df = TRUE), 3)
% head(getBMRPredictions(bmr, as.df = TRUE), 3)
% @
%
% \end{vbframe}
%
% \begin{vbframe}{Visualizing Performances}
%
% Inspect the distribution of the performance measures using box plots:
%
% <<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
% plotBMRBoxplots(bmr, measure = mlr::mse)
% @
%
% \framebreak
%
% The plot is a \texttt{ggplot2} object that can be changed (names, colors, etc.):
%
% <<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
% plotBMRBoxplots(bmr, measure = mlr::mse, style = "violin") +
%   aes(color = learner.id)
% @
%
% \framebreak
%
% Visualize the aggregated performance values as dot plot:
%
% <<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
% plotBMRSummary(bmr)
% @
%
% \framebreak
%
% Plot the rankings of the aggregated performance values:
%
% <<echo = TRUE, cache = TRUE, out.width="0.8\\textwidth", fig.height=5>>=
% plotBMRSummary(bmr, trafo = "rank")
% @
%
% \end{vbframe}


\begin{vbframe}{Learning Curves}

\begin{itemize}
\item The \textit{Learning Curve} compares the performance of a model on training and test data over a varying number of training instances.
\item It allows us to see how fast an inducer can learn the given relationship in the data.
\item Learning should usually be fast in the beginning and saturate after data becomes larger an larger
% \item We should generally see performance improve as the number of training points increases When we separate training and testing sets and graph them individually
% \item We can get an idea of how well the model can generalize to new data
\item It visualizes when a model has learned as much as it can when
\begin{itemize}
\item the performance on the training and test set reach a plateau.
\item there is a consistent gap between training and test error.
\end{itemize}
\end{itemize}

An ideal learning curve looks like:

<<echo = FALSE>>=
# rsrc data from rsrc/learning_curve.R
load("rsrc/learning_curve.RData")
@

<<out.width="0.8\\textwidth", fig.height=6>>=
opt = res.rpart
opt$mmce = opt$mmce - mean(opt$mmce) + 0.08

p = ggplot(data = opt, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.15) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@

\framebreak

In general, there are two reasons for a bad looking learning curve:

\begin{enumerate}
\item High bias in model / underfitting
\begin{itemize}
\item training and test errors converge at a high value.
\item model can't learn the underlying relationship and has high systematic errors, no matter how big the training set is.
\item poor fit, which also translates into high test error.
\end{itemize}

<<echo = FALSE, eval = TRUE, out.width="0.8\\textwidth", fig.height=4>>=
p = ggplot(data = res.rpart, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.2) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@

\framebreak

\item High variance in model / Overfitting
\begin{itemize}
\item large gap between training and test errors.
\item model requires more training data to improve.
\item model has a poor fit and does not generalize well.
%\item Can simplify the model with fewer or less complex features
\end{itemize}

<<echo = FALSE, eval = TRUE, out.width="0.95\\textwidth", fig.height=4>>=
p = ggplot(data = res.ranger, aes(x = percentage, y = mmce))
p + geom_hline((aes(yintercept = 0.08))) +
  geom_text(aes(0.1, 0.08, label = "desired error", vjust = 1, hjust = 0)) +
  geom_errorbar(aes(ymin = mmce - sd, ymax = mmce + sd, colour = measure), width = 0.025) +
  geom_line(aes(colour = measure)) +
  geom_point(aes(colour = measure)) +
  ylim(0, 0.2) +
  ylab(expression(widehat(GE))) +
  xlab("Training set percentage") +
  scale_color_discrete(labels = c("1" = expression(D[test]), "2" = expression(D[train]))) +
  theme_minimal()
@
\end{enumerate}
\end{vbframe}

\endlecture
